---
title: "DSA1101 FINAL: Everything I Need"
navbar:
    left:
      - href: index.qmd
        text: Home
format: 
  html: 
      toc: true
      toc-depth: 3
      toc-expand: 4
      toc-location: right
      fig-align: center
      theme: sketchy
editor: visual
---

kalo merasa berguna boleh paynow aku pls

# General Notes

### Bayes Theorem

Suppose that 5% of all emails are spams. The phrase "you are a winner" occurs in 50% spam emails, and in 10% of non-spam emails.

![](images/clipboard-2431012875.png)

### Odds and Odds Ratio

1.  Interpreting the Value of Odds Ratio:

    -   OR = 1: The odds of the event are the same in both groups; no association between the exposure and the outcome.
    -   OR \> 1: The event is more likely to occur in Group 1 than in Group 2. A higher odds ratio indicates a stronger association.
    -   OR \< 1: The event is less likely to occur in Group 1 than in Group 2, indicating a negative association or a protective effect.

2.  Practical Example: Suppose you are studying whether smoking (exposure) is associated with lung cancer (outcome):

    -   If the OR = 2.5, it means smokers are 2.5 times more likely to develop lung cancer than non-smokers.
    -   If the OR = 0.5, it means smokers are half as likely (or have a 50% reduced odds) to develop lung cancer compared to non-smokers.

Example in Context:

| Group       | Outcome = Yes | Outcome = No | Odds             |
|-------------|---------------|--------------|------------------|
| Exposed     | 30            | 70           | ( 30/70 = 0.43 ) |
| Not Exposed | 15            | 85           | ( 15/85 = 0.18 ) |

Odds ratio:

$\text{OR} = \frac{0.43}{0.18} \approx 2.39$

-   Interpretation: The odds of the outcome (e.g., disease) are 2.39 times higher for the exposed group compared to the non-exposed group.

### Confusion Matrix

![](images/clipboard-519856922.png)

![](images/clipboard-220838721.png)

### Renaming Column by Names and Index

`names(data)[names(data) == "old_name"] <- "new_name"`

`names(data)[1] <- "new_name"`

### Models Characteristics

#### **K-Nearest Neighbors (KNN)**

**Pros**

-   **Non-parametric**: Makes no assumptions about the underlying data distribution.
-   **Handles Nonlinear Boundaries**: Works well for complex decision boundaries.

**Cons**

-   **Require Feature Scaling**: Requires normalization or standardization of features to ensure fair distance calculations.
-   **Sensitive to Noise and Outliers**: Outliers can significantly affect predictions.

#### **Decision Tree**

**Pros**

-   **Interpretable and Visualizable**: Easy to understand and explain; can visualize the tree structure.
-   **Handles Both Numerical and Categorical Data**: Flexible in dealing with mixed data types.
-   **No Need for Feature Scaling**: Works directly with raw data.
-   **Captures Nonlinear Relationships**: Can model complex patterns in data.

**Cons**

-   **Prone to Overfitting**: Without pruning or limiting depth, it can memorize the training data.
-   **Instability**: A small change in data can lead to a completely different tree structure.
-   **Bias Towards Dominant Features**: May prioritize features with more unique levels or high variance.

#### **Linear Regression**

**Pros**

-   **Theoretical Insights**: Provides coefficients that indicate feature importance and direction of influence.
-   **Good for Linearly Related Data**: Performs well when the relationship between features and target is linear.

**Cons**

-   **Assumes Linearity and Symmetrical Data**: Performs poorly if the relationship between features and the target is nonlinear.

    ![](images/clipboard-789686564.png)

-   **Sensitive to Outliers**: Outliers can heavily skew the model.

-   **Multicollinearity Issues**: High correlation among features can lead to unreliable coefficients.

-   **Only Numerical Features**: Requires categorical features to be encoded (turned into factor), which adds preprocessing complexity.

#### **Logistic Regression**

**Pros**

-   **Simple and Interpretable**: Easy to implement and interpret, especially for binary classification.
-   **Efficient**: Works well with smaller datasets and linearly separable data.
-   **Probability Outputs**: Provides probabilities, making it suitable for tasks requiring confidence scores.

**Cons**

-   **Assumes Linear Decision Boundary**: Struggles with data that has nonlinear relationships unless transformed.
-   **Sensitive to Outliers**: Outliers can distort predictions and coefficients.
-   **Feature Engineering Required**: May need transformations or interaction terms for better performance.
-   **Only Numerical Features**: Requires encoding (turning into factor) of categorical variables.

#### **Naive Bayes**

**Pros**

-   **Simple and Interpretable**: Based on straightforward probability calculations.
-   **Handles Missing Data**: Some variants can handle missing values effectively.
-   **Good for Text Data**: Performs well in applications like spam filtering and sentiment analysis.

**Cons**

-   **Assumes Feature Independence**: Relies on the strong (and often unrealistic) assumption that features are independent.
-   **Sensitive to Correlated Features**: Correlated features can degrade performance significantly.
-   **Limited Decision Boundaries**: Cannot model complex relationships between features and the target.
-   **Class Imbalance Issues**: May perform poorly with imbalanced datasets without proper adjustments.

**Comparison Table**

| **Model**               | **Pros**                                         | **Cons**                                         |
|------------------|--------------------------|----------------------------|
| **KNN**                 | Non-parametric, simple, models nonlinear data    | Sensitive to scaling, computationally expensive  |
| **Decision Tree**       | Interpretable, nonlinear, handles mixed data     | Prone to overfitting, instability                |
| **Linear Regression**   | Simple, interpretable, fast                      | Assumes linearity, sensitive to outliers         |
| **Logistic Regression** | Efficient, interpretable, provides probabilities | Assumes linear boundary, sensitive to outliers   |
| **Naive Bayes**         | Fast, interpretable, handles high dimensions     | Assumes feature independence, limited boundaries |

Choosing a Model

-   Use KNN: For small datasets with nonlinear relationships.
-   Use Decision Tree: For interpretable models with mixed data types or nonlinear patterns.
-   Use Linear Regression: For simple regression problems with linearly related data.
-   Use Logistic Regression: For binary classification problems with linearly separable data.
-   Use Naive Bayes: For text classification or when feature independence is a reasonable assumption.

| Unsupervised **Algorithm** | Pros                                                     | **Cons**                                                                  |
|------------------|------------------------|------------------------------|
| **K-Means**                | Simple, scalable, handles numerical data well            | Sensitive to initialization, assumes spherical clusters                   |
| **Apriori Algorithm**      | Intuitive, interpretable results, useful for rule mining | Computationally expensive, struggles with large/high-dimensional datasets |

# Tutorial Questions

## T1 (Introduction to R)

### Offsite Questions (Functions)

General idea of this problem: You have just graduated from NUS and just started your first job. You plan to buy a flat on your own which has price = \$1,200,000 (1.2 million dollars). You need to save money for several years before you can afford to make the down payment which is 25% of the flat's price.

-   Call the amount that you have saved thus far: **saved**. You start the very first month with a savings of \$10,000 that your parents gave you.
-   Call your monthly salary as **salary** which is paid at the end of every month. Each month, you are going to dedicate 40% of your salary to save for the down payment.
-   Assume that you invest your savings wisely, with a monthly average return of 2%. That means: at the end of each month, you receive an additional of **saved** ×0.02 funds where **saved** is the amount you have from end of previous month to put into your savings.
-   At the end of each month, your savings will be increased by the return on your investment, plus 40% of your monthly salary.

Note: In your code for the questions below, you MUST use the names as given in bold above.

1.  Write the code to calculate how many months it will take you to save up enough money for the down payment for two persons of of diferent salary: (i) **salary** = \$7,000; and (ii) **salary** = \$10,000.

    ```{r}
    # Solution 1.i
    cost <- 1200000 * 0.25 
    salary <- 7000
    saved <- 10000
    month <- 0

    while(saved < cost){
    month <- month + 1
    saved <- saved + 0.4 * salary + 0.02 * saved }
    print(month) # 55

    # Solution 2.i
    cost <- 1200000 * 0.25 
    salary <- 10000
    saved <- 10000
    month <- 0

    while(saved < cost){
    month <- month + 1
    saved <- saved + 0.4 * salary + 0.02 * saved }
    print(month) # 44
    ```

2.  In question above, we unrealistically assumed that the salary doesn't change over the years. However, now we consider that the salary will be raised every 4 months by a rate named rate, this variable should be in decimal form (i.e. 0.03 for 3%). The new salary will be applied for the month after every batch of 4 months.

    With this further assumption, write the code to calculate how many months it will take a person to save up enough money for the down payment if that person has (i) (**salary** = \$7,000 and **rate** = 0.02); (ii) (**salary** = \$10,000 and **rate** = 0.01).

    ```{r}
    # Solution 2.i
    cost <- 1200000 * 0.25 
    salary <- 7000
    saved <- 10000
    month <- 0
    rate <- 0.02

    while (saved < cost) {
      month <- month + 1
      saved <- saved + 0.4 * salary + 0.02 * saved
      if (month %% 4 == 0) {
        salary <-  salary * (1 + rate)
      }
    }
    print(month) #52

    # Solution 2.ii
    cost <- 1200000 * 0.25 
    salary <- 10000
    saved <- 10000
    month <- 0
    rate <- 0.01

    while (saved < cost) {
      month <- month + 1
      saved <- saved + 0.4 * salary + 0.02 * saved
      if (month %% 4 == 0) {
        salary <-  salary * (1 + rate)
      }
    }
    print(month) #43
    ```

### Onsite Questions (Loops & Which)

1.  A sequence is generated using the following recursive relation:

$x_n = 2x_{n-1} - x_{n-2} + 5, \quad \text{for } n \geq 3,$

with $x_1 = 0$ and $x_2 = 1$.

(a) Use a "for" loop in R to find the 30th term of the series.

    ```{r}
    x <- numeric(30)
    x[1] <- 0
    x[2] <- 1 
    for(i in 3:30) {x[i] <- 2*x[i-1] - x[i-2] + 5}
    x[30] # 2059
    ```

(b) Find the smallest value of (n) such that $x_n \geq 1,000$.

    ```{r}
    print(which(x >= 1000))
    # all elements from x_22 onwards are larger than 1000
    # x_22 is the smallest, thus answer is n = 22

    ```

------------------------------------------------------------------------

2.  Consider another sequence which is generated using the following recursive relation:

$y_1 = 2800 + 1.02 \times y_0, \quad \text{with } y_0 = 10000$,

$y_n = 2800 + 1.02 \times y_{n-1}, \quad \text{for } n \geq 2$.

Find the smallest value of $n$ such that $y_n \geq 300,000$.

```{r}
y <- numeric()
y <- append(y, 2800 + 1.02*10000) 
while (max(y) < 300000) {
  y <- c(y, 2800 + 1.02*max(y))
}
length(y) # 55
y[55] # 305759.6
```

## T2 (Basic Prob and Stats)

```{r}
#df = read.csv(...) 
# Imports the dataframe
# BY DEFAULT, READ.CSV:
# 1. Assumes values are separated by commas
# 2. Assumes the data contains headers

df = read.csv(
	"ex_1.txt",
	sep = " ",
	header = FALSE
) 
# for tab use "\t"
```

```{r}
### Interpreting Scatterplots
# Is there any relationship? Is it strong?
# If there is, is it positive or negative?
# Relationship is linear or non-linear?
# Special observations?
# Is the variability of the response stable when x changes? ***

```

```{r}
### Interpreting Boxplots
# How many outliers
# Median
# Distribution
# Any visible pattern? (Optional)

```

### Offsite Questions (Box, Scatter, QQ, Hist)

Forced Expiratory Volume (FEV) is an index of pulmonary function that measures the volume of air expelled after 1 second of constant effort. The dataset `FEV.csv` contains measurements for 654 children aged 3 to 19 years of age. The purpose of the data collection was to study how FEV is affected by certain other variables. The variables that we shall work with are

**Age**: Age in years.\
**FEV**: FEV measurement.\
**Hgt**: Height in inches.\
**height**: Height in meters\
**Sex**: 0 = female, 1 = male.\
**Smoking status**: 0 = current non-smoker, 1 = current smoker.

\(a\) What is the response variable in this study?

```{r}
# FEV is the response variable in this study
```

\(b\) Create a histogram of FEV and comment on it.

```{r}
fev <- read.csv("FEV.csv")
hist(fev$FEV)

# Comments:
# range: 0.5 to 6
# unimodal
# slightly right skewed
# may have outliers
```

\(c\) Create a boxplot of FEV and identify how many outliers there are. Investigate your data and comment on these outliers.

```{r}
bp <- boxplot(fev$FEV)
outliers <- bp$out
length(outliers)
# There are 9 outliers

index <- which(fev$FEV %in% outliers)
fev[index,]

# Comments:
# 1. All outliers are male
# 2. Most (8/9) are
# non-smokers
# 3. They are rather
# tall
```

\(d\) Generally, is the sample of FEV normally distributed?

```{r}
qqnorm(fev$FEV, pch = 20)
qqline(fev$FEV, col = "red")

# Comments:
# Left tail sample quantiles are larger than expected,
# hence left tail shorter than normal. 
# Right tail sample quantiles larger than expected, hence # right tail longer than normal.

# Combined with the histogram of FEV, it is clear that the
# sample of FEV is not normally distributed and quite 
# right skewed.

```

\(e\) Create separate histograms for male and female FEV, then obtain separate numerical summaries for males and female FEV. Comment on what you observe.

```{r}
# First, obtain the male and female FEV values separately
# to plot onto two different histograms.

female = fev$FEV[which(fev$Sex==0)]
# Alternative: fev$FEV[fev$Sex==0]
male = fev$FEV[which(fev$Sex==1)] 
# Alternative: fev$FEV[fev$Sex==1]

# Now plot the two histograms side-by-side

opar <- par(mfrow=c(1,2)) 
hist(female, col = 2, freq= FALSE, 
main = "Histogram of Female FEV", ylim = c(0,0.52))
hist(male, col = 4, freq= FALSE, 
main = "Histogram of Male FEV", ylim = c(0,0.52))
# Remember to use par(mfrow=c(1,1))  to reset the diagram layout!

# obtaining separate numerical summaries for male and female
IQR(female) # 1.04
summary(female) 
var(female) # 0.4169424

IQR(male) # 1.5275
summary(male)
var(male) # 1.006866

# Comments:
# Both histogram are unimodal but have different shapes. 
# It is almost symmetrical for females, but quite right-skewed for males.
# Median FEV for females is much lower than males, 2.49 compared to 2.605.
# Variability for males is higher than females.
# The respective IQR are 1.54 and 1.05.



```

\(f\) Create a scatterplot with height (in metres) on the x-axis and FEV on the y-axis.

```{r}
plot(fev$height, fev$FEV)
points(female ~ fev$height[which(fev$Sex==0)], 
col = "red", pch = 20)
points(male ~ fev$height[which(fev$Sex==1)], 
col = "darkblue", pch = 20)
legend(1.2, 5, legend = c("Female", "Male"), 
col = c("red","darkblue"), pch=c(20,20))

```

\(g\) Compute the correlation between FEV and height and comment on your results.

```{r}
cor(fev$FEV, fev$height) # 0.8675619
# Computed correlation is quite high.
# It is clear from the plot that there is a strong 
# positive linear association between FEV and height. 
# The range of FEV and height for males appears larger than for females. 
# The variability of FEV at lower heights seems slightly 
# less than the variability of FEV at greater heights.

```

### Onsite Questions (Box)

Consider a dataset about HDB resale flats in Singapore given in `hdbresale_reg.csv`, which helps to investigate the factors that affect the resale price of the flats.

1.  Import the dataset into R.

    ```{r}
    hdb = read.csv("hdbresale_reg.csv")
    names(hdb)

    ```

2.  How many flats are in the sample given?

    ```{r}
    dim(hdb)

    # the dataframe has 6055 rows and 11 columns
    # thus there are 6055 flats

    ```

3.  Exploring the variable `resale_price`:

    (a) Create a histogram. Give your comments. Is the sample of resale prices normally distributed?

    ```{r}
    hist(hdb$resale_price)
    # range: 200k to ~1 mil
    # unimodal
    # clearly right skewed
    # suspected outliers
    ```

    (b) Create a box plot.

    ```{r}
    bp <- boxplot(hdb$resale_price)
    outliers <- bp$out
    head(outliers)
    length(outliers) # 284 outliers

    ```

## T3 (Linear Regression 1)

### Offsite Questions (Functions)

Consider the question given in Tutorial 1.

\(a\) For the first question in Tutorial 1, use the code to define a function, called F1, where the argument of F1 is salary. Run function F1 for the two cases mentioned.

```{r}
cost <- 1200000 * 0.25 
F1 <- function(salary) {
  saved <- 10000
  month <- 0
  while(saved < cost) {
    month <- month + 1
    saved <- saved + 0.4 * salary + 0.02 * saved
    }
  return(month)
}
```

\(b\) For the second question in Tutorial 1, use the code to define a function, called F2, where F2 has two arguments: salary and rate. Run function F2 for the two cases mentioned to obtain the results.

```{r}
F2 <- function(salary, price = 1200000, rate = 0.01, portion_save = 0.4) {
  r = 0.02 # monthly rate return from investment
  saved <- 10000 # savings given by parents initially
  month <- 0
  cost = 0.25*price
  while(saved < cost) {
    month = month +1
    saved = saved + portion_save * salary + saved * r
    if (month %% 4 ==0) {
      salary = salary*(1+rate)
    }
  }
  return(month)
}

```

### Onsite Questions (LM Mathematics)

1.  Read the data from the file Colleges.txt. Consider a simple linear regression of the percentage of applicants accepted (Acceptance) on the median combined math and verbal SAT score of students (SAT), called Model M1.

    \(a\) Write your own function in R, name the function as simple, to derive the intercept β0 and the slope β1 of Model M1. Hint: Use the formula of the estimated coefficients, β̂1 and β̂0, given in slide 31/52 of Topic 3.

    ```{r}
    simple <- function(x, y) {
    	beta1 = (sum(y*x) - sum(y)*mean(x)) /
    (sum(x^2) - sum(x)*mean(x))
    	beta0 = mean(y)- beta1 * mean(x)
    	return(c(beta0, beta1))
    }

    data = read.csv("Colleges.txt", sep = "\t")
    head(data)
    simple(data$SAT, data$Acceptance)

    ```

    \(b\) Use function lm() in R to derive the coefficients of Model M1. Compare with your answer in part (a).

    ```{r}
    lm(Acceptance ~ SAT , data = data)
    # Same coefficients from different functions
    # Helps verify implementation of 'simple' function

    ```

2.  *(If time permits)*, consider a dataset about HDB resale flats in Singapore given in hdbresale_reg.csv. Consider a simple model (Model M2) where the resale price is the response and the floor area in square meters is the only regressor.

    \(a\) Use the function simple you formed in the question above to find the coefficients of Model M2.

    ```{r}
    df <- read.csv("hdbresale_reg.csv")
    names(df)

    simple(x = df$floor_area_sqm, y = df$resale_price)
    ```

    \(b\) Use the function lm() in R to derive the coefficients of Model M2.

    ```{r}
    lm(df$resale_price ~ df$floor_area_sqm)
    ```

## T4 (Linear Regression 2)

Linear Model Assumptions:

1.  Quantitative

2.  Symmetric - Histogram

3.  Variability of y is stable when x changes - Scatterplot

Transformations:

y -\> log(y), sqrt(y), 1/y

Evaluating Goodness-Of-Fit:

-   F-Test (p-value)

-   R-squared

### Offsite Questions (Hist, Linear Regression)

Consider data set given in the file `hdbresale_reg.csv` on Canvas, which has the information of 6055 HDB resale flats in Singapore. We would want to form a linear model that helps to predict the resale price of HDB flats, based on the floor area in square meters and the type of the flats.

\(a\) Consider the resale price, plot a histogram of it and give your comments. Is it suitable to fit a\
linear model for this response variable? Explain.

```{r}
data <- read.csv("hdbresale_reg.csv")
str(data)
hist(data$resale_price)

# Comments: 
# Right-skewed histogram
# Hence, resale price is NOT suitable to be the response 
# as assumption of linear model (symmetric) is violated.
# For a right skewed variable, it is suggested to try 
# transforming the response by taking its logarithm.

```

\(b\) Consider the resale price, plot a histogram of log_e of it and give your comments. Is it more\
suitable to fit a linear model for this response variable than the original resale price?

```{r}
hist(log(data$resale_price))

# Comments: 
# The histogram of the log of the resale price 
# is more symmetric, hence it is more suitable than 
# the original resale price as a response for our linear model.

```

\(c\) Derive a scatter plot of the log_e of the resale price against the floor area in square meters. Give your comments.

```{r}
# creates a new column for the log(price)
data$log.price = log(data$resale_price) 
plot(data$log.price ~ data$floor_area_sqm)

# Comments:
# There seems to be a strong, positive, linear relationship
# between log(price) and floor area.
# The variability of the log(price) seems fairly stable 
# when the floor area changes.***

```

\(d\) Fit a linear model where the log of the resale price be the response. Write down the fitted equation.

```{r}
str(data)
unique(data$flat_type)

M = lm(log.price ~ floor_area_sqm + flat_type, data = data)
summary(M)

# fitted_log(price) = 
# 12.35 + 
# 0.003712 * floor_area_sqm +
# 0.119 ∗ I(flat type = 3 ROOM) +
# 0.2093 ∗ I(flat type = 4 ROOM) +
# 0.2762 ∗ I(flat type = 5 ROOM) +
# 0.4302 ∗ I(flat type = Executive)

```

\(e\) Report the coeficient of the floor area in square meters and interpret it.

```{r}
# The coefficient of it is 0.003712. 
# Meaning when comparing two flats of the same type, 
# then an increase of 1 square meter will increase 
# the predicted log(price) by 0.003712.
# Equivalently, the price will 
# increase by e^0.003712 = 1.003719 TIMES.

```

\(f\) Predict the resale price of a 4-room HDB at that is of 100 square meters.

```{r}
new = data.frame(
  floor_area_sqm = 100, 
  flat_type = "4 ROOM"
  )

predicted_log.price = predict(M, new)
predicted_price = exp(predicted_log.price)
print(predicted_price) # 412807.6
```

\(g\) Report $R^2$ of the model and interpret it.

```{r}
summary(M)$r.squared
# The R_squared value is 0.712. 
# That means model M can explain 71.2% 
# of the variability of the response in the sample.

```

### Onsite Questions (Linear Regression)

1.  A dataset on house selling price was randomly collected (`house_selling_prices_FL.csv`). It’s our interest to model how y = selling price (dollar) is dependent on x = the size of the house (square feet). A simple linear regression model (y regress on x) was fitted, called Model 1.

The given data has another variable, NW, which specifies if a house is in the part of the town considered less desirable (NW=0).

\(a\) Derive the correlation between x and y.

```{r}
data <- read.csv("house_selling_prices_FL.csv")
str(data)
cor(data$price, data$size)
# Correlation between
# 'price' and 'size' is 0.761
```

\(b\) Derive a scatter plot of y against x. Give your comments on the association of y and x.

```{r}
plot(data$price ~ data$size, pch = 20)
plot(data$size, data$price, pch = 20)

# 1. There is a clear relationship.
# 2. It is a positive association.
# 3. It seems linear.
# 4. Variability of price is quite stable as size changes.

```

\(c\) Derive $R^2$ of Model 1. Verify that $\sqrt{R^2} = |cor(y, x)|$. In which situation can we have $\sqrt{R^2} = cor(y, x)$?

```{r}
M1 <- lm(price ~ size, data = data)
r.squared <- summary(M1)$r.squared
print(r.squared) # 0.57952
print(sqrt(r.squared)) # 0.7612621
# Same as correlation value derived from 1.a.

# Given that we have verified that 
# sqrt(R^2) = abs(cor(y, x)),
# sqrt(R^2) = cor(y, x) when 
# abs(cor(y, x)) = cor(y, x).
# Thus, when cor(y, x) > 0,
# then in a simple model y ∼ x, 
# we always have sqrt(R^2) = cor(y, x).

```

\(d\) Form a model (called Model 2) which has two regressors (x and NW). Report the coefficient of variable NW in Model 2. Interpret it.

```{r}
str(data)
data$NW = as.factor(data$NW)
str(data)

M2 = lm(price ~ size + NW, data = data)
summary(M2)

# Fitted equation of linear model:
# fitted_price = -15257.514 + 77.985 * size + 
# 					30569.087 * I(NW = 1)
# The estimated coefficient of NW in Model 2 is 30569.1 
# This means that for houses of same size, those with 
# NW = 1 (Those located at NorthWest area) has predicted 
# price higher than that in the area of NW = 0 by $30569.1

```

\(e\) Estimate and report the price of a house where its size is 4000 square feet and is located at the more desirable part of the town.

```{r}
new_data <- data.frame(size = 4000, NW = "1")
predict(M2, newdata = new_data)

# The predicted price of a house with size x = 4000 and 
# NW = 1 is $327252.1.
```

## T5 (K-Nearest Neighbor)

### Offsite Questions (Multiple Scater, Linear Regression)

1.  (MLR) Consider the horseshoe female crab data given in the csv file `crab.csv`. We would want to form a model for the weight of the female crabs (kg), which depends on its width (cm) and its spine condition (1 = both good, 2 = one worn or broken, 3 = both worn or broken).

    ```{r}
    data <- read.csv("crab.csv")
    head(data)
    str(data)
    data$spine <- as.factor(data$spine)
    # 1 = both good
    # 2 = one worn or broken
    # 3 = both worn or broken
    str(data)
    ```

    \(a\) Produce a scatter plot of variable weight against width for different condition of spine.

    ```{r}
    attach(data)
    plot(weight ~ width, pch = 20, main = "weight vs width for each spine class",
         xlab = "width", ylab = "weight")
    points(weight[spine == "1"] ~ width[spine =="1"], pch = 15, col = "red")
    points(weight[spine == "2"] ~ width[spine =="2"], pch = 16, col = "blue")
    points(weight[spine == "3"] ~ width[spine =="3"], pch = 17, col = "green")
    legend(22,4,legend=c("1", "2", "3"),col=c("red", "blue", "green"), pch=c(15,16,17))
    ```

    \(b\) Fit a linear regression model for weight which has two explanatories, width and spine.

    ```{r}
    model <- lm(weight ~ width + spine, data = data)
    summary(model)
    ```

    \(c\) Is the fitted model signicant?

    ```{r}
    # the model f statistic showing p value is less than 2.2 x 10^-16.
    # Since this is far below 0.05, the model can be considered significant
    ```

    \(d\) Derive $R^2$ and adjusted $R^2$ of the fitted model.

    ```{r}
    summary(model)$r.squared
    # 0.7917598
    summary(model)$adj.r.squared
    # 0.7880632
    ```

    \(e\) Write down the fitted model.

    ```{r}
    summary(model)
    # weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) -0.06969*I(spine=3)
    ```

    \(f\) Two female crabs of the same width, and the diference of their weight if one has spines are of good condition and another one with broken spines.

    ```{r}
    # weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) + -0.06969*I(spine=3)
    # weight1 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*0
    # weight2 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*1

    weight.dif <- (0.05544*0 + -0.06969*0) - (0.05544*0 + -0.06969*1)
    weight.dif
    ```

    \(g\) Predict the weight of a female crab that has width of 27 cm and has both spines worn or broken.

    ```{r}
    newdata <- data.frame(width = 27, spine = "3")
    predict(model, newdata = newdata)
    ```

2.  Measures of classier performance

    Suppose we have developed a K-nearest neighbors classier for predicting diabetes status. The following table shows the actual response $Y$ (1 = yes, 0 =n o) and fitted value $\hat Y$ using the classier for 10 test data points. A test data point is predicted to be $\hat G$= 1 if $\hat Y$ \> δ, for a specied threshold value δ (Recall that we use δ = 0.5 in class, also known as the majority rule).

    (a) We define: $TPR = \frac {TP}{TP +FN}$, $FPR = \frac {FP}{FP +TN}$; For each of the thresholds δ = 0.3, 0.6 and 0.8, derive $TPR$ and $FPR$ in making predictions with the K-nearest neighbors classier for the 10 test data points. Plot $TPR$ against $FPR$ for the three thresholds.

    ```{r}
    y <- c(1,1,0,1,1,0,0,1,0,0)
    ycap <- c(0.9,0.5,0.7,0.4,0.5,0.2,0.7,0.9,0.1,0.1)
    tpr.all <- numeric(0)
    fpr.all <- numeric(0)

    sigma.list <- c(0.3,0.6,0.8)
    for (sigma in sigma.list) {
      pred <- ifelse(ycap >= sigma, 1 ,0)
      confusion.matrix <- table(y, pred)
      tpr <- confusion.matrix[2,2]/sum(confusion.matrix[2,])
      fpr <- confusion.matrix[1,2]/sum(confusion.matrix[1,])
      tpr.all <- append(tpr.all, tpr)
      fpr.all <- append(fpr.all, fpr)
    }

    plot(tpr.all ~ fpr.all, pch = 20,
         xlim = c(0,1),
         ylim = c(0,1))
    points(tpr.all[1] ~ fpr.all[1], pch = 15, col = "red")
    points(tpr.all[2] ~ fpr.all[2], pch = 16, col = "blue")
    points(tpr.all[3] ~ fpr.all[3], pch = 17, col = "green")
    legend(0.7,0.4,legend=c("sigma = 0.3", "sigma = 0.6", "sigma = 0.8"),
           col=c("red", "blue", "green"), pch=c(15,16,17))
    ```

    \(b\) Can we add the two points (0,0) and (1,1) to the plot of $TPR$ against $FPR$ in part (a). Explain why or why not.

    ```{r}
    # If σ > 0.9 then all test points have predicted ˆG = 0 (predicted as negative), 
    # so TPR = FPR = 0.
    # If σ < 0.1, then all test points have predicted ˆG = 1 (predicted as positive),
    # so TPR = FPR = 1.
    # Since there exist σ within the range from 0 to 1 for the two points to happen, 
    # these two points can be added to the plot.
    ```

3.  The CSV file `Caravan.csv` contains data on 5822 real customer records on caravan insurance purchase. This data set is owned and supplied by the Dutch data mining company, Sentient Machine Research, and is based on real world business data. Each record consists of 86 variables, containing sociodemographic data (variables 1-43) and product ownership (variables 44-86). Variable 86 (`Purchase`) indicates whether the customer purchased a caravan insurance policy. For this business, assume that the overall error rate (equivalently, the accuracy) is not of interest. Instead, the company wants to use the classier to predict who are the potential customers likely to purchase insurance. Then the metric precision will be important, since it relates the proportion of Individuals who will actually purchase the insurance, among the group of individuals who are predicted to purchase insurance.

    ```{r}
    cara <- read.csv("Caravan.csv")
    head(cara, n = 2)
    str(cara)
    dim(cara)
    ```

    \(a\) Without any classier, if the company tries to sell insurance to a random selection of customers, what is the success rate?

    ```{r}
    table(cara$Purchase)
    succesRate <- table(cara$Purchase)[2]/sum(table(cara$Purchase))
    succesRate

    # data set shows almost 6% of people purchased insurance
    ```

    \(b\) Standardize the input features. $Hint$: Use `scale()` command in R.

    ```{r}
    cara <- cara[,-1] #throwing the first column since it provides no information
    scaled.X <- scale(cara[,-86]) #throwing the response column
    ```

    \(c\) Randomly select 1000 observations to form the test data, and the remaining observations will be the training data.

    ```{r}
    set.seed(5)
    n <- dim(cara)[1] #train data number
    shuffled.index <- sample(c(1:n), size = 1000)
    X.train <- scaled.X[-shuffled.index,]
    X.test<- scaled.X[shuffled.index,]
    Y.train <- as.factor(cara$Purchase[-shuffled.index])
    Y.test <- as.factor(cara$Purchase[shuffled.index])
    ```

    \(d\) Use 1-nearest neighbor classier for the training data to predict if a customer will purchase insurance. Compute the precision of the classier.

    ```{r}
    library(class)

    pred <- knn(X.train, X.test, Y.train, k=1)
    confusion.matrix <- table(Y.test, pred) 
    # Precision = TP/TP+FP
    precision <- confusion.matrix[2,2]/sum(confusion.matrix[,2])
    confusion.matrix
    precision
    ```

    \(e\) Repeat question 3d, for k-nearest neighbor classier where k = 3,5,10. Which value of k gives the best precision?

    ```{r}
    precision.all <- data.frame()
    k.list <- c(3, 5, 10)
    for (k in k.list) {
      pred <- knn(X.train, X.test, Y.train, k = k)
      confusion.matrix <- table(Y.test, pred) 
      # Precision = TP/TP+FP
      precision <- confusion.matrix[2,2]/sum(confusion.matrix[,2])
      precision.all <- rbind(precision.all, data.frame(k, precision))
    }

    precision.all

    # So far, k = 5 gives the best precision.
    # However, one might use N -fold cross validation 
    # to have the average precision for each k.
    # With that, the value of k that gives largest average precision is chosen.
    ```

### Onsite Questions (KNN Mathematics)

1.  The k-nearest neighbor classifier

The table below provides a training data set containing six observations, three predictors, and one qualitative response variable, y.

| Obs | $X_1$ | $X_2$ | $X_3$ | Y     |
|-----|-------|-------|-------|-------|
| 1   | 0     | 3     | 0     | Red   |
| 2   | 2     | 0     | 0     | Red   |
| 3   | 0     | 1     | 3     | Red   |
| 4   | 0     | 1     | 2     | Green |
| 5   | -1    | 0     | 1     | Green |
| 6   | 1     | 1     | 1     | Red   |

Suppose we wish to use this data set to make a prediction for a new observation with response Y given that it has $X_1 = X_2 = X_3 = 0$ using K-nearest neighbors.

\(a\) Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$

```{r}
X1 <- c(0, 2, 0, 0, -1, 1)
X2 <- c(3, 0, 1, 1, 0, 1)
X3 <- c(0, 0, 3, 2, 1, 1)
Y <- c("Red", "Red", "Red", "Green", "Green", "Red")
dataframe <- data.frame(X1, X2, X3, Y)

dataframe$distance <- 
    sqrt(
        dataframe$X1^2 + 
        dataframe$X2^2 + 
        dataframe$X3^2
    )

dataframe <- dataframe[order(dataframe$distance),]
```

\(b\) What is our prediction with K=1? Why?

```{r}
# Prediction for K = 1 is 
# Green as the one nearest 
# point is also Green. Thus,
# by majority vote, KNN 
# classifies the new data
# point as Green.
```

\
(c) What is our prediction with K=3? Why?

```{r}
# Prediction for K = 3 is 
# Red as 2/3 of the nearest
# points are Red, with one 
# Green point. Thus,
# by majority vote, KNN 
# classifies the new data
# point as Red.

```

\
(d) If the Bayes decision boundary (the gold standard decision boundary) in this problem is highly non-linear, then would we expect the best value for KKK to be large or small? Why?

```{r}
# We would expect the best
# value of K to be small, 
# since it translates to 
# a more flexible 
# classification method.

```

## T6 (Decision Trees)

```{r}
# fit <- rpart(...)

# Predict using decision trees
#predict(fit, newdata, type = "class") # Classification
#predict(fit, newdata, type = "prob") # Raw probability
# **No need to remove features

```

### Offsite Questions (KNN Tuning, N-fold CV, Decision Trees)

1.  *(KNN and N -fold Cross Validation)*

    Loan managers often need to take into account an applicant's demographic and socioeconomic proles in deciding whether to approve a loan to the applicant, to minimize losses due to defaults. In this exercise we will build and evaluate a classier based on the German Credit Data to predict whether an applicant is considered as having good or bad credit risk. The features or predictors include (1) loan duration (in months), (2) credit amount, (3) Installment rate in percentage of disposable income and (4) age in years.

    \(a\) Read and explore the data from the file `German_credit.csv`.

    ```{r}
    data <- read.csv("German_credit.csv")
    head(data)
    str(data)
    ```

    \(b\) Standardize the input features.

    ```{r}
    X <- scale(data[-1])
    Y <- data$Creditability
    head(X)
    ```

    \(c\) Randomly select 800 customer records to form the training data, and the remaining 200 records will be the test data.

    ```{r}
    set.seed(100)
    n <- dim(data)[1]
    index <- sample(1:n, 800)

    training.X <- X[index, ]
    test.X <- X[-index, ]
    training.Y <- Y[index]
    test.Y <- Y[-index]
    ```

    \(d\) Use 1-nearest neighbor classier for the training data to predict if a loan applicant is credible for the 200 test points. Compute the accuracy of the classier.

    ```{r}
    library(class)
    pred <- knn(training.X, test.X, training.Y, k=1)
    confusion.matrix <- table(test.Y, pred)
    confusion.matrix

    accuracy <- sum(diag(confusion.matrix))/sum(confusion.matrix)
    accuracy
    ```

    \(e\) Use N -folds cross validation with N = 5 to find the average accuracy for the 1-nearest neighbor classifier.

    ```{r}
    library(class)
    set.seed(100)

    data <- read.csv("German_credit.csv")
    X <- scale(data[-1])
    Y <- data$Creditability

    n_folds <- 5
    n <- length(Y)
    folds_j <- sample(rep(1:n_folds, length.out = n ))

    accuracy <- numeric(n_folds)
    for (j in 1:n_folds) {
      test_j <- which(folds_j == j) 
      test.y <- Y[test_j] 
      train.X <- X[-test_j, ]  
      test.X <- X[test_j, ]  
      train.y <- Y[-test_j] 
        
      knn.pred <- knn(train.X, test.X, train.y, k = 1) 
        
      confusion.matrix <- table(test.y, knn.pred)
      accuracy[j] <- sum(diag(confusion.matrix))/sum(confusion.matrix)
    }

    ave.accuracy <- round(mean(accuracy), digits = 3)
    ave.accuracy
    ```

    \(f\) Repeat question 1e for K-nearest neighbor classifiers where K = 1, 2, ...100.

    ```{r}

    ### FULL KNN WITH N-FOLDS VALIDATION
    library(class)
    set.seed(100)

    data <- read.csv("German_credit.csv")
    X <- scale(data[-1])
    Y <- data$Creditability

    k <- 100
    n_folds <- 5
    n <- length(Y)
    folds_j <- sample(rep(1:n_folds, length.out = n ))
    ave.accuracy <- numeric(length(k))

    for (i in 1:k) {
      accuracy <- numeric(n_folds)
      for (j in 1:n_folds) {
        test_j <- which(folds_j == j) 
        test.y <- Y[test_j] 
        train.X <- X[-test_j, ]  
        test.X <- X[test_j, ]  
        train.y <- Y[-test_j] 
        
        knn.pred <- knn(train.X, test.X, train.y, k = i) 
        
        confusion.matrix=table(test.y, knn.pred)
        accuracy[j] <- sum(diag(confusion.matrix))/sum(confusion.matrix)
      }
      ave.accuracy[i] <- round(mean(accuracy), digits = 3)
    }

    ave.accuracy
    ```

    \(g\) Compare the 100 classifiers above, which few values of K give the best average accuracy?

    ```{r}

    library(glue)
    index = which(ave.accuracy == max(ave.accuracy))
    plot(x = 1:k, ave.accuracy)
    abline(v = index, col = "red")
    glue("most accurate k = {index} with average accuracy = {ave.accuracy[index]}")
    ```

2.  *(Decision Trees)*

    Consider the famous Iris Flower Data set which was rst introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation: the length and the width of the sepals and petals (in cm).

    \(a\) Use decision tree method to predict Iris species based on all four features.

    ```{r}
    iris <- read.csv("iris.csv")
    head(iris)
    str(iris)
    dim(iris)

    library(rpart)
    fit <- rpart(class ~ sepal.length + sepal.width + petal.length + petal.width,
                  data = iris,
                  method="class",
                  parms=list(split='information'),
                  control = rpart.control( minsplit =1))
    ```

    \(b\) Visualize the decision tree above, using the `rpart.plot` function.

    ```{r}
    library(rpart.plot)
    rpart.plot(fit, type=4, extra=2)
               
    # The fitted tree is given in the figure below.
    # If the measurement of petal length is less than 2.5 cm 
    # then the flower is of Iris-setosa
    # If the petal length is ≥ 2.5 cm with the petal width is ≥ 1.8 cm 
    # then high chance (45/46) it will be an Iris-virginica.
    # If the petal length is ≥ 2.5 cm with the petal width is < 1.8 cm
    # then we continue to check if the petal length is in the interval
    # [2.5, 5]. If yes, then high chance (47/48) it is Iris-versicolor.
    ```

    \(c\) What are the more important features in the fitted tree above?

    ```{r}
    # It seems the sepal length and sepal width 
    # are not important in the classification while
    # the petal length and petal width are more important.
    ```

## T7 (Decision Trees 2)

### Offsite Questions (Decision Trees, N-fold CV)

(DT and N-fold Cross Validation)

Consider the famous Iris Flower Data set which was first introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor).

Four features were measured from each observation: the length and the width of the sepals and petals (in cm).

![](images/clipboard-2361412568.png)

In Tutorial 6, we used the decision tree method to predict Iris species based on all four features. We now would want to use N-fold CV to check on how good the method is, based on the accuracy.

We’ll use 5-fold CV where we would want to keep the ratio of the three species the same (1:1:1) in both the training set and test set.

What’s the average accuracy of the decision tree method?

```{r}
df <- read.csv("iris.csv")
str(df)
table(df$class)
```

```{r}
set.seed(1101)
n_folds <- 5
folds_indexes_1 <- sample(rep(1:n_folds, length.out = 50))
print(table(folds_indexes_1))
folds_indexes_2 <- sample(rep(1:n_folds, length.out = 50))
print(table(folds_indexes_2))
folds_indexes_3 <- sample(rep(1:n_folds, length.out = 50))
print(table(folds_indexes_3))
```

```{r}
folds_indexes = c(folds_indexes_1, folds_indexes_2, folds_indexes_3)
print(table(folds_indexes))
print(folds_indexes)

accuracy <- numeric(n_folds)
for (i in 1:n_folds) {
  test_indexes <- which(i == folds_indexes)
  fit <- rpart(class ~ ., method = "class",
               data <- df[-test_indexes,])
  prediction = predict(fit, df[test_indexes,],
                       type = "class")
  accuracy[i] <- mean(prediction == df$class[test_indexes])
}

print(accuracy)
print(mean(accuracy))
# The average accuracy of the decision tree method is 0.94
```

### Onsite Questions (Decision Trees Tuning, N-fold CV)

(Decision Trees)

Customer churn is the loss of clients or customers. Banks, telephone service companies, Internet service providers, pay TV companies, and insurance firms often use customer churn analysis and customer churn rates as one of their key business metrics.

This is because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.

In this problem, a wireless telecommunications company wants to predict whether a customer will churn (switch to a different company) in the next six months. With a reasonably accurate prediction of a person's churning, the sales and marketing groups can attempt to retain the customer by offering various incentives. Variables of our concern are listed below.

\(i\) Age (years)

\(ii\) Married (true/false)

\(iii\) Duration as a customer (years)

\(iv\) Churned contacts - Number of the customer's contacts that have churned (count)

\(v\) Churned (true/false) — Whether the customer churned

\(a\) Build a decision tree for predicting customer churn, using the feature variables Age, Married, Cust_years, and Churned_contacts.

```{r}
set.seed(100)
df <- read.csv("churn.csv")
str(df)
df$Churned <- as.factor(df$Churned)
df$Married <- as.factor(df$Married)


library(rpart)
fit <- rpart(Churned ~ Age + Married + Cust_years + Churned_contacts, 
             data = df,
             method = "class",
             parms = list(split = 'information'),
             control = rpart.control(minsplit = 1))

library(rpart.plot)
rpart.plot(fit, type=4, extra=2)


```

\(b\) Consider the decision tree in part (a) to predict binary variable Churned. Use the tree to predict customer churn for the following observations.

![](images/clipboard-683929762.png)

```{r}
index <- c(2821, 96, 5085, 758, 487, 987, 6061, 3745, 4709, 2769)
df[index,]


pred <- predict(fit, newdata = df[index,], type = "class")
pred
confusion_matrix <- table(actual = df$Churned[index], predicted = pred)
confusion_matrix
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy
```

Recall that we studied N-fold cross-validation for the K-nearest neighbor classifier, in which the value of k is varied to control the complexity of the decision surface for the classifier.

For decision tree classification, when fitting a tree using function `rpart()`, we use the argument `control = rpart.control(minsplit = 1)` where `minsplit = 1` is to specify the minimum number of observations that must exist in a node in order for a split to be attempted. By default, `minsplit = 20`. This `minsplit` argument helps to draft the complexity of a tree, complex with many layers and branches or simple with few layers and less branches.

For this control = `rpart.control()`, there is a similar complexity parameter exists, which is denoted as `cp` where by default `cp = 0.01`:

```         
control = rpart.control(cp = 0.01)
```

Heuristically, smaller values of `cp` correspond to decision trees of larger sizes, and hence more complex decision surfaces.

For this problem, we will investigate n-fold cross-validation for a decision tree classifier.

Consider the data set "bank-sample.csv" we discussed in the lectures. For this exercise, we will fit a decision tree with `subscribed` as outcome; and `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, and `poutcome` as 8 feature variables. **We want to find the best cp value in terms of mis-classification error rate.**

1.  Randomly split the entire data set into 10 mutually exclusive data sets.

2.  Let cp take on the values $10^k$ for k=−5,−4,...,0,...,3,4,5.

3.  At each cp value, run the following loop for j=1,2,...,10.

    -   i\. Set the jth group to be the test set.

    -   ii\. Fit a decision tree on the other 9 sets with the value of cp.

    -   iii\. Predict the class assignment of `subscribed` for each observation of the test set.

    -   iv\. Calculate the number of mis-classification(s) by comparing predicted versus actual class labels in the test set.

4.  Determine the best cp value in terms of mis-classification error rate.

**Note:** mis-classification error rate is the complement of the accuracy.

```{r}
df <- read.csv("bank-sample.csv")
str(df)
col_names <- c("subscribed", "job", "marital", "education", "default", "housing", "loan", "contact", "poutcome")
df_new <- df[,col_names]
str(df_new)
```

```{r}
set.seed(100)
cp_values <- 10^(-5:5)
print(cp_values)
```

```{r}
library(rpart)
library(rpart.plot)
index <- 1
cp_err <- numeric(11)
n_folds <- 10
folds_indexes <- sample(rep(1:n_folds, length.out = dim(df_new)[1])) #nrow(df_new) the same 
for (cp_val in cp_values) {
  err <- numeric(10)
  for (j in 1:10) {
    test_indexes <- which(folds_indexes == j)
    fit <- rpart(subscribed ~ .,
                 method = "class",
                 data = df_new[-test_indexes,],
                 control = rpart.control(cp = cp_val))
    prediction <- predict(fit,
                          df_new[test_indexes,],
                          type = "class")
    err[j] <- mean(prediction != df_new$subscribed[test_indexes])
  }
  cp_err[index] <- mean(err)
  index <- index + 1
}
```

```{r}
plot(log(cp_values, base = 10), cp_err, type = 'b')
best.cp <- cp_values[which(cp_err == min(cp_err))]
print(best.cp)

# From the plot, we can observe that cp = 0.01 gives the lowest misclassification error rate.
```

```{r}
fit <- rpart(subscribed ~ ., 
             method = "class", 
             data = df_new, 
             control = rpart.control(cp = 0.01))
rpart.plot(fit, type = 4, extra = 2, clip.right.labs = FALSE, varlen = 0, faclen = 0)

```

## T8 (Naive Bayes)

### Offsite Questions (Naive Bayes, Decision Trees, ROC, AUC)

Consider the data set "Titanic.csv" again.

\(a\) Fit a decision tree on all the three feature variables, called M2, which uses `minsplit = 1` and information gain.

\(b\) Plot the tree M2.

\(c\) Plot the ROC curves and derive the AUC values for the two classifiers (naive Bayes from on-site question and decision tree). Which classifier has a larger AUC value?

```{r}
df <- read.csv("Titanic.csv")
str(df)

library(rpart)
library(rpart.plot)
library(e1071)
library(ROCR)

set.seed(100)
M1 <- naiveBayes(Survived ~ Class + Sex + Age, df)
M2 <- rpart(Survived ~ Class + Sex + Age, 
            data = df,
            method = "class",
            control = rpart.control(minsplit = 1),
            parms = list(split = 'information'))
rpart.plot(M2, type = 4, extra = 2, clip.right.labs = FALSE, varlen = 0, faclen = 0)

# Plot the ROC curves and derive the AUC values for the two classifiers (naive Bayes from on-site question and decision tree). Which classifier has a larger AUC value?

pred.M1 <- predict(M1, df, type = 'raw')
score <- pred.M1[,2]
survived_numeric <- ifelse(df$Survived == "Yes", 1, 0)
pred_nb <- prediction(score, survived_numeric)
roc_nb <- performance(pred_nb, measure = "tpr", x.measure = "fpr")
plot(roc_nb, col = "red")

pred.M2 <- predict(M2, df, type = 'prob')
score2 <- pred.M2[,2]
pred_dt <- prediction(score2, survived_numeric)
roc_dt <- performance(pred_dt, measure = "tpr", x.measure = "fpr")
plot(roc_dt, col = "blue")

plot(roc_nb, col = "red", lwd = 2, main = "ROC Curves: Naive Bayes vs Decision Tree")
# Add ROC curve for Decision Tree
plot(roc_dt, col = "blue", lwd = 2, add = TRUE)
legend(x = "bottomright", 
       legend = c("Naive Bayes", "Decision Tree"), 
       col = c("red", "blue"), 
       lwd = 2,
       lty = 1)

auc_nb <- performance(pred_nb, measure = "auc")
auc_nb_value <- as.numeric(auc_nb@y.values) # Extract the AUC value
print(auc_nb_value)
# 0.7164944


auc_dt <- performance(pred_dt, measure = "auc")
auc_dt_value <- as.numeric(auc_dt@y.values) # Extract the AUC value
print(auc_dt_value)
# 0.7262628

# From AUC values, the decision tree classifier is
# slightly better than the Naive Bayes classifier

```

### Onsite Questions (Naive Bayes Mathematics)

Data set "Titanic.csv" provides information on the fate of passengers on the fatal maiden voyage of the ocean liner Titanic. It includes the variables: economic status (class), sex, age, and survival. We will train a naive Bayes classifier using this data set, and predict survival.

1.  Compute the probabilities $P(Y=1)$ (survived) and $P(Y=0)$ (did not survive).

2.  Compute the conditional probabilities $P(X_i = x_i | Y = 1)$ and $P(X_i = x_i | Y = 0)$, where i=1,2,3,4 for the feature variables X={class,sex,age}

3.  Predict survival for an adult female passenger in 2nd class cabin.

4.  Compare your prediction in (c) with the one performed by the `naiveBayes()` function in package `e1071`

```{r}
df <- read.csv("Titanic.csv")
str(df)
prop.table(table(df$Survived))
```

```{r}
tab <- table(df[,c("Survived", "Class")])
print(tab)
class.pro <- prop.table(tab, margin = 1)
print(class.pro)

# P (Class = 1st | Y = 1) = 0.286
# P (Class = 2nd | Y = 1) = 0.166
# P (Class = 3rd | Y = 1) = 0.250
# P (Class = Crew | Y = 1) = 0.298

# P (Class = 1st | Y = 0) = 0.0819
# P (Class = 2nd | Y = 0) = 0.112
# P (Class = 3rd | Y = 0) = 0.354
# P (Class = Crew | Y = 0) = 0.452


```

```{r}
tab <- table(df[,c("Survived", "Sex")])
print(tab)
sex.pro <- prop.table(tab, margin = 1)
print(sex.pro)

# P (Class = Female | Y = 1) = 0.484
# P (Class = Male | Y = 1) = 0.516

# P (Class = Female | Y = 0) = 0.085
# P (Class = Male | Y = 0) = 0.915

```

```{r}
tab <- table(df[,c("Survived", "Age")])
print(tab)
age.pro <- prop.table(tab, margin = 1)
print(age.pro)

# P (Class = Adult | Y = 1) = 0.920
# P (Class = Child | Y = 1) = 0.080

# P (Class = Adult | Y = 0) = 0.965
# P (Class = Child | Y = 0) = 0.035
```

```{r}
# Predict survival for an adult female passenger in 2nd class cabin.
# P(Y = 1) *.                   # Survival 
# P(Class = 2nd|Y = 1) *        # 2nd Class
# P(Sex = Female|Y = 1) *.      # Female
# P(Age = Adult|Y = 1)          # Adult
# = 0.323035 * 0.16596343 * 0.48382560 * 0.91983122
# = 0.02385937

# P(Y = 0) *                    # Survival 
# P(Class = 2nd|Y = 0) *        # 2nd Class
# P(Sex = Female|Y = 0) *       # Female
# P(Age = Adult|Y = 0)          # Adult
# = 0.676965 * 0.11208054 * 0.08456376 * 0.96510067
# = 0.006192319

# Thus, the ratio between probability of survived 
# and not survived is 0.02385937/0.006192319 (3.85)
# which is nearly 4 times. 

# We predict that an adult female passenger in 2nd class
# cabin will survive.
```

```{r}
# 4. Compare your prediction in (c) with naiveBayes() function
library(e1071)
M1 <- naiveBayes(Survived ~ Class + Sex + Age, df)
newdata <- data.frame(Class = "2nd", Sex = "Female", Age = "Adult")

pred.label <- predict(M1, newdata)
print(pred.label)

pred.prob <- predict(M1, newdata, "raw")
print(pred.prob)

print(pred.prob[1,"Yes"] / pred.prob[1,"No"])

# Ratio is same as calculated.

```

## T9 (Logistic Regression)

```{r}
# M1 = glm(y ~ x1 + x2 + ..., data = ..., family = binomial)

# predict(M1, newdata = ..., type = "response")
#    Get probability
# predict(M1, newdata = ..., type = "link")
#    Get Log-odds (Rarely used)

# Coefficient of numerical features:
# If other regressors are fixed, when X increases by one
# unit, the LOG-odds of Y increases/decreases by C units.
# Thus, odds of Y will be e^C = ... TIMES larger/smaller.
```

### Offsite Questions (Logistic Regression, ROC, AUC)

Consider data set `Titanic.csv` again which was mentioned in Tutorial 8.

1.  Perform logistic regression using all the feature variables to predict the survival status, called model M2.

    ```{r}
    df <- read.csv("Titanic.csv")
    str(df)
    df$survived_numeric <- ifelse(df$Survived == "No", 0, 1)

    M2 <- glm(survived_numeric ~ Class + Sex + Age, data = df, family = binomial)
    summary(M2)
    ```

2.  Write down the fitted equation of model M2.

    ```{r}
    # p_hat = predicted probability of survival
    # log(p_hat/(1-p_hat)) = 2.0438 - 1.0181 * I(Class=2nd) - 1.7778 * I(Class = 3rd) 
    # - 0.8577 * I(Class = Crew) - 2.4201 * I(Sex = Male) + 1.0615 * I(Age = Child)
    ```

3.  Interpret the coefficient of the variable ‘Sex’ in M2.

    ```{r}
    # Female is the reference, Male is indicated by indicator.
    # Meaning, given the same condition on class and age, when 
    # comparing to a female, the LOG-odds of survival for a 
    # male is less than that of a female by 2.4201.
    # Thus, odds of survival of a male passenger will be less
    # than that of a female by e^2.4201 = 11.25 TIMES

    ```

4.  Interpret the coefficient of the variable ‘Age’ in M2.

    ```{r}
    # Adult is the reference, Child is indicated by indicator.
    # Meaning, given the same condition on class and sex, when # comparing to a adult, the LOG-odds of survival for a 
    # child is higher than that of an adult by 1.0615.
    # Thus, odds of survival of a child passenger will be more
    # than that of an adult by e^1.0615 = 2.89 TIMES
    ```

5.  Obtain and compare the ROC curves and AUC for the two classifiers: naive Bayes (from Tutorial 8) and logistic regression.

    ```{r}
    library('e1071')
    M1 <- naiveBayes(survived_numeric ~ Class + Sex + Age, data = df)
    M2 <- glm(survived_numeric ~ Class + Sex + Age, data = df, family = binomial)

    pred.M2 <- predict(M2, newdata = df, type = 'response')
    pred_lr <- prediction(pred.M2, df$survived_numeric)
    roc_lr <- performance(pred_lr, measure = "tpr", x.measure = "fpr")
    plot(roc_lr, col = "red")

    pred.M1 <- predict(M1, newdata = df, type = 'raw')
    pred_nb <- prediction(pred.M1[, 2], df$survived_numeric) # pred.M1[,2] is score 
    roc_nb <-  performance(pred_nb, measure = "tpr", x.measure = "fpr")
    plot(roc_nb, col = "blue", add = TRUE)

    legend(x = "bottomright", 
           c("logistic regression", "naiveBayes"),
           col = c("red", "blue"), 
           lty = 1)

    auc_lr = performance(pred_lr, measure ="auc")@y.values[[1]]
    auc_lr # 0.7597259

    auc_nb <- performance(pred_nb, "auc")@y.values[[1]]
    auc_nb #0.7164944

    # LR is slightly better than NB in 
    # predicting survival status 


    ```

### Onsite Questions (Logistic Regression, Threshold)

(Data set `Smarket.csv` contains data on percentage returns for the S&P 500 stock index over 1250 days, from the beginning of 2001 until the end of 2005. For each date, the data contains the trading volume and the percentage returns for each of the five previous trading days, `Lag1` through `Lag5`.

1.  Find the proportion of the days that the direction is up.

    ```{r}
    df <- read.csv("Smarket.csv")
    str(df)
    df$X <- NULL
    tab <- prop.table(table(df$Direction))
    print(tab)
    # Proportion of days where direction is up is 0.5184
    ```

2.  Fit a logistic regression model (called model M3) that helps to predict the direction of S&P 500 index, based on trading volume and the percentage returns for each of the five previous trading days, `Lag1` through `Lag5`.

    ```{r}
    df$num_dir <- ifelse(df$Direction == "Up", 1, 0)
    str(df)
    M3 <- glm(num_dir ~ Volume + Lag1 + Lag2 + Lag3 + Lag4 + Lag5,
              data = df,
              family = binomial)
    summary(M3)

    ```

3.  Consider δ = 0.5184 as the border line to predict the direction be up or down, calculate the accuracy of model M3 when predicting for the days in the data set given.\
    *Hint: To recall on what the role of δ is, please revise Tutorial 5 question 2.*

    ```{r}
    sigma <- 0.5184
    pred.prob <- predict(M3, df, type = "response")
    head(pred.prob)

    pred.lab <- ifelse(pred.prob >= sigma, 1, 0)
    head(pred.lab)

    acc <- mean(pred.lab == df$num_dir)
    print(acc)
    # acc got 0.5352

    ```

4.  (Extra) Write R code to plot the ROC curve for model M3. Derive and report AUC value of it.

    ```{r}
    library(ROCR)
    pred <- prediction(pred.prob, df$num_dir)
    roc <- performance(pred, measure = "tpr", x.measure = "fpr")
    plot(roc, col = "blue")

    auc <- performance(pred , "auc")@y.values[[1]]
    print(auc) # 0.5387341


    ```

## T10 (K-Means Clustering)

Use kmeans() function: `kout <- kmeans(data, centers = n)`

To get final centroid positions: `kout$centers`

To get size of each cluster: `kout$size`

To get WSS for each cluster: `kout$withinss`

To get Total WSS: `sum(kout$withinss)` OR `kout$tot.withinss`

### Offsite Questions (K-Means Mathematics)

1.  Suppose we have data for five objects on two features:

| Object | x1  | x2  |
|--------|-----|-----|
| A      | 1   | 1   |
| B      | 1.5 | 2   |
| C      | 3   | 4   |
| D      | 3.5 | 5   |
| E      | 4.5 | 5   |

We set k=2 to cluster the five data points into two clusters, P and Q, and initialize the algorithm with the centroids $(x_1, x_2, P) = (2, 2)$ and $(x_1, x_2, Q) = (4, 4)$.

\(a\) Fill up the following table to identify the objects in each cluster during the first iteration of the kkk-means algorithm:

| Cluster | Object(s) |
|---------|-----------|
| P       |           |
| Q       |           |

```{r}
x1 = c(1, 1.5, 3, 3.5, 4.5)
x2 = c(1,2,4,5,5)

plot(x1, x2, pch = 20, col = "blue")

text(1.1,1.1,"A")
text(1.6, 2.2, 'B')
text(3.1, 4.1, 'C')
text(3.63, 5, 'D')
text(4.35, 5, 'E')

# Adding the starting centroids 
points(2,2, pch = 2, col = 'red')
text(2.2, 2.1, 'C-P')
points(4,4, pch = 10, col = 'darkgreen')
text(4,3.8, 'C-Q')

# 2.458333

```

\(b\) Compute the new centroids for the two clusters based on cluster assignment in (a).

```{r}
# Adding the new centroids after the first iteration:
plot(x1, x2, pch = 20, col = "blue")

points(1.25, 1.5, col = 'red', pch = 2)
text(1.35, 1.4, 'C-P-new')
points(11/3, 14/3, col = 'darkgreen', pch = 10)
text(11/3, 4.5, 'C-Q-new')
```

\(c\) Based on the centroids computed in (b), identify the objects in each cluster during the second iteration of the k-means algorithm.

```{r}
data = data.frame(x1, x2)
data
kout = kmeans(data, centers = 2)
```

\(d\) Calculate the Within Sum of Squares (WSS) for the clustering assignment in (c).

```{r}
kout$withinss
kout$tot.withinss
```

2.  (K-Means) Consider data set `hdb-2012-to-2014.csv` which was extracted from the published data. The file has information on the HDB resale flats from Jan 2012 to Dec 2014.

\(a\) Load data into R. Use k-means algorithm to pick an optimal value for k in terms of **WSS**, based on two variables, `resale_price` and `floor_area_sqm`.

```{r}
df <- read.csv("hdb-2012-to-2014.csv")
# str(df)
df$X <- NULL
numeric_df <- df[sapply(df, is.numeric)]
numeric_df <- scale(numeric_df)
# str(numeric_df)

K <- 20
wss <- numeric(K)
for (k in 1:K) {
  kout <- kmeans(numeric_df[,c("resale_price", "floor_area_sqm")], centers = k)
  wss[k] <- kout$tot.withinss
}
print(wss)

plot(1:K, wss, type="b")
# k = 3 is a good choice, 
# because the reduction in WSS 
# from k = 3 to k = 4
# is not significant
```

\(b\) With the optimal k in part (a), plot the data points in the k clusters determined.

```{r}
kout <- kmeans(numeric_df, centers = 3)
plot(
  df$floor_area_sqm,
  df$resale_price,
  col = kout$cluster
)
```

```{r}
unscaled_kout <- kmeans(df[,c("floor_area_sqm", "resale_price")], centers = 3)
plot(
  df$floor_area_sqm,
  df$resale_price,
  col = unscaled_kout$cluster
)

```

### Onsite Questions (K-Means Clustering)

Consider the famous Iris Flower Data set which was first introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of observations from flowers of Iris species. For each observation, four features were measured: the flower's length and width of the sepals and petals (in cm).\
The data set is given in a file named `iris.csv`.

1.  Use K-means clustering method to cluster all the flowers into k groups where k=1,2,3,…,10 where for each value of k the value of **WSS** - the within sum of squares is obtained.

    ```{r}
    df <- read.csv("iris.csv")
    str(df)
    df$class <- NULL
    df <- scale(df)

    wss_list <- numeric(10)
    set.seed(10)
    for (k in 1:10) {
      kout <- kmeans(df, centers = k)
      wss_list[k] <- kout$tot.withinss
      }
    print(wss_list)
    ```

2.  Write code to obtain the plot of **WSS** against k. Which value of k would you choose as the number of clusters for all the observations in the data set? Explain.

    ```{r}
    plot(1:10, wss_list, 
         type = 'b',
         col = "black",
         xlab = "Number of Clusters", 
         ylab = "Within Sum of Squares")

    # k = 3 is a good choice, 
    # because the reduction in WSS 
    # from k = 3 to k = 4
    # is not significant

    ```

3.  With the value of k chosen above, report the centroids of all the clusters and the number of the observations in each cluster.

    ```{r}
    kout <- kmeans(df, centers = 3)
    kout$centers
    kout$size

    ```

## T11 (Association Rules)

Machine learning concept used in Market Based Analysis

-   Patterns of co-occurrence

-   Link between products purchased together

Rules **do not** extract an individual’s preference, rather find relationships between set of elements of every distinct transaction. Different from collaborative filtering.

**Support:** Measure of how frequent an itemset is in all the transactions.

![](images/clipboard-2372333094.png)

**Confidence:** Measure of likeliness of occurrence of buying a specific item given that the user is buying another item.

![](images/clipboard-1735566391.png)

**Lift:** The rise in probability of having {Y} in the cart with the knowledge of {X} being present (compared to no knowledge on {X} being present)

![](images/clipboard-832366.png)

Lift \< 1 : Having {X} on the cart does not increase the chances of occurrence of {Y} on the cart

Lift \> 1 :  Vouches for high association between {Y} and {X}. Higher value indicates greater chances of preference to buy {Y} if the customer has already bought {X}

**Leverage:** Similar notion to Lift, except instead of using Ratios, it uses differences

![](images/clipboard-3216104608.png)

Similar notion to Lift, except instead of using Ratios, it uses differences

Leverage \> 0 : Strong relationship for {X} -\> {Y}

Leverage = 0 : Statistically Independent ({X} does not affect {Y})

Leverage \< 0 : Negative relationship between {X} and {Y}

### Offsite Questions (Association Rules Mathematics)

1.  A local retailer has a database that stores 10,000 transactions of last summer. After analyzing the data, a data science team has identified the following statistics:

-   {battery} appears in 6000 transactions
-   {sunscreen} appears in 5000 transactions
-   {sandals} appears in 4000 transactions
-   {bowls} appears in 2000 transactions
-   {battery, sunscreen} appears in 1500 transactions
-   {battery, sandals} appears in 1000 transactions
-   {battery, bowls} appears in 250 transactions
-   {battery, sunscreen, sandals} appears in 600 transactions

(a) What are the support values of the preceding itemsets?

    ![](images/clipboard-4287054081.png)

(b) Assuming the minimum support is 0.05, which itemsets are considered frequent?

    ![](images/clipboard-3013610408.png)

(c) What are the confidence values of {battery} → {sunscreen} and {battery, sunscreen} → {sandals}? Which of these two rules is more interesting, i.e., has higher values of confidence?

![](images/clipboard-3457055832.png)

2.  Suppose for three products ( A, B ) and ( C ), support({A}) = 0.6, support({B}) = 0.6, confidence({B} → {A}) = 0.9 and confidence({C} → {A, B}) = 0.5. Compute the following quantities:
    -   Lift({A} → {B})

    -   Leverage({A} → {B})

    -   Confidence({A} → {B})

    -   Lift({A, B} → {C})

        ![](images/clipboard-1247641227.png)

### Q&A

1.  Can you explain the order function?

    Returns indexes of sorted version of vector passed to it

    ```{r}
    x <- c(9, 18, 5, 15, 12)
    order(x)
    ```

    ```{r}
    x <- c(9, 18, 5, 15, 12)
    sorted_indexes <- order(x)
    x[sorted_indexes]
    ```

    ```{r}
    x <- c(9, 18, 5, 15, 12)
    x[order(x)]
    ```

    ```{r}
    df <- data.frame(
      x = c(9, 18, 5, 15, 12),
      y = c("A", "B", "C", "D", "E"))

    df

    sorted_index <- order(df$x)
    df[sorted_index, ]

    sorted_index <- order(df$x, decreasing = TRUE)
    df[sorted_index, ]


    ```

2.  Is it possible to give a template code for every topic?

    -   Template code is provided in the Tutorial R Code files on Canvas

    -   Can also look through previous tutorial slides for sample code

    -   Go through the code, understand the logic and be prepared to modify it where necessary to suit the final’s questions’ needs

    ```{r}
    #| eval: false

    # (3.e) Repeat question 3d for k = 3, 5, 10. Which k gives the best precision?

    k_values = c(3, 5, 10)
    for (k_val in k_values) {
      knn.pred <- knn(train.X, test.X, train.Y, k = k_val) 
      conf.matrix <- table(test.Y, knn.pred)
      precision <- conf.matrix[2,2]/sum(conf.matrix[,2])
      print(paste(k_val, precision))
    }

    ```

    ```{r}
    #| eval: false


    # (1.e.) Use 5-fold cross validation to get average accuracy (K = 1).

    n_folds <- 5
    folds <- rep(1:n_folds, length.out = dim(data)[1])
    folds <- sample(folds)
    accuracies <- numeric(n_folds)

    for (curr_fold in 1:n_folds) {
      test_index <- which(folds == curr_fold)
      train.X <- standardised.X[-test_index,]
      test.X <- standardised.X[test_index,]
      train.Y <- data$Creditability[-test_index]
      test.Y <- data$Creditability[test_index]
      pred <- knn(train.X, test.X, train.Y, k = 1)
      accuracies[curr_fold] <- mean(test.Y == pred)
    }
    ```

3.  If a NB question has numeric inputs in the dataset given, are we expected to change it to categorical by using as.factor? Even if qn doesn’t say anything? Or qn wud usually specify?

    -   Naive Bayes can actually handle numeric features, but it is not taught in this course

    -   As such, any features for Naive Bayes should be categorical

    -   Do not change it to categorical yourself, question will usually specify

    1.  Can you explain conditional entropy and its calculation?
        -   Entropy is a measure of the randomness

            ![](images/clipboard-451619793.png)

        -   Imagine using this to measure entropy of flipping a coin

            ![](images/clipboard-157113776.png)

        -   Formula for conditional entropy

            ![](images/clipboard-3492415708.png)

# Midterm Practice

1.  (50 points) The data file patient_satisfaction.csv is a random sample collected from a\
    hospital about the satisfactory of patients about the hospital's service when they were discharged.\
    The data set includes information about patient's age, the score of illness severity, the anxiety\
    score, and the status if the patient went through surgery (1) or only had medical treatment (0).\
    The given file contains columns with names listed below.

    ![](images/clipboard-3191248250.png)

    please run function setwd() in a separate line (if you need it) when importing the data set\
    into R. Use set.seed(310)

    ```{r}
    # setwd("/Users/rayana/Desktop/DSAFIN")
    set.seed(310)

    ## Part 1 ##

    data <- read.csv("Patient Satisfaction.csv")
    #Q1
    names(data)[4] <- "Surgical"
    #Q2
    prop.table(table(data$Surgical))
    # 56% patients had surgery
    #Q3
    hist(data$Satisfaction, probability = TRUE, 
         main = "Histogram of Satisfaction with Normal Density Curve",
         xlab = "Satisfaction", col = "lightblue", border = "black")

    x_values <- seq(min(data$Satisfaction), max(data$Satisfaction), length = 100)
    y_values <- dnorm(x_values, mean = mean(data$Satisfaction), sd = sd(data$Satisfaction))
    lines(x_values, y_values, col = "red", lwd = 2)

    ```

    ```{r}
    #Q4
    qqnorm(data$Satisfaction, pch = 20)
    qqline(data$Satisfaction, col = "red")

    # Comments:
    # BOTH LEFT TAIL AND RIGHT TAIL ARE NOT CLEARLY CONTRADICT FROM NORMAL 
    # SOME MIGHT SAY: RIGHT TAIL POSSIBLY/LIGHTLY SHORTER THAN NORMAL

    ```

    ```{r}
    #Q5

    bp <- boxplot(data$Satisfaction ~ data$Surgical)

    # outliers <- bp$out
    # length(outliers)
    # index <- which(data$Satisfaction %in% outliers)
    # fev[index,]

    # the median point of satisfaction for patients got surgical is lower.
    # 2 boxes are overlapping. 
    # Hence, NO clear difference in the satisfaction points of patients with or without surgical. 
    # no outlier for both groups 

    ```

    ```{r}
    #Q6
    #unsurge = data$Satisfaction[which(data$Surgical==0)]
    #surge = data$Satisfaction[which(data$Surgical==1)] 

    plot(data$Satisfaction ~ data$Age, type = "n")
    points(data$Satisfaction[which(data$Surgical == 1)] ~ data$Age[which(data$Surgical==1)], col = "red", pch = 20)
    points(data$Satisfaction[which(data$Surgical == 0)] ~ data$Age[which(data$Surgical==0)], col = "blue", pch = 20)
    legend(25, 50, legend = c("Surgical", "No surgical"), col = c("red","blue"), pch=c(20,20))

    cor(data$Satisfaction, data$Age)
    # -0.87
    # strong negative association, quite linear  
    # the variablity of satisfaction is quite stable when age changes 
    ```

    ```{r}
    ## Part 2 ##
    #Q7
    data$Surgical <- as.factor(data$Surgical)
    M <- lm(Satisfaction ~ ., data = data)
    summary(M)
    # (Surgical = 1) and Anxiety variables are not significant at significance level 0.1

    #Q8
    A <- data.frame(Age = 35, Severity = 45, Anxiety = 2.5, Surgical = "0")
    B <- data.frame(Age = 60, Severity = 40, Anxiety = 3, Surgical = "1")
    predict(M, newdata = A)
    predict(M, newdata = B)

    # REPORT THE OUTCOMES
    # A: 82.19378 
    # B: 58.83136
    ```

    ```{r}
    ## Part 3 ##
    #Q9
    data$S <- ifelse(data$Satisfaction <= 70, "Not Good", "Good")
    #Q10
    data.X <- scale(data[,c("Age", "Severity", "Anxiety")])
    #Q11
    train.index <- sample(nrow(data), size = 15)

    ```

    ```{r}
    #Q12
    Y <- data$S
    train.X <- data.X[train.index, ]  
    test.X <- data.X[-train.index, ] 
    train.y <- Y[train.index] 
    test.y <- Y[-train.index] 

    library(class)
    k_values <- c(3, 5, 7, 9, 11)
    accuracy <- numeric()  

    for (k in k_values) {  
        knn.pred <- knn(train.X, test.X, train.y, k = k) 
        confusion.matrix <- table(test.y, knn.pred)  
        accuracy <- append(accuracy, sum(diag(confusion.matrix)) / sum(confusion.matrix))  
    }

    print(accuracy)

    ```

    ```{r}
    #Q13
    plot(k_values, accuracy, type = "b")
    #Q14
    # best k is either 3,5,7
    ```

    ```{r}
    #Q15
    scaled_A <- data.frame(
      Age = (A$Age - mean(data$Age)) / sd(data$Age),
      Severity = (A$Severity - mean(data$Severity)) / sd(data$Severity),
      Anxiety = (A$Anxiety - mean(data$Anxiety)) / sd(data$Anxiety)
    )

    knn.pred <- knn(train.X, scaled_A, train.y, k = 3)
    knn.pred

    # REPORT: PATIENT A is predicted as will rank Good for for hospital service. 

    ```

```{r}
### 2
#Q1
annual_salary <- 50000
salary_increase <- 0.05
saving <- 30000
target <- 100000
saved_proportion <- 0.2

function.year <- function(annual_salary, salary_increase, saving, target, saved_proportion) {
  year <- 0
  while (saving < target) {
    annual_salary <- annual_salary * (1 + salary_increase)
    saving <- saving + annual_salary * saved_proportion
    year <- year + 1
  }
 return(year)
}

function.year(annual_salary, salary_increase, saving, target, saved_proportion)
#Alena need to save for 6 years
```

```{r}
#Q2
annual_salary <- 50000
salary_increase <- 0.05
saving <- 30000
target <- 100000
saved_proportion <- 0.2

year <- function.year(annual_salary, salary_increase, saving, target, saved_proportion)
while (year > 5) {
  saved_proportion <- saved_proportion + 0.01
  year <- function.year(annual_salary, salary_increase, saving, target, saved_proportion)
}
print(saved_proportion)
# smallest proportion of annual salary that she should save so that she could have enough
# money for her start-up at the end of 2029 is 25%
```

# Final Practice

### 23/24 Semester 2

![](images/clipboard-1262997958.png)

```{r}
# ===== Question 1 =====

# a) True.
# b) True
# c) False
# d) False. 
# e) True
```

![](images/clipboard-42922271.png)

![](images/clipboard-1716315755.png)

```{r}
# ===== Question 2 =====

# a) Right skewed
# b) Histogram
# c) Scatter Plot 
# d) Box Plot 
# e) A 

```

![](images/clipboard-1551795137.png)

```{r}
# ===== Question 3 =====
df1 <- read.csv('crab.csv')
df1$color = as.factor(df1$color)
df1$spine = as.factor(df1$spine)

```

```{r}
## ---- PART I: LINEAR REGRESSION MODEL ----
hist(df1$satell,col = 'red')
# The histogram for satell is heavily right skewed, 
# there are no gaps in the range of values, and
# there might be suspected outliers in the data

plot(df1$width,df1$satell,pch = 20)
# Comment: No clear association between satell and width.

M1 <- lm(satell ~ color + spine + width + weight, data = df1)
summary(M1)$r.squared
# comment: R squared value of M1 is 0.151, this means that the model can only explain 15.1% of the variance of the dataset

# Width is the most insignificant, it has the highest p-value

```

![](images/clipboard-3024193786.png)

![](images/clipboard-147900429.png)

```{r}
## ---- PART II: LOGISTIC REGRESSION MODEL ----
df1$status <- ifelse(df1$satell >0, 1,0)

table(df1$status)
# There are 111 female crabs that have at least 1 satellite 

CS.table <- table(df1$color,df1$status)
CS.table
# There are 69 female crabs that are of medium colour and has at least a satellite

prop.table(CS.table,margin = 1)
# P(Status = 1 | Color = 2) = 0.75
# P(Status = 1 | Color = 3) = 0.73
# P(Status = 1 | Color = 4) = 0.59
# P(Status = 1 | Color = 5) = 0.32

# Ans: There is a negative association between color and the probability that it has satellites.
#      From the table, as color increases, meaning the darker the crab, 
#      the conditional probability that the crab has at least one satellite decreases. 

M2 <- glm(status ~ width + weight + color, data = df1,family = binomial)
summary(M2)
# Equation: log(p/(1-p)) = -8.6445 + 0.2906 * width 
#                                            + 0.7727 * weight
#                                            + 0.1310 * I(color = 3)
#                                            - 0.1610 * I(color = 4)
#                                            - 1.2453 * I(color = 5)

# Ans: I represents the indicator variable,
#      and p represents the probability that a crab has at least one satellite

# Ans: Coefficient of weight is 0.7727.
#      This means with all other variables kept the same,
#      for every 1kg increase in the weight of a crab, the log odds of that crab having at least 1 satellite increases by 
#      0.7727. Equivalently, for each 1kg increase in weight, 
#      the odds of a crab having at least 1 satellite will be e^0.7727 times higher. 

# Ans: Coefficient of medium color is 0.1310 and coefficient of darker color is -1.2453.
#      This means with all other variables kept the same,
#      the log odds of a crab with medium color having at least 1 satellite will be 0.1310 MORE than a crab with light color, and 
#      the log odds of a crab with dark color having at least 1 satellite be 1.2453 LESS than a crab with light color.
#      This means that the log odds of a crab with medium color having at least 1 satellite will be 0.1310-(-1.2453) = 1.3763 MORE 
#      than a crab with darker color.
#      Equivalently, This means the odds of having at least 1 satellite for a crab with a medium colour is 
#      e^1.3763 = 3.96 times more than that of a crab with darker color.

width <- c(26,30)
weight <- c(2.6,4.0)
color = c("4","2")
spine = c("1","3")
newdata = data.frame(width,weight,color,spine);newdata
log_reg_new_pred <- predict(M2,newdata= newdata,type = 'response')
log_reg_new_pred
# Probability of having satellites for crab A: 68.1%
# Probability of having satellites for crab B: 95.9%

head(df1)
log_reg_pred <- predict(M2,newdata = df1,type = 'response')
library(ROCR)
log_reg_prediction <- prediction(log_reg_pred,df1[,6])
log_reg_perf <- performance(log_reg_prediction, "tpr","fpr")
plot(log_reg_perf,col = 'red')
log_reg_auc <- performance(log_reg_prediction,"auc")@y.values[[1]]
log_reg_auc
# auc = 0.777

delta_values <- log_reg_perf@alpha.values[[1]]
fpr_values <- log_reg_perf@x.values[[1]]
tpr_values <-log_reg_perf@y.values[[1]]
par(mar = c(5 ,5 ,2 ,5))
plot(delta_values ,tpr_values , xlab ="Threshold", xlim =c(0 ,1) ,
     ylab = "True positive rate ", type ="l", col = "blue")
par( new ="True")
plot(delta_values ,fpr_values , xlab ="", ylab ="", axes =F, xlim =c(0 ,1) , type ="l", col = "red" )
axis( side =4) # to create an axis at the 4th side
mtext(side =4, line =3, "False positive rate")
text(0.5 ,0.4, "FPR", col = "red")
text(0.58 ,0.9 , "TPR", col = "blue")

delta_indices <- which(delta_values >=0.59 & delta_values <=0.6)
tpr_values[delta_indices]
fpr_values[delta_indices]

log_reg_pred_binary <- ifelse(log_reg_pred >=0.5,1,0)
log_reg_cm <- table(log_reg_pred_binary,df1[,"status"])
log_reg_acc <- sum(diag(log_reg_cm))/sum(log_reg_cm)
log_reg_acc
# Accuracy 0.711
```

![](images/clipboard-1859428080.png)

```{r}
## ---- PART III: NAIVE BAYES CLASSIFIER ----
library(e1071)
M3 <- naiveBayes(status ~ color + spine + width + weight, data = df1)
nb_pred <- predict(M3,newdata= df1[,c(1,2,3,5)])
nb_cm <- table(nb_pred,df1[,"status"])
nb_acc <- sum(diag(nb_cm))/sum(nb_cm)
nb_acc
# nb_acc: 0.705

predict(M3,newdata =newdata,type = 'raw')
# Probability of having satellites for crab A: 67.6%
# Probability of having satellites for crab B: 99.9%
```

![](images/clipboard-2332687476.png)\

```{r}
#| eval: false
# ===== Question 4 =====
data <- read.csv('penguins-dsa1101.csv')
head(data)
plot(data$bill_depth,data$mass)
# I would choose 2 clusters, just looking at the scatterplots

data.X = scale(data[,c("bill_depth","mass")])
k <- 8 
wss <- numeric(k)
for (i in 1:k){
  kout <- kmeans(data.X,centers = i)
  wss[i] <- kout$tot.withinss
}
wss

plot(1:k,wss,type = 'b')
# I would choose 5 clusters. 

kout <- kmeans(data.X,centers = 5)
kout$centers
# bill_depth       mass
#1  0.1678949 -0.9957828
#2 -0.4620534  1.8340519
#3  1.0700111 -0.2252665
#4 -1.5269045  0.5077693
#5 -1.0217963  1.2903276

kout$size
# [1] 110  31 109  53  39
plot(data$bill_depth,data$mass,col = kout$cluster)
```

### 23/24 Semester 1

![](images/clipboard-637032502.png)

![](images/clipboard-2324260449.png)

![](images/clipboard-3573320976.png)

![](images/clipboard-2103380400.png)

![](images/clipboard-880892070.png)

![](images/clipboard-541912188.png)

![](images/clipboard-1598151284.png)

![](images/clipboard-804333387.png)

![](images/clipboard-593481924.png)

![](images/clipboard-160726913.png)

![](images/clipboard-276878906.png)

![](images/clipboard-2853785811.png)

![](images/clipboard-222807988.png)

![](images/clipboard-1407415003.png)

![](images/clipboard-1908122565.png)

![](images/clipboard-456727249.png)

![](images/clipboard-2977436779.png)

### Canvas Practice Final

![](images/clipboard-9636182.png)

```{r}
# ===== PART 0: DATA PREPARATION =====
set.seed(1101)
## ---- Q1) Import dataset and remove the first 3 columns ----
data = read.csv("data_finals.csv")
data = data[, -c(1:3)]

## ---- Q2) Report the number of developed countries in the dataset ----
table(data$Status)
# Ans: There are 14 developed countries in the dataset.

## ---- Q3) Transform the variable "Status" into numeric binary ----
data$Status = ifelse(data$Status == "Developing", 0, 1)

```

![](images/clipboard-3122530433.png)

```{r}
# ===== PART 1: LINEAR REGRESSION MODEL =====
## ---- Q4) Form a linear model for "Status" and report its R^2 ----
M1 = lm(Status ~., data)
summary(M1)$r.squared
# Ans: The R^2 value of M1 is 0.300.

## ---- Q5) Number of fitted values of M1 are less than 0 ----
sum(M1$fitted.values < 0)
# Ans: There are 24 fitted values that are less than 0.

## ---- Q6) Limitations of fitting a linear model to "Status" ----
# Ans: 1) Status is a categorical variable, which is inappropriate for linear models that assume a quantitative response.
#      2) Linear model predictions can extend beyond the range [0, 1], limiting their interpretation as probabilities.

## ---- Q7) Derive the area under the ROC Curve for M1 ----
library(ROCR)
M1.prob = M1$fitted.values
M1.rocr = prediction(M1.prob, data$Status)
M1.auc = performance(M1.rocr, "auc")@y.values[[1]]; M1.auc
# Ans: The ROC-AUC value for M1 is 0.890.
```

![](images/clipboard-2513850972.png)

```{r}
# ===== PART 2: LOGISTIC REGRESSION MODEL ====
## ---- Q8) Form a logistic regression model for "Status" with its equation ----
M2 = glm(Status ~., data, family = binomial(link ="logit"))
M2$coefficients
# Ans: log[p/(1-p)] = -19.526 + 0.228 * (Life_expectancy)
#                             + 0.000855 * (Adult_mortality)
#                             - 0.472 * (infant_deaths)
#                             + 0.149 * (Alcohol)
# Notation: p is the predicted probability that a country is developed.

## ---- Q9) Report and interpret the coefficient of "Alcohol" in M2 ----
# Ans: The coefficient for Alcohol is 0.149.
#      Each 1L increase in alcohol consumption multiplies the odds of being a developed country by 1.160, 
#      assuming other factors are held constant.

## ---- Q10) Predict the probability that a country is developed ----
new_country = data.frame(Life_expectancy = 83,
                         Adult_mortality = 57,
                         infant_deaths = 2,
                         Alcohol = 3)

predict(M2, new_country, type = "response")
# Ans: The probability that this country is developed is 0.253.

## ---- Q11) Derive the area under the ROC Curve for M2 ----
M2.prob = predict(M2, data, type = "response")
M2.rocr = prediction(M2.prob, data$Status)
M2.auc = performance(M2.rocr, "auc")@y.values[[1]]; M2.auc
# Ans: The ROC-AUC value for M2 is 0.934.

## ---- Q12) Plot TPR and FPR against Threshold for M2 ----
M2.perf = performance(M2.rocr, "tpr", "fpr")
M2.tpr = M2.perf@y.values[[1]]
M2.fpr = M2.perf@x.values[[1]]
M2.threshold = M2.perf@alpha.values[[1]]
plot(M2.threshold, M2.tpr, 
     xlim = c(0, 1), ylim = c(0, 1), 
     type = "l", col = "blue", 
     main = "TPR & FPR vs Threshold for M2", 
     xlab = "Threshold", ylab = "Rate")
lines(M2.threshold, M2.fpr, type = "l", col = "red")
legend("topright", legend = c("TPR", "FPR"), col = c("blue", "red"), lty = 1)

## ---- Q13) Evaluate the choice of δ = 0.5 ----
# Ans: A 0.5 threshold yields a very low FPR (<5%) but a sub-optimal TPR (≈65%).
#      There exist lower thresholds that improve TPR significantly with minimal increase in FPR.
#      Therefore, a 0.5 threshold is not ideal for balancing TPR and FPR.

## ---- Q14) Report the values of TPR & FPR when δ ∈ (0.19, 0.2) and comment ----
M2.metrics = data.frame(Threshold = M2.threshold, TPR = M2.tpr, FPR = M2.fpr)
M2.metrics[M2.metrics$Threshold > 0.19 & M2.metrics$Threshold < 0.20, ]
# Ans: When δ ∈ (0.19, 0.2), the TPR is 92.9% and the FPR ranges from 18.7% to 20.0%.
#      The classifier can correctly identify significantly more developed countries.
#      However, this comes with a slight increase in mislabeled developing countries.
```

![](images/clipboard-2661946738.png)

```{r}
# ===== PART 3: NAIVE BAYES CLASSIFIER =====
## ---- Q15) Form a Naive Bayes Classifier for "Status" ----
library(e1071)
M3 = naiveBayes(Status ~., data)

## ---- Q16) Calculate the accuracy of M3 ----
M3.pred = predict(M3, data, type = "class")
M3.matrix = table(data$Status, M3.pred)
M3.acc = sum(diag(M3.matrix))/sum(M3.matrix); M3.acc
# Ans: The accuracy of M3 is 0.742.

## ---- Q17) Predict the probability that a country is developed ----
predict(M3, new_country, type = "raw")
# Ans: The probability that this country is developed is 0.985.

## ---- Q18) Derive the area under the ROC Curve for M3 ----
M3.prob = predict(M3, data, type = "raw")[, 2]
M3.rocr = prediction(M3.prob, data$Status)
M3.auc = performance(M3.rocr, "auc")@y.values[[1]]; M3.auc
# Ans: The ROC-AUC value for M3 is 0.936.
```

![](images/clipboard-1071989663.png)

```{r}
# ===== PART 4: K-NEAREST NEIGHBOURS ====
## ---- Q19) Form the best KNN classifier based on accuracy ----
library(class)
data.scale = data.frame(Status = data$Status, scale(data[, -1]))
k_values = c(2:10)
acc_values = numeric(length(k_values))

for (i in seq_along(k_values)){
  pred = knn(data.scale[, -1], data.scale[, -1], data.scale$Status, k = k_values[i])
  matrix = table(data.scale$Status, pred)
  acc = sum(diag(matrix))/sum(matrix)
  acc_values[i] = acc
}

knn.metrics = data.frame(K = k_values, Accuracy = acc_values)
knn.metrics[order(knn.metrics$Accuracy, decreasing = TRUE), ]
# Ans: K = 3 gives the best accuracy of 0.955.

## ---- Q20) Derive the area under the ROC Curve for M4 ----
M4.pred = knn(data.scale[, -1], data.scale[, -1], data.scale$Status, k = 3, prob = TRUE)
M4.prob_major = attr(M4.pred, "prob") 
M4.prob = ifelse(M4.pred == "1", M4.prob_major, 1 - M4.prob_major)
M4.rocr = prediction(M4.prob, data.scale$Status)
M4.auc = performance(M4.rocr, "auc")@y.values[[1]]; M4.auc
# Ans: The ROC-AUC value for M4 is 0.981.
```

![](images/clipboard-2552298950.png)

```{r}
# ===== PART 5: DECISION TREES ====
## ---- Q21) Form a Decision Tree for "Status" ----
library(rpart)
M5 = rpart(Status ~ ., data, method = "class",
           control = rpart.control(cp = 0.01), 
           parms = list(split = 'gini'))

## ---- Q22) Most important features for predicting "Status" ----
library(rpart.plot)
rpart.plot(M5, type = 4, extra = 2)
M5$variable.importance
# Ans: Alcohol consumption is the most important feature, followed by life expectancy.

## ---- Q23) Plot the ROC of M3 and M5 in one figure with a legend ----
M3.perf = performance(M3.rocr, "tpr", "fpr")
M5.prob = predict(M5, data, type = "prob")[, 2]
M5.rocr = prediction(M5.prob, data$Status)
M5.perf = performance(M5.rocr, "tpr", "fpr")

plot(M3.perf, col = "red", main = paste("ROC Curves"),
     xlab = "False Positive Rate", ylab = "True Positive Rate")
plot(M5.perf, col = "blue", add = TRUE)
abline(a = 0, b = 1, col = "black", lty = 2)
legend("bottomright", legend = c("Naive Bayes (M3)", "Decision Tree (M5)"),
       lty = 1, col = c("red", "blue"))

## ---- Q24) Determining model with the highest AUC value ----
M5.auc = performance(M5.rocr, "auc")@y.values[[1]]
which.max(c(M1.auc, M2.auc, M3.auc, M4.auc, M5.auc))
max(c(M1.auc, M2.auc, M3.auc, M4.auc, M5.auc))
# Ans: M4 (KNN) has the highest ROC-AUC of 0.981.
```
