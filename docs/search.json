[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSA1101: Everything You Need",
    "section": "",
    "text": "Hi! Iâ€™m Ray! Welcome to your ultimate DSA1101 survival guide! ðŸŽ‰ Whether youâ€™re new to data science or leveling up for midterms, this helpsheet blog has bite-sized tips, easy code, and all the cheat codes you need. ðŸš€ From data wrangling to machine learning, Iâ€™ve got you covered. Ready to dive in? Letâ€™s go! ðŸ˜Ž"
  },
  {
    "objectID": "index.html#working-directory",
    "href": "index.html#working-directory",
    "title": "DSA1101: Everything You Need",
    "section": "Working Directory",
    "text": "Working Directory\n\n# setwd(\"/Users/rayana/Documents/DSA1101/\")\n# getwd()"
  },
  {
    "objectID": "index.html#reading-file",
    "href": "index.html#reading-file",
    "title": "DSA1101: Everything You Need",
    "section": "Reading File",
    "text": "Reading File\n\ndata &lt;- read.table(\"crab.csv\", header = FALSE, sep = \"\")\ndata &lt;- read.csv(\"crab.csv\", header = TRUE, sep = \",\") \n\n# Reading a tab-separated file\ndata &lt;- read.delim(\"crab.csv\")\ndata &lt;- read.csv(\"crab.csv\", sep = \"\\t\")"
  },
  {
    "objectID": "index.html#common-plots",
    "href": "index.html#common-plots",
    "title": "DSA1101: Everything You Need",
    "section": "Common Plots",
    "text": "Common Plots\n\n# pie.chart &lt;- pie(x, labels, radius, main, col, clockwise)\n# bar.chart &lt;- barplot(H, xlab, ylab, main, names.arg, col)\n# box.chart &lt;- boxplot(x, data, notch, varwidth, names, main)\n# histogram &lt;- hist(v, main, xlab, xlim, ylim, breaks, col, border)\n# linegraph &lt;- plot(v, type, col, xlab, ylab)\n# scatterplot &lt;- plot(x, y, xlab, ylab, xlim, ylim, axes)"
  },
  {
    "objectID": "index.html#uselful-functions",
    "href": "index.html#uselful-functions",
    "title": "DSA1101: Everything You Need",
    "section": "Uselful Functions",
    "text": "Uselful Functions\nHere are some functions that will make your life so much easierâ€”but letâ€™s be real, you probably forget how to use them half the time! ðŸ˜…\n\nFunction: ifelse()\n\nx &lt;- 1:10\nx &lt;- ifelse(x %% 2 == 0, \"even\", \"odd\")\nx\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n\n\nFunction: which()\n\ndf &lt;- data.frame(\nx = c(1, 2, 3, 4, 5), \ny = c(5, 4, 3, 2, 1))\n\nrows &lt;- which(df$x &gt; 3)\nrows\n\n[1] 4 5\n\n\n\n\nFunction: append()\n\nvec &lt;- c(1, 2)\ncombined_vec &lt;- append(vec, 3)\ncombined_vec\n\n[1] 1 2 3"
  },
  {
    "objectID": "index.html#interpreting-scatterplots",
    "href": "index.html#interpreting-scatterplots",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Scatterplots",
    "text": "Interpreting Scatterplots\n\nIs there any relationship? Is it strong?\nIf there is, is it positive or negative?\nRelationship is linear or non-linear?\nSpecial observations?\nIs the variability of the response stable when x changes?\n\n\ndata &lt;- read.csv(\"house_selling_prices_FL.csv\")\nplot(data$price ~ data$size, pch = 20)\n\n\n\n\n\n\n\n# Comments:\n# 1. There is a clear relationship.\n# 2. It is a positive association.\n# 3. It seems linear.\n# 4. Variability of price is quite stable as size changes.\n\n\nTwo Categorical Scatterplots\n\nfev &lt;- read.csv(\"FEV.csv\")\nfemale = fev$FEV[which(fev$Sex==0)]\n# Alternative: fev$FEV[fev$Sex==0]\nmale = fev$FEV[which(fev$Sex==1)] \n# Alternative: fev$FEV[fev$Sex==1]\n\nplot(fev$height, fev$FEV, type = \"n\")\npoints(female ~ fev$height[which(fev$Sex==0)], \ncol = \"red\", pch = 20)\npoints(male ~ fev$height[which(fev$Sex==1)], \ncol = \"darkblue\", pch = 20)\nlegend(1.2, 5, legend = c(\"Female\", \"Male\"), \ncol = c(\"red\",\"darkblue\"), pch=c(20,20))\n\n\n\n\n\n\n\n# Comments:\n# Computed correlation is quite high.\n# It is clear from the plot that there is a strong \n# positive linear association between FEV and height. \n# The range of FEV and height for males appears larger than for females. \n# The variability of FEV at lower heights seems slightly \n# less than the variability of FEV at greater heights."
  },
  {
    "objectID": "index.html#interpreting-histograms",
    "href": "index.html#interpreting-histograms",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Histograms",
    "text": "Interpreting Histograms\n\nRange\nBimodal/Unimodal\nSymmetric/Skewed\nData has Gaps/is Clustered\nSuspected outliers\n\n\nhdb &lt;- read.csv(\"hdbresale_reg.csv\")\nhist(hdb$resale_price)\n\n\n\n\n\n\n\n# Comments:\n# range: 200k to ~1 mil\n# unimodal\n# clearly right skewed\n# suspected outliers\n\nfev &lt;- read.csv(\"FEV.csv\")\nhist(fev$FEV)\n\n\n\n\n\n\n\n# Comments:\n# range: 0.5 to 6\n# unimodal\n# slightly right skewed\n# may have outliers\n\n\nCreating Separates Histograms\n\nfev &lt;- read.csv(\"FEV.csv\")\nfemale = fev$FEV[which(fev$Sex==0)]\n# Alternative: fev$FEV[fev$Sex==0]\nmale = fev$FEV[which(fev$Sex==1)] \n# Alternative: fev$FEV[fev$Sex==1]\n\nopar &lt;- par(mfrow=c(1,2)) \nhist(female, col = 2, freq= FALSE, \nmain = \"Histogram of Female FEV\", ylim = c(0,0.52))\nhist(male, col = 4, freq= FALSE, \nmain = \"Histogram of Male FEV\", ylim = c(0,0.52))\n\n\n\n\n\n\n\n# Remember to use par(mfrow=c(1,1)) to reset the diagram layout!\n\n# Comments:\n# Both histogram are unimodal but have different shapes. \n# It is almost symmetrical for females, but quite \n# right-skewed for males.\n# Median FEV for females is much lower than males,\n# 2.49 compared to 2.605.\n# Variability for males is higher than females.\n# The respective IQR are 1.54 and 1.05."
  },
  {
    "objectID": "index.html#interpreting-boxplots",
    "href": "index.html#interpreting-boxplots",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Boxplots",
    "text": "Interpreting Boxplots\n\nHow many outliers\nMedian\nDistribution\nAny visible pattern? (Optional)\n\n\nhbd &lt;- read.csv(\"hdbresale_reg.csv\")\nbp &lt;- boxplot(hdb$resale_price)\n\n\n\n\n\n\n\noutliers &lt;- bp$out\nhead(outliers)\n\n[1] 680000 728000 680000 707000 640000 659500"
  },
  {
    "objectID": "index.html#interpreting-q-q-plot",
    "href": "index.html#interpreting-q-q-plot",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Q-Q Plot",
    "text": "Interpreting Q-Q Plot\n\nfev &lt;- read.csv(\"FEV.csv\")\nqqnorm(fev$FEV, pch = 20)\nqqline(fev$FEV, col = \"red\")\n\n\n\n\n\n\n\n# Comments:\n# Left tail sample quantiles are larger than expected,\n# hence left tail shorter than normal. \n# Right tail sample quantiles larger than expected, hence # right tail longer than normal.\n\n# Combined with the histogram of FEV, it is clear that the\n# sample of FEV is not normally distributed and quite \n# right skewed."
  },
  {
    "objectID": "index.html#linear-regression-model",
    "href": "index.html#linear-regression-model",
    "title": "DSA1101: Everything You Need",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nThere are assumptions have to be fulfilled before using linear model:\n\nQuantitative response (check through str() function)\nRemember to as.factor() categorical features\nSymmetric (check through histogram and/or qqplot)\nVariability of y is stable when x changes (check through scatterplot)\n\nIf data is found to be asymmetric, can do transformation: log(Y), sqrt(Y), 1/Y\n\n\n\n\n\n\n\n\nTransformation\nRestriction\nReverse Transformation\n\n\n\n\nnew_Y &lt;- log(Y)\nY canâ€™t be 0 nor negative\npred &lt;- exp(pred)\n\n\nnew_Y &lt;- sqrt(Y)\nY canâ€™t be negative\npred &lt;- pred**2\n\n\nnew_Y &lt;- 1/Y\nY canâ€™t be 0\npred &lt;- 1/pred\n\n\n\nTo evaluate goodness of fit, we consider F-test and \\(R^2\\). When comparing models however, use adjusted \\(R^2\\)"
  },
  {
    "objectID": "index.html#k-nearest-neighbor-knn-model",
    "href": "index.html#k-nearest-neighbor-knn-model",
    "title": "DSA1101: Everything You Need",
    "section": "K-Nearest Neighbor (KNN) Model",
    "text": "K-Nearest Neighbor (KNN) Model\nThere are some important steps when doing KNN:\n\nImporting the required library\n\nlibrary(class)\n\nStandardizing NUMERIC input features.\n\nVery important, especially when they are in different magnitudes.\nUse scale() Function\n\nRandomly split original data to train and test set.\n\nUsually 80% train and 20% split.\nUse shuffle() function\nRemember to set.seed()\n\n\nTo evaluate goodness of fit, N-fold cross validation is usually do to. Itâ€™s a fair way to evaluate most of classifier models. Confusion matrix is also used to check accuracy, precision, etc.\nBayes decision boundary (the gold standard decision boundary) is also often used to check the goodness of fit, especially in choosing the right K.\n\nSmall K (more flexibility): When K is small, like K = 1, the KNN model is more flexible, fitting the data closely and capturing non-linear patterns. This allows it to adapt better to local variations in the data, which is important for non-linear decision boundaries. However, small KKK values can also make the model more sensitive to noise and prone to overfitting.\nLarge K (More Smoothing): A large K smooths out the decision boundary because the prediction is averaged over more neighbors. This can be useful for linear or simple decision boundaries, but in the case of highly non-linear boundaries, a large K would result in underfitting, where the model oversimplifies the decision boundary and fails to capture the complex patterns."
  },
  {
    "objectID": "index.html#decision-tree-model",
    "href": "index.html#decision-tree-model",
    "title": "DSA1101: Everything You Need",
    "section": "Decision Tree Model",
    "text": "Decision Tree Model\nThere are some important steps when doing Decision Tree:\n\nimporting the required library\n\nlibrary(rpart) : minsplit, maxdepth, cp\nlibrary(rpart.plot) : varlen, faclen\n\nUnderstanding the fitted tree. Itâ€™s crucial to know the how and why of the fitted tree to avoid bias\n\nTo evaluate goodness of fit, N-fold cross validation can do. But most of the times confusion matrix is used."
  },
  {
    "objectID": "index.html#confusion-matrix",
    "href": "index.html#confusion-matrix",
    "title": "DSA1101: Everything You Need",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nNot a ML model, but a way to evaluate most of classifiers\n\n\n\n\n\nSome other formulas:"
  },
  {
    "objectID": "index.html#tutorial-1-introduction-to-r",
    "href": "index.html#tutorial-1-introduction-to-r",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 1 (Introduction to R)",
    "text": "Tutorial 1 (Introduction to R)\n\nOffsite Questions\nGeneral idea of this problem: You have just graduated from NUS and just started your first job. You plan to buy a flat on your own which has price = $1,200,000 (1.2 million dollars). You need to save money for several years before you can afford to make the down payment which is 25% of the flatâ€™s price.\n\nCall the amount that you have saved thus far: saved. You start the very first month with a savings of $10,000 that your parents gave you.\nCall your monthly salary as salary which is paid at the end of every month. Each month, you are going to dedicate 40% of your salary to save for the down payment.\nAssume that you invest your savings wisely, with a monthly average return of 2%. That means: at the end of each month, you receive an additional of saved Ã—0.02 funds where saved is the amount you have from end of previous month to put into your savings.\nAt the end of each month, your savings will be increased by the return on your investment, plus 40% of your monthly salary.\n\nNote: In your code for the questions below, you MUST use the names as given in bold above.\n\nWrite the code to calculate how many months it will take you to save up enough money for the down payment for two persons of of different salary: (i) salary = $7,000; and (ii) salary = $10,000.\n\n# Solution 1.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 7000\nsaved &lt;- 10000\nmonth &lt;- 0\n\nwhile(saved &lt; cost){\nmonth &lt;- month + 1\nsaved &lt;- saved + 0.4 * salary + 0.02 * saved }\nprint(month) # 55\n\n[1] 55\n\n# Solution 2.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 10000\nsaved &lt;- 10000\nmonth &lt;- 0\n\nwhile(saved &lt; cost){\nmonth &lt;- month + 1\nsaved &lt;- saved + 0.4 * salary + 0.02 * saved }\nprint(month) # 44\n\n[1] 44\n\n\nIn question above, we unrealistically assumed that the salary doesnâ€™t change over the years. However, now we consider that the salary will be raised every 4 months by a rate named rate, this variable should be in decimal form (i.e.Â 0.03 for 3%). The new salary will be applied for the month after every batch of 4 months.\nWith this further assumption, write the code to calculate how many months it will take a person to save up enough money for the down payment if that person has (i) (salary = $7,000 and rate = 0.02); (ii) (salary = $10,000 and rate = 0.01).\n\n# Solution 2.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 7000\nsaved &lt;- 10000\nmonth &lt;- 0\nrate &lt;- 0.02\n\nwhile (saved &lt; cost) {\n  month &lt;- month + 1\n  saved &lt;- saved + 0.4 * salary + 0.02 * saved\n  if (month %% 4 == 0) {\n    salary &lt;-  salary * (1 + rate)\n  }\n}\nprint(month) #52\n\n[1] 52\n\n# Solution 2.ii\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 10000\nsaved &lt;- 10000\nmonth &lt;- 0\nrate &lt;- 0.01\n\nwhile (saved &lt; cost) {\n  month &lt;- month + 1\n  saved &lt;- saved + 0.4 * salary + 0.02 * saved\n  if (month %% 4 == 0) {\n    salary &lt;-  salary * (1 + rate)\n  }\n}\nprint(month) #43\n\n[1] 43\n\n\n\n\n\nOnsite Questions\n\nA sequence is generated using the following recursive relation\n\\(x_n\\) = \\(2x_{n-1}\\) \\(-\\) \\(x_{n-2}\\) \\(+\\) \\(5\\), \\(\\text{ for } n \\geq 3\\)\nwith \\(x_1\\) = 0 and \\(x_2\\) = 1.\n(a) Use for loop in R to find the 30th term of the series.\n\nx &lt;- numeric(30)\nx[1] &lt;- 0\nx[2] &lt;- 1\n\nfor (i in 3:30) {\n  x[i] &lt;- 2*x[i-1] - x[i-2] + 5\n}\nx[30]\n\n[1] 2059\n\n\n\n\n\nFind the smallest value of n such that \\(x_n\\) â‰¥ 1,000\n\n\nx &lt;- numeric(10) \nx[1] &lt;- 0\nx[2] &lt;- 1\ni &lt;- 3\n\nwhile (x[i-1] &lt;= 1000) {\n  x[i] &lt;- 2 * x[i-1] - x[i-2] + 5\n  i &lt;- i + 1\n}\n\nmax(x)\n\n[1] 1071\n\nwhich(x == max(x))\n\n[1] 22\n\n\nConsider another sequence which is generated using the following recursive relation\n\\(y_1 = 2800 + 1.02\\times y_0, \\text{ with } y_0 = 1000\\) and\n\\(y_n = 2800 + 1.02\\times y_{n-1}, \\text{ for } n \\geq 2\\)\nfind the smallest value of \\(n\\) such as \\(y_n\\) \\(\\geq\\) \\(300,000\\)\n\ny &lt;- numeric()  # Initialize an empty numeric vector\ny[1] &lt;- 2800 + 1.02 * 10000  # y_1\n\nn &lt;- 2\nwhile (y[n-1] &lt; 300000) {\n  y[n] &lt;- 2800 + 1.02 * y[n-1]\n  n &lt;- n + 1\n}\n\nmax(y)\n\n[1] 305759.6\n\nwhich(y == max(y))\n\n[1] 55"
  },
  {
    "objectID": "index.html#tutorial-2-basic-prob-and-stats",
    "href": "index.html#tutorial-2-basic-prob-and-stats",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 2 (Basic Prob and Stats)",
    "text": "Tutorial 2 (Basic Prob and Stats)\n\nOffsite Questions\nForced Expiratory Volume (FEV) is an index of pulmonary function that measures the volume of air expelled after 1 second of constant effort. The dataset FEV.csv contains measurements for 654 children aged 3 to 19 years of age. The purpose of the data collection was to study how FEV is affected by certain other variables. The variables that we shall work with are\nAge: Age in years.\nFEV: FEV measurement.\nHgt: Height in inches.\nheight: Height in meters\nSex: 0 = female, 1 = male.\nSmoking status: 0 = current non-smoker, 1 = current smoker.\n(a) What is the response variable in this study?\n\n# FEV is the response variable in this study\n\n(b) Create a histogram of FEV and comment on it.\n\nfev &lt;- read.csv(\"FEV.csv\")\nhist(fev$FEV)\n\n\n\n\n\n\n\n# Comments:\n# range: 0.5 to 6\n# unimodal\n# slightly right skewed\n# may have outliers\n\n(c) Create a boxplot of FEV and identify how many outliers there are. Investigate your data and comment on these outliers.\n\nbp &lt;- boxplot(fev$FEV)\n\n\n\n\n\n\n\noutliers &lt;- bp$out\nlength(outliers)\n\n[1] 9\n\n# There are 9 outliers\n\nindex &lt;- which(fev$FEV %in% outliers)\nfev[index,]\n\n       ID Age  FEV Hgt Sex Smoke height\n321  2142  14 4.84  72   1     0   1.83\n452 33041  12 5.22  70   1     0   1.78\n464 37241  13 4.88  73   1     0   1.85\n517 49541  13 5.08  74   1     0   1.88\n609  6144  19 5.10  72   1     0   1.83\n624 25941  15 5.79  69   1     0   1.75\n632 37441  17 5.63  73   1     0   1.85\n648 71141  17 5.64  70   1     0   1.78\n649 71142  16 4.87  72   1     1   1.83\n\n# Comments:\n# 1. All outliers are male\n# 2. Most (8/9) are\n# non-smokers\n# 3. They are rather\n# tall\n\n(d) Generally, is the sample of FEV normally distributed?\n\nqqnorm(fev$FEV, pch = 20)\nqqline(fev$FEV, col = \"red\")\n\n\n\n\n\n\n\n# Comments:\n# Left tail sample quantiles are larger than expected,\n# hence left tail shorter than normal. \n# Right tail sample quantiles larger than expected, hence # right tail longer than normal.\n\n# Combined with the histogram of FEV, it is clear that the\n# sample of FEV is not normally distributed and quite \n# right skewed.\n\n(e) Create separate histograms for male and female FEV, then obtain separate numerical summaries for males and female FEV. Comment on what you observe.\n\n# First, obtain the male and female FEV values separately\n# to plot onto two different histograms.\n\nfemale = fev$FEV[which(fev$Sex==0)]\n# Alternative: fev$FEV[fev$Sex==0]\nmale = fev$FEV[which(fev$Sex==1)] \n# Alternative: fev$FEV[fev$Sex==1]\n\n# Now plot the two histograms side-by-side\n\nopar &lt;- par(mfrow=c(1,2)) \nhist(female, col = 2, freq= FALSE, \nmain = \"Histogram of Female FEV\", ylim = c(0,0.52))\nhist(male, col = 4, freq= FALSE, \nmain = \"Histogram of Male FEV\", ylim = c(0,0.52))\n\n\n\n\n\n\n\n# Remember to use par(mfrow=c(1,1))  to reset the diagram layout!\n\n# obtaining separate numerical summaries for male and female\nIQR(female) # 1.04\n\n[1] 1.04\n\nsummary(female) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.790   1.950   2.490   2.452   2.990   3.840 \n\nvar(female) # 0.4169424\n\n[1] 0.4169424\n\nIQR(male) # 1.5275\n\n[1] 1.5275\n\nsummary(male)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.800   2.007   2.605   2.813   3.535   5.790 \n\nvar(male) # 1.006866\n\n[1] 1.006866\n\n# Comments:\n# Both histogram are unimodal but have different shapes. \n# It is almost symmetrical for females, but quite right-skewed for males.\n# Median FEV for females is much lower than males, 2.49 compared to 2.605.\n# Variability for males is higher than females.\n# The respective IQR are 1.54 and 1.05.\n\n(f) Create a scatterplot with height (in metres) on the x-axis and FEV on the y-axis.\n\nplot(fev$height, fev$FEV)\npoints(female ~ fev$height[which(fev$Sex==0)], \ncol = \"red\", pch = 20)\npoints(male ~ fev$height[which(fev$Sex==1)], \ncol = \"darkblue\", pch = 20)\nlegend(1.2, 5, legend = c(\"Female\", \"Male\"), \ncol = c(\"red\",\"darkblue\"), pch=c(20,20))\n\n\n\n\n\n\n\n\n(g) Compute the correlation between FEV and height and comment on your results.\n\ncor(fev$FEV, fev$height) # 0.8675619\n\n[1] 0.8675619\n\n# Computed correlation is quite high.\n# It is clear from the plot that there is a strong \n# positive linear association between FEV and height. \n# The range of FEV and height for males appears larger than for females. \n# The variability of FEV at lower heights seems slightly \n# less than the variability of FEV at greater heights.\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-3-linear-regression-1",
    "href": "index.html#tutorial-3-linear-regression-1",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 3 (Linear Regression 1)",
    "text": "Tutorial 3 (Linear Regression 1)\n\nOffsite Questions\nConsider the question given in Tutorial 1.\n(a) For the first question in Tutorial 1, use the code to define a function, called F1, where the argument of F1 is salary. Run function F1 for the two cases mentioned.\n\ncost &lt;- 1200000 * 0.25 \nF1 &lt;- function(salary) {\n  saved &lt;- 10000\n  month &lt;- 0\n  while(saved &lt; cost) {\n    month &lt;- month + 1\n    saved &lt;- saved + 0.4 * salary + 0.02 * saved\n    }\n  return(month)\n}\n\n(b) For the second question in Tutorial 1, use the code to define a function, called F2, where F2 has two arguments: salary and rate. Run function F2 for the two cases mentioned to obtain the results.\n\nF2 &lt;- function(salary, price = 1200000, rate = 0.01, portion_save = 0.4) {\n  r = 0.02 # monthly rate return from investment\n  saved &lt;- 10000 # savings given by parents initially\n  month &lt;- 0\n  cost = 0.25*price\n  while(saved &lt; cost) {\n    month = month +1\n    saved = saved + portion_save * salary + saved * r\n    if (month %% 4 ==0) {\n      salary = salary*(1+rate)\n    }\n  }\n  return(month)\n}\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-4-linear-regression-2",
    "href": "index.html#tutorial-4-linear-regression-2",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 4 (Linear Regression 2)",
    "text": "Tutorial 4 (Linear Regression 2)\n\nOffsite Questions\nConsider data set given in the file hdbresale_reg.csv on Canvas, which has the information of 6055 HDB resale flats in Singapore. We would want to form a linear model that helps to predict the resale price of HDB flats, based on the floor area in square meters and the type of the flats.\n(a) Consider the resale price, plot a histogram of it and give your comments. Is it suitable to fit a\nlinear model for this response variable? Explain.\n\ndata &lt;- read.csv(\"hdbresale_reg.csv\")\nstr(data)\n\n'data.frame':   6055 obs. of  11 variables:\n $ X                  : int  580 581 582 583 584 585 586 587 588 589 ...\n $ month              : chr  \"2012-03\" \"2012-03\" \"2012-03\" \"2012-03\" ...\n $ town               : chr  \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" ...\n $ flat_type          : chr  \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" ...\n $ block              : chr  \"640\" \"640\" \"668\" \"5\" ...\n $ street_name        : chr  \"ROWELL RD\" \"ROWELL RD\" \"CHANDER RD\" \"TG PAGAR PLAZA\" ...\n $ storey_range       : chr  \"01 TO 05\" \"06 TO 10\" \"01 TO 05\" \"11 TO 15\" ...\n $ floor_area_sqm     : num  74 74 73 59 68 75 68 77 82 105 ...\n $ flat_model         : chr  \"Model A\" \"Model A\" \"Model A\" \"Improved\" ...\n $ lease_commence_date: int  1984 1984 1984 1977 1979 2003 1980 1974 1978 2002 ...\n $ resale_price       : num  380000 388000 400000 460000 488000 ...\n\nhist(data$resale_price)\n\n\n\n\n\n\n\n# Comments: \n# Right-skewed histogram\n# Hence, resale price is NOT suitable to be the response \n# as assumption of linear model (symmetric) is violated.\n# For a right skewed variable, it is suggested to try \n# transforming the response by taking its logarithm.\n\n(b) Consider the resale price, plot a histogram of log_e of it and give your comments. Is it more\nsuitable to fit a linear model for this response variable than the original resale price?\n\nhist(log(data$resale_price))\n\n\n\n\n\n\n\n# Comments: \n# The histogram of the log of the resale price \n# is more symmetric, hence it is more suitable than \n# the original resale price as a response for our linear model.\n\n(c) Derive a scatter plot of the log_e of the resale price against the floor area in square meters. Give your comments.\n\n# creates a new column for the log(price)\ndata$log.price = log(data$resale_price) \nplot(data$log.price ~ data$floor_area_sqm)\n\n\n\n\n\n\n\n# Comments:\n# There seems to be a strong, positive, linear relationship\n# between log(price) and floor area.\n# The variability of the log(price) seems fairly stable \n# when the floor area changes.***\n\n(d) Fit a linear model where the log of the resale price be the response. Write down the fitted equation.\n\nstr(data)\n\n'data.frame':   6055 obs. of  12 variables:\n $ X                  : int  580 581 582 583 584 585 586 587 588 589 ...\n $ month              : chr  \"2012-03\" \"2012-03\" \"2012-03\" \"2012-03\" ...\n $ town               : chr  \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" ...\n $ flat_type          : chr  \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" ...\n $ block              : chr  \"640\" \"640\" \"668\" \"5\" ...\n $ street_name        : chr  \"ROWELL RD\" \"ROWELL RD\" \"CHANDER RD\" \"TG PAGAR PLAZA\" ...\n $ storey_range       : chr  \"01 TO 05\" \"06 TO 10\" \"01 TO 05\" \"11 TO 15\" ...\n $ floor_area_sqm     : num  74 74 73 59 68 75 68 77 82 105 ...\n $ flat_model         : chr  \"Model A\" \"Model A\" \"Model A\" \"Improved\" ...\n $ lease_commence_date: int  1984 1984 1984 1977 1979 2003 1980 1974 1978 2002 ...\n $ resale_price       : num  380000 388000 400000 460000 488000 ...\n $ log.price          : num  12.8 12.9 12.9 13 13.1 ...\n\nunique(data$flat_type)\n\n[1] \"3 ROOM\"    \"4 ROOM\"    \"5 ROOM\"    \"EXECUTIVE\" \"2 ROOM\"   \n\nM = lm(log.price ~ floor_area_sqm + flat_type, data = data)\nsummary(M)\n\n\nCall:\nlm(formula = log.price ~ floor_area_sqm + flat_type, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.28208 -0.07054 -0.01515  0.04323  0.79797 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.235e+01  1.891e-02 653.013  &lt; 2e-16 ***\nfloor_area_sqm     3.712e-03  1.817e-04  20.429  &lt; 2e-16 ***\nflat_type3 ROOM    1.190e-01  1.714e-02   6.944  4.2e-12 ***\nflat_type4 ROOM    2.093e-01  1.869e-02  11.196  &lt; 2e-16 ***\nflat_type5 ROOM    2.762e-01  2.099e-02  13.160  &lt; 2e-16 ***\nflat_typeEXECUTIVE 4.302e-01  2.527e-02  17.023  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1156 on 6049 degrees of freedom\nMultiple R-squared:  0.7116,    Adjusted R-squared:  0.7114 \nF-statistic:  2986 on 5 and 6049 DF,  p-value: &lt; 2.2e-16\n\n# fitted_log(price) = \n# 12.35 + \n# 0.003712 * floor_area_sqm +\n# 0.119 âˆ— I(flat type = 3 ROOM) +\n# 0.2093 âˆ— I(flat type = 4 ROOM) +\n# 0.2762 âˆ— I(flat type = 5 ROOM) +\n# 0.4302 âˆ— I(flat type = Executive)\n\n(e) Report the coefficient of the floor area in square meters and interpret it.\n\n# The coefficient of it is 0.003712. \n# Meaning when comparing two flats of the same type, \n# then an increase of 1 square meter will increase \n# the predicted log(price) by 0.003712.\n# Equivalently, the price will \n# increase by e^0.003712 = 1.003719 TIMES.\n\n(f) Predict the resale price of a 4-room HDB flat that is of 100 square meters.\n\nnew = data.frame(\n  floor_area_sqm = 100, \n  flat_type = \"4 ROOM\"\n  )\n\npredicted_log.price = predict(M, new)\npredicted_price = exp(predicted_log.price)\nprint(predicted_price) # 412807.6\n\n       1 \n412807.6 \n\n\n(g) Report \\(R^2\\) of the model and interpret it.\n\nsummary(M)$r.squared\n\n[1] 0.7116378\n\n# The R_squared value is 0.712. \n# That means model M can explain 71.2% \n# of the variability of the response in the sample.\n\n\n\nOnsite Questions\nA dataset on house selling price was randomly collected, house_selling_prices_FL.csv. Itâ€™s our interest to model how \\(y\\) = selling price (dollar) is dependent on \\(x\\) = the size of the house (square feet). A simple linear regression model (\\(y\\) regress on \\(x\\)) was fitted, called Model 1.\nThe given data has another variable, NW, which specifies if a house is in the part of the town considered\nless desirable (NW = 0).\n\nhouse &lt;- read.csv(\"house_selling_prices_FL.csv\")\ndim(house)\n\n[1] 100   9\n\nhouse$NW &lt;- as.factor(house$NW)\nstr(house)\n\n'data.frame':   100 obs. of  9 variables:\n $ House   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes   : int  1360 1050 1010 830 2150 1230 150 1470 1850 320 ...\n $ Bedrooms: int  3 1 3 3 3 3 2 3 3 3 ...\n $ Baths   : num  2 1 1.5 2 2 2 2 2 2 2 ...\n $ Quadrant: chr  \"NW\" \"NW\" \"NW\" \"SW\" ...\n $ NW      : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 2 2 2 2 ...\n $ price   : int  145000 68000 115000 69000 163000 69900 50000 137000 121300 70000 ...\n $ size    : int  1240 370 1130 1120 1710 1010 860 1420 1270 1160 ...\n $ lot     : int  18000 25000 25000 17000 14000 8000 15300 18000 16000 8000 ...\n\n\n(a) Derive the correlation between \\(x\\) and \\(y\\).\n\ncor(house$size, house$price)\n\n[1] 0.7612621\n\n\n(b) Derive a scatter plot of \\(y\\) against \\(x\\). Give your comments on the association of y and x.\n\nplot(house$price ~ house$size, pch = 20)\n\n\n\n\n\n\n\n# Comments:\n# There is a clear (obvious) association shown.\n# The association is positive.\n# The association is quite linear.\n# The variability of y (the price) is quite stable when x (the size) changes.\n\n(c) Derive \\(R^2\\) of Model 1. Verify that \\(\\sqrt{R^2}\\) = \\(|cor(y, x)|\\). In which situation we can have \\(\\sqrt{R^2}\\) = \\(cor(y, x)\\)?\n\nM1 = lm(price ~ size, data = house)\nsummary(M1)$r.squared\n\n[1] 0.57952\n\nsqrt(summary(M1)$r.squared ) # same as cor(price, size) # 0.7612621\n\n[1] 0.7612621\n\n# From the code above, we can see that \n# âˆšR^2 = |cor(y, x)|: R^2 = 0.5795, hence, âˆš0.5795 = 0.761 = |cor(y, x)|.\n# When cor(y, x) &gt; 0 then in a simple model y âˆ¼ x, we always have âˆšR2 = cor(y, x)\n\n(d) Form a model (called Model 2) which has two regressors (x and NW). Report the coefficient of variable NW in Model 2. Interpret it.\n\nM2 = lm(price ~ size + NW, data = house)\nsummary(M2)\n\n\nCall:\nlm(formula = price ~ size + NW, data = house)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-83207 -22968    215  14135 109149 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -15257.514  11908.297  -1.281 0.203160    \nsize            77.985      6.209  12.560  &lt; 2e-16 ***\nNW1          30569.087   7948.742   3.846 0.000215 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34390 on 97 degrees of freedom\nMultiple R-squared:  0.6352,    Adjusted R-squared:  0.6276 \nF-statistic: 84.43 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n# The fitted equation of Model 2:\n# y_pred = âˆ’15257.5 + 77.99x + 30569.1 Ã— I(NW = 1).\n\n# The estimated coefficient of NW in Model 2 is 30569.1\n# This value means: for two houses of the same size (fix x), \n# the house in the more desirable part (NW = 1) is \n# $30569.1 more than the one in the less desirable part (NW = 0)\n\n(e) Estimate and report the price of a house where its size is 4000 square feet and is located at the more\ndesirable part of the town.\n\npredict(M2, newdata=data.frame(size=4000, NW = \"1\"))\n\n       1 \n327252.1 \n\n# The mean price of a house with size x = 4000 and NW = 1 is $327252.1."
  },
  {
    "objectID": "index.html#tutorial-5-k-nearest-neighbor",
    "href": "index.html#tutorial-5-k-nearest-neighbor",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 5 (K-Nearest Neighbor)",
    "text": "Tutorial 5 (K-Nearest Neighbor)\n\nOffsite Questions\n\n(MLR) Consider the horseshoe female crab data given in the csv file crab.csv. We would want to form a model for the weight of the female crabs (kg), which depends on its width (cm) and its spine condition (1 = both good, 2 = one worn or broken, 3 = both worn or broken).\n\ndata &lt;- read.csv(\"crab.csv\")\nhead(data)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : int  3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\ndata$spine &lt;- as.factor(data$spine)\n# 1 = both good\n# 2 = one worn or broken\n# 3 = both worn or broken\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\n\n(a) Produce a scatter plot of variable weight against width for different condition of spine.\n\nattach(data)\nplot(weight ~ width, pch = 20, main = \"weight vs width for each spine class\",\n     xlab = \"width\", ylab = \"weight\")\npoints(weight[spine == \"1\"] ~ width[spine ==\"1\"], pch = 15, col = \"red\")\npoints(weight[spine == \"2\"] ~ width[spine ==\"2\"], pch = 16, col = \"blue\")\npoints(weight[spine == \"3\"] ~ width[spine ==\"3\"], pch = 17, col = \"green\")\nlegend(22,4,legend=c(\"1\", \"2\", \"3\"),col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Fit a linear regression model for weight which has two explanatories, width and spine.\n\nmodel &lt;- lm(weight ~ width + spine, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n\n(c) Is the fitted model signicant?\n\n# the model f statistic showing p value is less than 2.2 x 10^-16.\n# Since this is far below 0.05, the model can be considered significant\n\n(d) Derive \\(R^2\\) and adjusted \\(R^2\\) of the fitted model.\n\nsummary(model)$r.squared\n\n[1] 0.7917598\n\n# 0.7917598\nsummary(model)$adj.r.squared\n\n[1] 0.7880632\n\n# 0.7880632\n\n(e) Write down the fitted model.\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) -0.06969*I(spine=3)\n\n(f) Two female crabs of the same width, and the diference of their weight if one has spines are of good condition and another one with broken spines.\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) + -0.06969*I(spine=3)\n# weight1 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*0\n# weight2 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*1\n\nweight.dif &lt;- (0.05544*0 + -0.06969*0) - (0.05544*0 + -0.06969*1)\nweight.dif\n\n[1] 0.06969\n\n\n(g) Predict the weight of a female crab that has width of 27 cm and has both spines worn or broken.\n\nnewdata &lt;- data.frame(width = 27, spine = \"3\")\npredict(model, newdata = newdata)\n\n       1 \n2.582352 \n\n\nMeasures of classier performance\nSuppose we have developed a K-nearest neighbors classier for predicting diabetes status. The following table shows the actual response \\(Y\\) (1 = yes, 0 =n o) and fitted value \\(\\hat Y\\) using the classier for 10 test data points. A test data point is predicted to be \\(\\hat G\\)= 1 if \\(\\hat Y\\) &gt; Î´, for a specied threshold value Î´ (Recall that we use Î´ = 0.5 in class, also known as the majority rule).\n\n\nWe define: \\(TPR = \\frac {TP}{TP +FN}\\), \\(FPR = \\frac {FP}{FP +TN}\\); For each of the thresholds Î´ = 0.3, 0.6 and 0.8, derive \\(TPR\\) and \\(FPR\\) in making predictions with the K-nearest neighbors classier for the 10 test data points. Plot \\(TPR\\) against \\(FPR\\) for the three thresholds.\n\n\ny &lt;- c(1,1,0,1,1,0,0,1,0,0)\nycap &lt;- c(0.9,0.5,0.7,0.4,0.5,0.2,0.7,0.9,0.1,0.1)\ntpr.all &lt;- numeric(0)\nfpr.all &lt;- numeric(0)\n\nsigma.list &lt;- c(0.3,0.6,0.8)\nfor (sigma in sigma.list) {\n  pred &lt;- ifelse(ycap &gt;= sigma, 1 ,0)\n  confusion.matrix &lt;- table(y, pred)\n  tpr &lt;- confusion.matrix[2,2]/sum(confusion.matrix[2,])\n  fpr &lt;- confusion.matrix[1,2]/sum(confusion.matrix[1,])\n  tpr.all &lt;- append(tpr.all, tpr)\n  fpr.all &lt;- append(fpr.all, fpr)\n}\n\nplot(tpr.all ~ fpr.all, pch = 20,\n     xlim = c(0,1),\n     ylim = c(0,1))\npoints(tpr.all[1] ~ fpr.all[1], pch = 15, col = \"red\")\npoints(tpr.all[2] ~ fpr.all[2], pch = 16, col = \"blue\")\npoints(tpr.all[3] ~ fpr.all[3], pch = 17, col = \"green\")\nlegend(0.7,0.4,legend=c(\"sigma = 0.3\", \"sigma = 0.6\", \"sigma = 0.8\"),\n       col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Can we add the two points (0,0) and (1,1) to the plot of \\(TPR\\) against \\(FPR\\) in part (a). Explain why or why not.\n\n# If Ïƒ &gt; 0.9 then all test points have predicted Ë†G = 0 (predicted as negative), \n# so TPR = FPR = 0.\n# If Ïƒ &lt; 0.1, then all test points have predicted Ë†G = 1 (predicted as positive),\n# so TPR = FPR = 1.\n# Since there exist Ïƒ within the range from 0 to 1 for the two points to happen, \n# these two points can be added to the plot.\n\nThe CSV file Caravan.csv contains data on 5822 real customer records on caravan insurance purchase. This data set is owned and supplied by the Dutch data mining company, Sentient Machine Research, and is based on real world business data. Each record consists of 86 variables, containing socio-demographic data (variables 1-43) and product ownership (variables 44-86). Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy. For this business, assume that the overall error rate (equivalently, the accuracy) is not of interest. Instead, the company wants to use the classier to predict who are the potential customers likely to purchase insurance. Then the metric precision will be important, since it relates the proportion of Individuals who will actually purchase the insurance, among the group of individuals who are predicted to purchase insurance.\n\ncara &lt;- read.csv(\"Caravan data.csv\")\nhead(cara, n = 2)\n\n  X MOSTYPE MAANTHUI MGEMOMV MGEMLEEF MOSHOOFD MGODRK MGODPR MGODOV MGODGE\n1 1      33        1       3        2        8      0      5      1      3\n2 2      37        1       2        2        8      1      4      1      4\n  MRELGE MRELSA MRELOV MFALLEEN MFGEKIND MFWEKIND MOPLHOOG MOPLMIDD MOPLLAAG\n1      7      0      2        1        2        6        1        2        7\n2      6      2      2        0        4        5        0        5        4\n  MBERHOOG MBERZELF MBERBOER MBERMIDD MBERARBG MBERARBO MSKA MSKB1 MSKB2 MSKC\n1        1        0        1        2        5        2    1     1     2    6\n2        0        0        0        5        0        4    0     2     3    5\n  MSKD MHHUUR MHKOOP MAUT1 MAUT2 MAUT0 MZFONDS MZPART MINKM30 MINK3045 MINK4575\n1    1      1      8     8     0     1       8      1       0        4        5\n2    0      2      7     7     1     2       6      3       2        0        5\n  MINK7512 MINK123M MINKGEM MKOOPKLA PWAPART PWABEDR PWALAND PPERSAUT PBESAUT\n1        0        0       4        3       0       0       0        6       0\n2        2        0       5        4       2       0       0        0       0\n  PMOTSCO PVRAAUT PAANHANG PTRACTOR PWERKT PBROM PLEVEN PPERSONG PGEZONG\n1       0       0        0        0      0     0      0        0       0\n2       0       0        0        0      0     0      0        0       0\n  PWAOREG PBRAND PZEILPL PPLEZIER PFIETS PINBOED PBYSTAND AWAPART AWABEDR\n1       0      5       0        0      0       0        0       0       0\n2       0      2       0        0      0       0        0       2       0\n  AWALAND APERSAUT ABESAUT AMOTSCO AVRAAUT AAANHANG ATRACTOR AWERKT ABROM\n1       0        1       0       0       0        0        0      0     0\n2       0        0       0       0       0        0        0      0     0\n  ALEVEN APERSONG AGEZONG AWAOREG ABRAND AZEILPL APLEZIER AFIETS AINBOED\n1      0        0       0       0      1       0        0      0       0\n2      0        0       0       0      1       0        0      0       0\n  ABYSTAND Purchase\n1        0       No\n2        0       No\n\nstr(cara)\n\n'data.frame':   5822 obs. of  87 variables:\n $ X       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MOSTYPE : int  33 37 37 9 40 23 39 33 33 11 ...\n $ MAANTHUI: int  1 1 1 1 1 1 2 1 1 2 ...\n $ MGEMOMV : int  3 2 2 3 4 2 3 2 2 3 ...\n $ MGEMLEEF: int  2 2 2 3 2 1 2 3 4 3 ...\n $ MOSHOOFD: int  8 8 8 3 10 5 9 8 8 3 ...\n $ MGODRK  : int  0 1 0 2 1 0 2 0 0 3 ...\n $ MGODPR  : int  5 4 4 3 4 5 2 7 1 5 ...\n $ MGODOV  : int  1 1 2 2 1 0 0 0 3 0 ...\n $ MGODGE  : int  3 4 4 4 4 5 5 2 6 2 ...\n $ MRELGE  : int  7 6 3 5 7 0 7 7 6 7 ...\n $ MRELSA  : int  0 2 2 2 1 6 2 2 0 0 ...\n $ MRELOV  : int  2 2 4 2 2 3 0 0 3 2 ...\n $ MFALLEEN: int  1 0 4 2 2 3 0 0 3 2 ...\n $ MFGEKIND: int  2 4 4 3 4 5 3 5 3 2 ...\n $ MFWEKIND: int  6 5 2 4 4 2 6 4 3 6 ...\n $ MOPLHOOG: int  1 0 0 3 5 0 0 0 0 0 ...\n $ MOPLMIDD: int  2 5 5 4 4 5 4 3 1 4 ...\n $ MOPLLAAG: int  7 4 4 2 0 4 5 6 8 5 ...\n $ MBERHOOG: int  1 0 0 4 0 2 0 2 1 2 ...\n $ MBERZELF: int  0 0 0 0 5 0 0 0 1 0 ...\n $ MBERBOER: int  1 0 0 0 4 0 0 0 0 0 ...\n $ MBERMIDD: int  2 5 7 3 0 4 4 2 1 3 ...\n $ MBERARBG: int  5 0 0 1 0 2 1 5 8 3 ...\n $ MBERARBO: int  2 4 2 2 0 2 5 2 1 3 ...\n $ MSKA    : int  1 0 0 3 9 2 0 2 1 1 ...\n $ MSKB1   : int  1 2 5 2 0 2 1 1 1 2 ...\n $ MSKB2   : int  2 3 0 1 0 2 4 2 0 1 ...\n $ MSKC    : int  6 5 4 4 0 4 5 5 8 4 ...\n $ MSKD    : int  1 0 0 0 0 2 0 2 1 2 ...\n $ MHHUUR  : int  1 2 7 5 4 9 6 0 9 0 ...\n $ MHKOOP  : int  8 7 2 4 5 0 3 9 0 9 ...\n $ MAUT1   : int  8 7 7 9 6 5 8 4 5 6 ...\n $ MAUT2   : int  0 1 0 0 2 3 0 4 2 1 ...\n $ MAUT0   : int  1 2 2 0 1 3 1 2 3 2 ...\n $ MZFONDS : int  8 6 9 7 5 9 9 6 7 6 ...\n $ MZPART  : int  1 3 0 2 4 0 0 3 2 3 ...\n $ MINKM30 : int  0 2 4 1 0 5 4 2 7 2 ...\n $ MINK3045: int  4 0 5 5 0 2 3 5 2 3 ...\n $ MINK4575: int  5 5 0 3 9 3 3 3 1 3 ...\n $ MINK7512: int  0 2 0 0 0 0 0 0 0 1 ...\n $ MINK123M: int  0 0 0 0 0 0 0 0 0 0 ...\n $ MINKGEM : int  4 5 3 4 6 3 3 3 2 4 ...\n $ MKOOPKLA: int  3 4 4 4 3 3 5 3 3 7 ...\n $ PWAPART : int  0 2 2 0 0 0 0 0 0 2 ...\n $ PWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSAUT: int  6 0 6 6 0 6 6 0 5 0 ...\n $ PBESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PTRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBROM   : int  0 0 0 0 0 0 0 3 0 0 ...\n $ PLEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBRAND  : int  5 2 2 2 6 0 0 0 0 3 ...\n $ PZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAPART : int  0 2 1 0 0 0 0 0 0 1 ...\n $ AWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSAUT: int  1 0 1 1 0 1 1 0 1 0 ...\n $ ABESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ ATRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABROM   : int  0 0 0 0 0 0 0 1 0 0 ...\n $ ALEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABRAND  : int  1 1 1 1 1 0 0 0 0 1 ...\n $ AZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Purchase: chr  \"No\" \"No\" \"No\" \"No\" ...\n\ndim(cara)\n\n[1] 5822   87\n\n\n(a) Without any classier, if the company tries to sell insurance to a random selection of customers, what is the success rate?\n\ntable(cara$Purchase)\n\n\n  No  Yes \n5474  348 \n\nsuccesRate &lt;- table(cara$Purchase)[2]/sum(table(cara$Purchase))\nsuccesRate\n\n       Yes \n0.05977327 \n\n# data set shows almost 6% of people purchased insurance\n\n(b) Standardize the input features. \\(Hint\\): Use scale() command in R.\n\ncara &lt;- cara[,-1] #throwing the first column since it provides no information\nscaled.X &lt;- scale(cara[,-86]) #throwing the response column\n\n(c) Randomly select 1000 observations to form the test data, and the remaining observations will be the training data.\n\nset.seed(5)\nn &lt;- dim(cara)[1] #train data number\nshuffled.index &lt;- sample(c(1:n), size = 1000)\nX.train &lt;- scaled.X[-shuffled.index,]\nX.test&lt;- scaled.X[shuffled.index,]\nY.train &lt;- as.factor(cara$Purchase[-shuffled.index])\nY.test &lt;- as.factor(cara$Purchase[shuffled.index])\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a customer will purchase insurance. Compute the precision of the classier.\n\nlibrary(class)\n\npred &lt;- knn(X.train, X.test, Y.train, k=1)\nconfusion.matrix &lt;- table(Y.test, pred) \n# Precision = TP/TP+FP\nprecision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\nconfusion.matrix\n\n      pred\nY.test  No Yes\n   No  892  55\n   Yes  44   9\n\nprecision\n\n[1] 0.140625\n\n\n(e) Repeat question 3d, for k-nearest neighbor classier where k = 3,5,10. Which value of k gives the best precision?\n\nprecision.all &lt;- data.frame()\nk.list &lt;- c(3, 5, 10)\nfor (k in k.list) {\n  pred &lt;- knn(X.train, X.test, Y.train, k = k)\n  confusion.matrix &lt;- table(Y.test, pred) \n  # Precision = TP/TP+FP\n  precision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\n  precision.all &lt;- rbind(precision.all, data.frame(k, precision))\n}\n\nprecision.all\n\n   k  precision\n1  3 0.05263158\n2  5 0.14285714\n3 10        NaN\n\n# So far, k = 5 gives the best precision.\n# However, one might use N -fold cross validation \n# to have the average precision for each k.\n# With that, the value of k that gives largest average precision is chosen.\n\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-6-decision-tree",
    "href": "index.html#tutorial-6-decision-tree",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 6 (Decision Tree)",
    "text": "Tutorial 6 (Decision Tree)\n\nOffsite Questions\n\n(KNN and N -fold Cross Validation)\nLoan managers often need to take into account an applicantâ€™s demographic and socioeconomic proles in deciding whether to approve a loan to the applicant, to minimize losses due to defaults. In this exercise we will build and evaluate a classier based on the German Credit Data to predict whether an applicant is considered as having good or bad credit risk. The features or predictors include (1) loan duration (in months), (2) credit amount, (3) Installment rate in percentage of disposable income and (4) age in years.\n(a) Read and explore the data from the file German_credit.csv.\n\ndata &lt;- read.csv(\"German credit.csv\")\nhead(data)\n\n  Creditability Duration Amount Instalment Age\n1             1       18   1049          4  21\n2             1        9   2799          2  36\n3             1       12    841          2  23\n4             1       12   2122          3  39\n5             1       12   2171          4  38\n6             1       10   2241          1  48\n\nstr(data)\n\n'data.frame':   1000 obs. of  5 variables:\n $ Creditability: int  1 1 1 1 1 1 1 1 1 1 ...\n $ Duration     : int  18 9 12 12 12 10 8 6 18 24 ...\n $ Amount       : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n $ Instalment   : int  4 2 2 3 4 1 1 2 4 1 ...\n $ Age          : int  21 36 23 39 38 48 39 40 65 23 ...\n\n\n(b) Standardize the input features.\n\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\nhead(X)\n\n       Duration     Amount  Instalment         Age\n[1,] -0.2407368 -0.7872630  0.91801781 -1.28093214\n[2,] -0.9870788 -0.1673006 -0.86974813  0.04034293\n[3,] -0.7382981 -0.8609500 -0.86974813 -1.10476213\n[4,] -0.7382981 -0.4071375  0.02413484  0.30459795\n[5,] -0.7382981 -0.3897785  0.91801781  0.21651294\n[6,] -0.9041519 -0.3649800 -1.76363111  1.09736299\n\n\n(c) Randomly select 800 customer records to form the training data, and the remaining 200 records will be the test data.\n\nset.seed(100)\nn &lt;- dim(data)[1]\nindex &lt;- sample(1:n, 800)\n\ntraining.X &lt;- X[index, ]\ntest.X &lt;- X[-index, ]\ntraining.Y &lt;- Y[index]\ntest.Y &lt;- Y[-index]\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a loan applicant is credible for the 200 test points. Compute the accuracy of the classier.\n\nlibrary(class)\npred &lt;- knn(training.X, test.X, training.Y, k=1)\nconfusion.matrix &lt;- table(test.Y, pred)\nconfusion.matrix\n\n      pred\ntest.Y  0  1\n     0 24 35\n     1 47 94\n\naccuracy &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\naccuracy\n\n[1] 0.59\n\n\n(e) Use N -folds cross validation with N = 5 to find the average accuracy for the 1-nearest neighbor classifier.\n\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\n\naccuracy &lt;- numeric(n_folds)\nfor (j in 1:n_folds) {\n  test_j &lt;- which(folds_j == j) \n  test.y &lt;- Y[test_j] \n  train.X &lt;- X[-test_j, ]  \n  test.X &lt;- X[test_j, ]  \n  train.y &lt;- Y[-test_j] \n\n  knn.pred &lt;- knn(train.X, test.X, train.y, k = 1) \n\n  confusion.matrix &lt;- table(test.y, knn.pred)\n  accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n}\n\nave.accuracy &lt;- round(mean(accuracy), digits = 3)\nave.accuracy\n\n[1] 0.622\n\n\n(f) Repeat question 1e for K-nearest neighbor classifiers where K = 1, 2, â€¦100.\n\n### FULL KNN WITH N-FOLDS VALIDATION\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nk &lt;- 100\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\nave.accuracy &lt;- numeric(length(k))\n\nfor (i in 1:k) {\n  accuracy &lt;- numeric(n_folds)\n  for (j in 1:n_folds) {\n    test_j &lt;- which(folds_j == j) \n    test.y &lt;- Y[test_j] \n    train.X &lt;- X[-test_j, ]  \n    test.X &lt;- X[test_j, ]  \n    train.y &lt;- Y[-test_j] \n\n    knn.pred &lt;- knn(train.X, test.X, train.y, k = i) \n\n    confusion.matrix=table(test.y, knn.pred)\n    accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n  }\n  ave.accuracy[i] &lt;- round(mean(accuracy), digits = 3)\n}\n\nave.accuracy\n\n  [1] 0.622 0.597 0.661 0.674 0.684 0.671 0.676 0.686 0.686 0.691 0.702 0.699\n [13] 0.705 0.702 0.707 0.695 0.699 0.702 0.700 0.693 0.695 0.695 0.697 0.695\n [25] 0.699 0.697 0.696 0.700 0.693 0.688 0.697 0.692 0.697 0.698 0.697 0.692\n [37] 0.695 0.695 0.695 0.696 0.696 0.701 0.699 0.701 0.699 0.699 0.699 0.698\n [49] 0.699 0.696 0.698 0.699 0.699 0.696 0.695 0.699 0.696 0.697 0.696 0.698\n [61] 0.697 0.696 0.699 0.697 0.697 0.697 0.697 0.696 0.698 0.699 0.699 0.696\n [73] 0.698 0.697 0.698 0.696 0.698 0.698 0.698 0.699 0.699 0.698 0.698 0.700\n [85] 0.698 0.698 0.700 0.700 0.699 0.700 0.700 0.701 0.701 0.701 0.700 0.700\n [97] 0.699 0.698 0.699 0.699\n\n\n(g) Compare the 100 classifiers above, which few values of K give the best average accuracy?\n\nlibrary(glue)\nindex = which(ave.accuracy == max(ave.accuracy))\nplot(x = 1:k, ave.accuracy)\nabline(v = index, col = \"red\")\n\n\n\n\n\n\n\nglue(\"most accurate k = {index} with average accuracy = {ave.accuracy[index]}\")\n\nmost accurate k = 15 with average accuracy = 0.707\n\n\n(Decision Trees)\nConsider the famous Iris Flower Data set which was rst introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation: the length and the width of the sepals and petals (in cm).\n\n(a) Use decision tree method to predict Iris species based on all four features.\n\niris &lt;- read.csv(\"Iris dataset.csv\")\nhead(iris)\n\n  sepal.length sepal.width petal.length petal.width       class\n1          5.1         3.5          1.4         0.2 Iris-setosa\n2          4.9         3.0          1.4         0.2 Iris-setosa\n3          4.7         3.2          1.3         0.2 Iris-setosa\n4          4.6         3.1          1.5         0.2 Iris-setosa\n5          5.0         3.6          1.4         0.2 Iris-setosa\n6          5.4         3.9          1.7         0.4 Iris-setosa\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ sepal.length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ sepal.width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ petal.length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal.width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\ndim(iris)\n\n[1] 150   5\n\nlibrary(rpart)\nfit &lt;- rpart(class ~ sepal.length + sepal.width + petal.length + petal.width,\n              data = iris,\n              method=\"class\",\n              parms=list(split='information'),\n              control = rpart.control( minsplit =1))\n\n(b) Visualize the decision tree above, using the rpart.plot function.\n\nlibrary(rpart.plot)\nrpart.plot(fit, type=4, extra=2)\n\n\n\n\n\n\n\n# The fitted tree is given in the figure below.\n# If the measurement of petal length is less than 2.5 cm \n# then the flower is of Iris-setosa\n# If the petal length is â‰¥ 2.5 cm with the petal width is â‰¥ 1.8 cm \n# then high chance (45/46) it will be an Iris-virginica.\n# If the petal length is â‰¥ 2.5 cm with the petal width is &lt; 1.8 cm\n# then we continue to check if the petal length is in the interval\n# [2.5, 5]. If yes, then high chance (47/48) it is Iris-versicolor.\n\n(c) What are the more important features in the fitted tree above?\n\n# It seems the sepal length and sepal width \n# are not important in the classification while\n# the petal length and petal width are more important."
  },
  {
    "objectID": "index.html#y2223-s2",
    "href": "index.html#y2223-s2",
    "title": "DSA1101: Everything You Need",
    "section": "Y(22/23) S2",
    "text": "Y(22/23) S2\n\nQuestions 1 (50 points)\nA study of nesting horseshoe crabs (J. Rrockmann, Ethology, 102: 1-21, 1996) collected data on 173 female horseshoe crabs. Explanatory variables included the female crabâ€™s color (2,3,4 and 5 which increases when the darkness increases), spine condition, weight (kg), and carapace width (cm).\nVariable satell indicates the number of male crab (called satellite) attached to the female crabs (for example, the first crab has 8 satellites attached to it). In this problem, we are interested in the possible factors like color, weight and width that may affect the number of satellites attach to a female crab.\nData are given in the file data1.txt.\nImport the given data into R and keep all the variable names as original.\n\ndata&lt;-read.table('data1.txt', header=T) \nhead(data)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\ndim(data)\n\n[1] 173   5\n\nnames(data)\n\n[1] \"color\"  \"spine\"  \"width\"  \"satell\" \"weight\"\n\nattach(data)\n\nThe following objects are masked from data (pos = 7):\n\n    color, satell, spine, weight, width\n\n\nPart I: Exploring the response variable - satell\n\nCreate a histogram of this variable with a normal density curve overlaying. Give your comment about this plot.\n\nhist(satell, probability = TRUE, \n     main = \"Histogram of Satell with Normal Density Curve\",\n     xlab = \"Number of Satellites\", col = \"lightblue\", border = \"black\")\n\nx_values &lt;- seq(min(satell), max(satell), length = 100)\ny_values &lt;- dnorm(x_values, mean = mean(satell), sd = sd(satell))\nlines(x_values, y_values, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Comments:\n# The histogram of the satell variable shows values ranging from 0 to 15, \n# with a single peak (unimodal) where most of the data is concentrated around 1. \n# The distribution is clearly right-skewed, no gap.\n# There are a few suspected outliers on the far right, with values around 15.\n\nCreate a box plot of this variable. Does it show any outliers? If yes, retrieve the full information (full row) of the crabs that are outliers in number of satellites. Copy the information and paste into R code file as comments.\n\nboxplot(satell)\nout &lt;- boxplot(satell)$out; out\n\n\n\n\n\n\n\n\n[1] 14 15\n\nindex &lt;- which(satell %in% out)\ndata[index,]\n\n   color spine width satell weight\n15     3     1  26.0     14    2.3\n56     3     3  28.3     15    3.0\n\n# There are two outliers, as following:\n# color spine width satell weight\n# 15     3     1  26.0     14    2.3\n# 56     3     3  28.3     15    3.0\n\nCreate a qq plot of this variable. Give your comment about this plot.\n\nqqnorm(satell,  pch = 20)\nqqline(satell, col = \"red\")\n\n\n\n\n\n\n\n# Comments:\n# Observed values on the left are clearly higher then the expected values,\n# meaning the data has shorter left tail.\n# Hence, satell data is not normally distributed\n\n\nPart II: Variable color\n\nWrite the code to create a new categorical variable, col, which equals to light if the crab has color 2 or 3, and equals to dark if the crab has color 4 or 5.\n\ndata$col &lt;- numeric(length(satell))\ndata$col[which(color &lt;= 3)] &lt;- \"light\"\ndata$col[which(color &gt; 3)] &lt;- \"dark\"\n\nhead(data)\n\n  color spine width satell weight   col\n1     3     3  28.3      8   3.05 light\n2     4     3  22.5      0   1.55  dark\n3     2     1  26.0      9   2.30 light\n4     4     3  24.8      0   2.10  dark\n5     4     3  26.0      4   2.60  dark\n6     3     3  23.8      0   2.10 light\n\nattach(data)\n\nThe following objects are masked from data (pos = 3):\n\n    color, satell, spine, weight, width\n\n\nThe following objects are masked from data (pos = 8):\n\n    color, satell, spine, weight, width\n\n\nCreate a frequency table for variable col created above. How many crabs are of light color and how many crabs are of dark color?\n\ntable(col)\n\ncol\n dark light \n   66   107 \n\n\nPlot a scatter plot of weight and satell, classified by col. Add a legend box for this plot. Give your comment about this plot.\n\nplot(satell ~ weight, \n     type = \"n\",\n     xlab=\"Weight\",\n     ylab=\"Satell\", \n     main = \"Crab Data\")\n\npoints(satell[col == \"light\"] ~ weight[col == \"light\"], pch = 20, col = \"red\")\npoints(satell[col == \"dark\"] ~ weight[col == \"dark\"], pch = 20, col = \"blue\")\nlegend(4.5, 15, legend=c(\"light\", \"dark\"),col = c(\"red\", \"blue\"), pch = 20)\n\n\n\n\n\n\n\n# Comments:\n# Overall, weight and satell has (quiet weak) POSITIVE\n# and possibily LINEAR relationship.\n# For light color or for dark color, the relationship between satell and weight\n# are about similar, no clear difference\n# as weight changes, the variability of satell gets larger\n\n\nPart III: Modelling\n\nFit a linear regression model for satell using three features color, weight and width.\n\ndata$color = as.factor(data$color)\nM = lm(satell ~ color + weight + width, data = data)\n\nReport the value of \\(R^2\\) of the model above. Give your comments on the goodness-of-fit of the model.\n\nsummary(M)\n\n\nCall:\nlm(formula = satell ~ color + weight + width, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5238 -2.1270 -0.6765  1.5393 11.1459 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -1.85827    4.51379  -0.412    0.681  \ncolor3      -0.61976    0.90368  -0.686    0.494  \ncolor4      -1.23133    0.96969  -1.270    0.206  \ncolor5      -1.17540    1.07476  -1.094    0.276  \nweight       1.68797    0.84358   2.001    0.047 *\nwidth        0.05576    0.23188   0.240    0.810  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.947 on 167 degrees of freedom\nMultiple R-squared:  0.1492,    Adjusted R-squared:  0.1237 \nF-statistic: 5.857 on 5 and 167 DF,  p-value: 5.168e-05\n\n# Comments:\n# R^2 = 0.1492\n# Though F-test for the significance of the overall  model: model is significant\n# However, R^2 is too low. It means model doesn't fit the data well.\n\n\n\n\nQuestions 2 (30 points)\nA data set data2.csv contains cases from a study that was conducted between 1958 and 1970 at the University of Chicagoâ€™s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\nA table of variable description is given below. status is the response in this study.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nage\nage of patient at which they undergone surgery\n\n\nyear\nyear in which patient was undergone surgery (1958 - 1969)\n\n\nnode\nnumber of lymph nodes that have cancer cells detected\n\n\nstatus\n1 = the patient survived 5 years or longer (negative); and\n2 = the patient died within 5 years (positive)\n\n\n\nFor this problem, we will not consider the year when patient was undergone the surgery, year, as a feature for classification.\nIn R, use set.seed(999).\nWe would:\n\nUse KNN algorithm where K can be any positive integers, from 1 up to 50, to form classifiers to predict the outcome.\nFor each classier, evaluate itâ€™s performance by 3-fold cross validation.\nUse type I error rate and type II error rate as the measures to evaluate each classifiers.\nSelect the best K.\n\nset.seed(999)\n\nhab &lt;- read.csv('data2.csv')\nhead(hab)\n\n  age year node survival.status\n1  30   64    1               1\n2  30   62    3               1\n3  30   65    0               1\n4  31   59    2               1\n5  31   65    4               1\n6  33   58   10               1\n\ndim(hab)\n\n[1] 306   4\n\nnames(hab)[4] = \"status\"\n\nhab$status = as.factor(hab$status)\nhab[, 1:3] = lapply(hab[ , 1:3],scale) # SCALING THE FEATURES\n\nattach(hab)\ntable(status)\n\nstatus\n  1   2 \n225  81 \n\n\n\n\nWrite the code for the purposes above. For each classier, the average of Type I error rates from 3-fold cross validation is saved in a vector, (named ave.type1); and the average of Type II error rates is saved in another vector (named ave.type2).\n\nlibrary(class)\n\nX &lt;- hab[, c(1,3)] # we do not use the 2nd column, year.\nY &lt;- hab[, 4] # response\n\nn &lt;- length(Y) # sample size\nn_folds &lt;- 3\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\ntable(folds_j)\n\nfolds_j\n  1   2   3 \n102 102 102 \n\nK = 50\nave.type1 = numeric(K) # to store the accuracy for each k of KNN, k is from 1 to K = 50.\nave.type2 = numeric(K) # to store the precision for each k of KNN\ntype1=numeric(n_folds)\ntype2=numeric(n_folds)\n\nfor(i in 1:K) {\n  for (j in 1:n_folds) {\n    test_j &lt;- which(folds_j == j) # get the index of the points that will be in the test set\n    test.y = Y[test_j] # response for the test points\n    knn.pred &lt;- knn(train=X[ -test_j, ], test=X[test_j, ], cl=Y[-test_j ], k=i) \n    confusion.matrix=table(test.y, knn.pred) # 2 = positive and 1 = negative\n\n    type1[j] = confusion.matrix[1,2]/sum(confusion.matrix[1,]) # row 1 means actual negative\n    type2[j] = confusion.matrix[2,1]/sum(confusion.matrix[2,]) # row 2 means actual positive\n  }\n        ave.type1[i] = round(mean(type1), digits = 3)\n        ave.type2[i] = round(mean(type2), digits = 3)\n}\n\nave.type1\n\n [1] 0.164 0.142 0.098 0.102 0.111 0.089 0.085 0.093 0.089 0.085 0.084 0.093\n[13] 0.085 0.080 0.085 0.080 0.080 0.080 0.080 0.063 0.067 0.067 0.067 0.071\n[25] 0.071 0.080 0.071 0.067 0.067 0.071 0.058 0.049 0.049 0.044 0.044 0.049\n[37] 0.040 0.035 0.035 0.031 0.035 0.036 0.031 0.027 0.027 0.022 0.022 0.022\n[49] 0.022 0.022\n\nave.type2\n\n [1] 0.634 0.630 0.707 0.654 0.640 0.655 0.652 0.688 0.713 0.727 0.690 0.715\n[13] 0.678 0.691 0.678 0.700 0.715 0.739 0.690 0.715 0.691 0.703 0.703 0.727\n[25] 0.703 0.728 0.764 0.764 0.753 0.776 0.752 0.753 0.779 0.790 0.814 0.789\n[37] 0.801 0.835 0.837 0.826 0.850 0.850 0.862 0.863 0.889 0.887 0.887 0.899\n[49] 0.899 0.912\n\n\nReport the length of vector ave.type1 and the length of vector ave.type2.\n\nlength(ave.type1) # 50, same as K\n\n[1] 50\n\nlength(ave.type2) # 50, same as K\n\n[1] 50\n\n\nWrite the code to produce a scatter plot where ave.type1 is in X-axis and ave.type2 is in Y-axis.\n\nplot(ave.type1, ave.type2, pch = 20)\n\n\n\n\n\n\n\n\nFor this study, we assume that type I error can be tolerated while type II error is not. Which value of K would you choose among the three smallest type II error rate, yet the type I error rate is not larger than 15%? Report the type I and type II error rate for that value of K.\n\nsort(ave.type2)[1:3] \n\n[1] 0.630 0.634 0.640\n\n# index of K that produced three smallest type 2 error\nindex = which(ave.type2 %in% c(sort(ave.type2)[1:3])) \nindex\n\n[1] 1 2 5\n\n# values of type 1 error for those K with 3 smallest type 2 error\nave.type1[index] \n\n[1] 0.164 0.142 0.111\n\n# among those values of K, we choose the one that gives types 1 error smaller than 15%\n# values of K could be chosen are: 2, 5\n# since K = 2 gives second smallest type 2 error = 63.4% with type 1 error = 14.2%,\n# and K = 5 gives third smalles type 2 error = 64% with type 1 error = 11.1%."
  },
  {
    "objectID": "index.html#y2324-s1",
    "href": "index.html#y2324-s1",
    "title": "DSA1101: Everything You Need",
    "section": "Y(23/24) S1",
    "text": "Y(23/24) S1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#tutorial-5-linreg-knn",
    "href": "index.html#tutorial-5-linreg-knn",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 5 (Linreg & KNN)",
    "text": "Tutorial 5 (Linreg & KNN)\n\nOffsite Questions\n\n(MLR) Consider the horseshoe female crab data given in the csv file crab.csv. We would want to form a model for the weight of the female crabs (kg), which depends on its width (cm) and its spine condition (1 = both good, 2 = one worn or broken, 3 = both worn or broken).\n\ndata &lt;- read.csv(\"crab.csv\")\nhead(data)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : int  3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\ndata$spine &lt;- as.factor(data$spine)\n# 1 = both good\n# 2 = one worn or broken\n# 3 = both worn or broken\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\n\n(a) Produce a scatter plot of variable weight against width for different condition of spine.\n\nattach(data)\nplot(weight ~ width, pch = 20, main = \"weight vs width for each spine class\",\n     xlab = \"width\", ylab = \"weight\")\npoints(weight[spine == \"1\"] ~ width[spine ==\"1\"], pch = 15, col = \"red\")\npoints(weight[spine == \"2\"] ~ width[spine ==\"2\"], pch = 16, col = \"blue\")\npoints(weight[spine == \"3\"] ~ width[spine ==\"3\"], pch = 17, col = \"green\")\nlegend(22,5,\n       legend=c(\"Spine = 1\", \"Spine = 2\", \"Spine = 3\"),\n       col=c(\"red\", \"blue\", \"green\"), \n       pch=c(15,16,17)\n       )\n\n\n\n\n\n\n\n\n(b) Fit a linear regression model for weight which has two explanatories, width and spine.\n\nmodel &lt;- lm(weight ~ width + spine, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n\n(c) Is the fitted model significant?\n\n# the model f statistic showing p value is less than 2.2 x 10^-16.\n# Since this is far below 0.05, the model can be considered significant\n\n(d) Derive \\(R^2\\) and adjusted \\(R^2\\) of the fitted model.\n\nsummary(model)$r.squared\n\n[1] 0.7917598\n\n# 0.7917598\nsummary(model)$adj.r.squared\n\n[1] 0.7880632\n\n# 0.7880632\n\n(e) Write down the fitted model.\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) - 0.06969*I(spine=3)\n\n(f) Two female crabs of the same width, and the difference of their weight if one has spines are of good condition and another one with broken spines.\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) - 0.06969*I(spine=3)\n# weight1 = -3.92955 + 0.24376*width + 0.05544*0 - 0.06969*0\n# weight2 = -3.92955 + 0.24376*width + 0.05544*0 - 0.06969*1\n\nweight.dif &lt;- (0.05544*0 - 0.06969*0) - (0.05544*0 - 0.06969*1)\nweight.dif\n\n[1] 0.06969\n\n# When they have the same width, then on average the one that has spines\n# of good condition is heavier than the one with broken spines by 0.06969 kg.\n\n(g) Predict the weight of a female crab that has width of 27 cm and has both spines worn or broken.\n\nnewdata &lt;- data.frame(width = 27, spine = \"3\")\npredict(model, newdata = newdata)\n\n       1 \n2.582352 \n\n\nMeasures of classier performance\nSuppose we have developed a K-nearest neighbors classier for predicting diabetes status. The following table shows the actual response \\(Y\\) (1 = yes, 0 =n o) and fitted value \\(\\hat Y\\) using the classier for 10 test data points. A test data point is predicted to be \\(\\hat G\\)= 1 if \\(\\hat Y\\) &gt; Î´, for a specied threshold value Î´ (Recall that we use Î´ = 0.5 in class, also known as the majority rule).\n\n\nWe define: \\(TPR = \\frac {TP}{TP +FN}\\), \\(FPR = \\frac {FP}{FP +TN}\\); For each of the thresholds Î´ = 0.3, 0.6 and 0.8, derive \\(TPR\\) and \\(FPR\\) in making predictions with the K-nearest neighbors classier for the 10 test data points. Plot \\(TPR\\) against \\(FPR\\) for the three thresholds.\n\n\n\ny &lt;- c(1,1,0,1,1,0,0,1,0,0)\nycap &lt;- c(0.9,0.5,0.7,0.4,0.5,0.2,0.7,0.9,0.1,0.1)\ntpr.all &lt;- numeric(0)\nfpr.all &lt;- numeric(0)\n\ndelta.list &lt;- c(0.3,0.6,0.8)\nfor (delta in delta.list) {\n  pred &lt;- ifelse(ycap &gt;= delta, 1 ,0)\n  confusion.matrix &lt;- table(y, pred)\n  tpr &lt;- confusion.matrix[2,2]/sum(confusion.matrix[2,])\n  fpr &lt;- confusion.matrix[1,2]/sum(confusion.matrix[1,])\n  tpr.all &lt;- append(tpr.all, tpr)\n  fpr.all &lt;- append(fpr.all, fpr)\n}\n\nplot(tpr.all ~ fpr.all, type = \"n\", pch = 20,\n     xlim = c(0,1),\n     ylim = c(0,1))\npoints(tpr.all[1] ~ fpr.all[1], pch = 15, col = \"red\")\npoints(tpr.all[2] ~ fpr.all[2], pch = 16, col = \"blue\")\npoints(tpr.all[3] ~ fpr.all[3], pch = 17, col = \"green\")\nlegend(0.7,0.4,legend=c(\"sigma = 0.3\", \"sigma = 0.6\", \"sigma = 0.8\"),\n       col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Can we add the two points (0,0) and (1,1) to the plot of \\(TPR\\) against \\(FPR\\) in part (a). Explain why or why not.\n\n# If Ïƒ &gt; 0.9 then all test points have predicted Ë†G = 0 (predicted as negative), \n# so TPR = FPR = 0.\n# If Ïƒ &lt; 0.1, then all test points have predicted Ë†G = 1 (predicted as positive),\n# so TPR = FPR = 1.\n# Since there exist Ïƒ within the range from 0 to 1 for the two points to happen, \n# these two points can be added to the plot.\n\nThe CSV file Caravan.csv contains data on 5822 real customer records on caravan insurance purchase. This data set is owned and supplied by the Dutch data mining company, Sentient Machine Research, and is based on real world business data. Each record consists of 86 variables, containing socio-demographic data (variables 1-43) and product ownership (variables 44-86). Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy. For this business, assume that the overall error rate (equivalently, the accuracy) is not of interest. Instead, the company wants to use the classier to predict who are the potential customers likely to purchase insurance. Then the metric precision will be important, since it relates the proportion of Individuals who will actually purchase the insurance, among the group of individuals who are predicted to purchase insurance.\n\ncara &lt;- read.csv(\"Caravan data.csv\")\nhead(cara, n = 2)\n\n  X MOSTYPE MAANTHUI MGEMOMV MGEMLEEF MOSHOOFD MGODRK MGODPR MGODOV MGODGE\n1 1      33        1       3        2        8      0      5      1      3\n2 2      37        1       2        2        8      1      4      1      4\n  MRELGE MRELSA MRELOV MFALLEEN MFGEKIND MFWEKIND MOPLHOOG MOPLMIDD MOPLLAAG\n1      7      0      2        1        2        6        1        2        7\n2      6      2      2        0        4        5        0        5        4\n  MBERHOOG MBERZELF MBERBOER MBERMIDD MBERARBG MBERARBO MSKA MSKB1 MSKB2 MSKC\n1        1        0        1        2        5        2    1     1     2    6\n2        0        0        0        5        0        4    0     2     3    5\n  MSKD MHHUUR MHKOOP MAUT1 MAUT2 MAUT0 MZFONDS MZPART MINKM30 MINK3045 MINK4575\n1    1      1      8     8     0     1       8      1       0        4        5\n2    0      2      7     7     1     2       6      3       2        0        5\n  MINK7512 MINK123M MINKGEM MKOOPKLA PWAPART PWABEDR PWALAND PPERSAUT PBESAUT\n1        0        0       4        3       0       0       0        6       0\n2        2        0       5        4       2       0       0        0       0\n  PMOTSCO PVRAAUT PAANHANG PTRACTOR PWERKT PBROM PLEVEN PPERSONG PGEZONG\n1       0       0        0        0      0     0      0        0       0\n2       0       0        0        0      0     0      0        0       0\n  PWAOREG PBRAND PZEILPL PPLEZIER PFIETS PINBOED PBYSTAND AWAPART AWABEDR\n1       0      5       0        0      0       0        0       0       0\n2       0      2       0        0      0       0        0       2       0\n  AWALAND APERSAUT ABESAUT AMOTSCO AVRAAUT AAANHANG ATRACTOR AWERKT ABROM\n1       0        1       0       0       0        0        0      0     0\n2       0        0       0       0       0        0        0      0     0\n  ALEVEN APERSONG AGEZONG AWAOREG ABRAND AZEILPL APLEZIER AFIETS AINBOED\n1      0        0       0       0      1       0        0      0       0\n2      0        0       0       0      1       0        0      0       0\n  ABYSTAND Purchase\n1        0       No\n2        0       No\n\nstr(cara)\n\n'data.frame':   5822 obs. of  87 variables:\n $ X       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MOSTYPE : int  33 37 37 9 40 23 39 33 33 11 ...\n $ MAANTHUI: int  1 1 1 1 1 1 2 1 1 2 ...\n $ MGEMOMV : int  3 2 2 3 4 2 3 2 2 3 ...\n $ MGEMLEEF: int  2 2 2 3 2 1 2 3 4 3 ...\n $ MOSHOOFD: int  8 8 8 3 10 5 9 8 8 3 ...\n $ MGODRK  : int  0 1 0 2 1 0 2 0 0 3 ...\n $ MGODPR  : int  5 4 4 3 4 5 2 7 1 5 ...\n $ MGODOV  : int  1 1 2 2 1 0 0 0 3 0 ...\n $ MGODGE  : int  3 4 4 4 4 5 5 2 6 2 ...\n $ MRELGE  : int  7 6 3 5 7 0 7 7 6 7 ...\n $ MRELSA  : int  0 2 2 2 1 6 2 2 0 0 ...\n $ MRELOV  : int  2 2 4 2 2 3 0 0 3 2 ...\n $ MFALLEEN: int  1 0 4 2 2 3 0 0 3 2 ...\n $ MFGEKIND: int  2 4 4 3 4 5 3 5 3 2 ...\n $ MFWEKIND: int  6 5 2 4 4 2 6 4 3 6 ...\n $ MOPLHOOG: int  1 0 0 3 5 0 0 0 0 0 ...\n $ MOPLMIDD: int  2 5 5 4 4 5 4 3 1 4 ...\n $ MOPLLAAG: int  7 4 4 2 0 4 5 6 8 5 ...\n $ MBERHOOG: int  1 0 0 4 0 2 0 2 1 2 ...\n $ MBERZELF: int  0 0 0 0 5 0 0 0 1 0 ...\n $ MBERBOER: int  1 0 0 0 4 0 0 0 0 0 ...\n $ MBERMIDD: int  2 5 7 3 0 4 4 2 1 3 ...\n $ MBERARBG: int  5 0 0 1 0 2 1 5 8 3 ...\n $ MBERARBO: int  2 4 2 2 0 2 5 2 1 3 ...\n $ MSKA    : int  1 0 0 3 9 2 0 2 1 1 ...\n $ MSKB1   : int  1 2 5 2 0 2 1 1 1 2 ...\n $ MSKB2   : int  2 3 0 1 0 2 4 2 0 1 ...\n $ MSKC    : int  6 5 4 4 0 4 5 5 8 4 ...\n $ MSKD    : int  1 0 0 0 0 2 0 2 1 2 ...\n $ MHHUUR  : int  1 2 7 5 4 9 6 0 9 0 ...\n $ MHKOOP  : int  8 7 2 4 5 0 3 9 0 9 ...\n $ MAUT1   : int  8 7 7 9 6 5 8 4 5 6 ...\n $ MAUT2   : int  0 1 0 0 2 3 0 4 2 1 ...\n $ MAUT0   : int  1 2 2 0 1 3 1 2 3 2 ...\n $ MZFONDS : int  8 6 9 7 5 9 9 6 7 6 ...\n $ MZPART  : int  1 3 0 2 4 0 0 3 2 3 ...\n $ MINKM30 : int  0 2 4 1 0 5 4 2 7 2 ...\n $ MINK3045: int  4 0 5 5 0 2 3 5 2 3 ...\n $ MINK4575: int  5 5 0 3 9 3 3 3 1 3 ...\n $ MINK7512: int  0 2 0 0 0 0 0 0 0 1 ...\n $ MINK123M: int  0 0 0 0 0 0 0 0 0 0 ...\n $ MINKGEM : int  4 5 3 4 6 3 3 3 2 4 ...\n $ MKOOPKLA: int  3 4 4 4 3 3 5 3 3 7 ...\n $ PWAPART : int  0 2 2 0 0 0 0 0 0 2 ...\n $ PWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSAUT: int  6 0 6 6 0 6 6 0 5 0 ...\n $ PBESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PTRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBROM   : int  0 0 0 0 0 0 0 3 0 0 ...\n $ PLEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBRAND  : int  5 2 2 2 6 0 0 0 0 3 ...\n $ PZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAPART : int  0 2 1 0 0 0 0 0 0 1 ...\n $ AWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSAUT: int  1 0 1 1 0 1 1 0 1 0 ...\n $ ABESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ ATRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABROM   : int  0 0 0 0 0 0 0 1 0 0 ...\n $ ALEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABRAND  : int  1 1 1 1 1 0 0 0 0 1 ...\n $ AZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Purchase: chr  \"No\" \"No\" \"No\" \"No\" ...\n\ndim(cara)\n\n[1] 5822   87\n\n\n(a) Without any classier, if the company tries to sell insurance to a random selection of customers, what is the success rate?\n\ntable(cara$Purchase)\n\n\n  No  Yes \n5474  348 \n\nsuccesRate &lt;- table(cara$Purchase)[2]/sum(table(cara$Purchase))\nsuccesRate\n\n       Yes \n0.05977327 \n\n# data set shows almost 6% of people purchased insurance\n\n(b) Standardize the input features. \\(Hint\\): Use scale() command in R.\n\ncara &lt;- cara[,-1] #throwing the first column since it provides no information\nscaled.X &lt;- scale(cara[,-86]) #throwing the response column\n\n(c) Randomly select 1000 observations to form the test data, and the remaining observations will be the training data.\n\nset.seed(99)\nn &lt;- dim(cara)[1] #train data number\nshuffled.index &lt;- sample(c(1:n), size = 2000)\nX.train &lt;- scaled.X[-shuffled.index,]\nX.test&lt;- scaled.X[shuffled.index,]\nY.train &lt;- as.factor(cara$Purchase[-shuffled.index])\nY.test &lt;- as.factor(cara$Purchase[shuffled.index])\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a customer will purchase insurance. Compute the precision of the classier.\n\nlibrary(class)\n\npred &lt;- knn(X.train, X.test, Y.train, k=1)\nconfusion.matrix &lt;- table(Y.test, pred) \n# Precision = TP/TP+FP\nprecision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\nconfusion.matrix\n\n      pred\nY.test   No  Yes\n   No  1761  121\n   Yes  101   17\n\nprecision\n\n[1] 0.1231884\n\n\n(e) Repeat question 3d, for k-nearest neighbor classier where k = 3,5,10. Which value of k gives the best precision?\n\nprecision.all &lt;- data.frame()\nk.list &lt;- c(3, 5, 10)\nfor (k in k.list) {\n  pred &lt;- knn(X.train, X.test, Y.train, k = k)\n  confusion.matrix &lt;- table(Y.test, pred) \n  # Precision = TP/TP+FP\n  precision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\n  precision.all &lt;- rbind(precision.all, data.frame(k, precision))\n}\n\nprecision.all\n\n   k precision\n1  3 0.2285714\n2  5 0.3571429\n3 10 0.0000000\n\n# So far, k = 5 gives the best precision.\n# However, one might use N -fold cross validation \n# to have the average precision for each k.\n# With that, the value of k that gives largest average precision is chosen.\n\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-6-knn-decision-tree",
    "href": "index.html#tutorial-6-knn-decision-tree",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 6 (KNN & Decision Tree)",
    "text": "Tutorial 6 (KNN & Decision Tree)\n\nOffsite Questions\n\n(KNN and N -fold Cross Validation)\nLoan managers often need to take into account an applicantâ€™s demographic and socioeconomic proles in deciding whether to approve a loan to the applicant, to minimize losses due to defaults. In this exercise we will build and evaluate a classier based on the German Credit Data to predict whether an applicant is considered as having good or bad credit risk. The features or predictors include (1) loan duration (in months), (2) credit amount, (3) Installment rate in percentage of disposable income and (4) age in years.\n(a) Read and explore the data from the file German_credit.csv.\n\ndata &lt;- read.csv(\"German credit.csv\")\nhead(data)\n\n  Creditability Duration Amount Instalment Age\n1             1       18   1049          4  21\n2             1        9   2799          2  36\n3             1       12    841          2  23\n4             1       12   2122          3  39\n5             1       12   2171          4  38\n6             1       10   2241          1  48\n\nstr(data)\n\n'data.frame':   1000 obs. of  5 variables:\n $ Creditability: int  1 1 1 1 1 1 1 1 1 1 ...\n $ Duration     : int  18 9 12 12 12 10 8 6 18 24 ...\n $ Amount       : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n $ Instalment   : int  4 2 2 3 4 1 1 2 4 1 ...\n $ Age          : int  21 36 23 39 38 48 39 40 65 23 ...\n\n\n(b) Standardize the input features.\n\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\nhead(X)\n\n       Duration     Amount  Instalment         Age\n[1,] -0.2407368 -0.7872630  0.91801781 -1.28093214\n[2,] -0.9870788 -0.1673006 -0.86974813  0.04034293\n[3,] -0.7382981 -0.8609500 -0.86974813 -1.10476213\n[4,] -0.7382981 -0.4071375  0.02413484  0.30459795\n[5,] -0.7382981 -0.3897785  0.91801781  0.21651294\n[6,] -0.9041519 -0.3649800 -1.76363111  1.09736299\n\n\n(c) Randomly select 800 customer records to form the training data, and the remaining 200 records will be the test data.\n\nset.seed(100)\nn &lt;- dim(data)[1]\nindex &lt;- sample(1:n, 800)\n\ntraining.X &lt;- X[index, ]\ntest.X &lt;- X[-index, ]\ntraining.Y &lt;- Y[index]\ntest.Y &lt;- Y[-index]\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a loan applicant is credible for the 200 test points. Compute the accuracy of the classier.\n\nlibrary(class)\npred &lt;- knn(training.X, test.X, training.Y, k=1)\nconfusion.matrix &lt;- table(test.Y, pred)\nconfusion.matrix\n\n      pred\ntest.Y  0  1\n     0 24 35\n     1 47 94\n\naccuracy &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\naccuracy\n\n[1] 0.59\n\n\n(e) Use N -folds cross validation with N = 5 to find the average accuracy for the 1-nearest neighbor classifier.\n\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\n\naccuracy &lt;- numeric(n_folds)\nfor (j in 1:n_folds) {\n  test_j &lt;- which(folds_j == j) \n  test.y &lt;- Y[test_j] \n  train.X &lt;- X[-test_j, ]  \n  test.X &lt;- X[test_j, ]  \n  train.y &lt;- Y[-test_j] \n\n  knn.pred &lt;- knn(train.X, test.X, train.y, k = 1) \n\n  confusion.matrix &lt;- table(test.y, knn.pred)\n  accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n}\n\nave.accuracy &lt;- round(mean(accuracy), digits = 3)\nave.accuracy\n\n[1] 0.622\n\n\n(f) Repeat question 1e for K-nearest neighbor classifiers where K = 1, 2, â€¦100.\n\n### FULL KNN WITH N-FOLDS VALIDATION\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nk &lt;- 100\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\nave.accuracy &lt;- numeric(length(k))\n\nfor (i in 1:k) {\n  accuracy &lt;- numeric(n_folds)\n  for (j in 1:n_folds) {\n    test_j &lt;- which(folds_j == j) \n    test.y &lt;- Y[test_j] \n    train.X &lt;- X[-test_j, ]  \n    test.X &lt;- X[test_j, ]  \n    train.y &lt;- Y[-test_j] \n\n    knn.pred &lt;- knn(train.X, test.X, train.y, k = i) \n\n    confusion.matrix=table(test.y, knn.pred)\n    accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n  }\n  ave.accuracy[i] &lt;- round(mean(accuracy), digits = 3)\n}\n\nave.accuracy\n\n  [1] 0.622 0.597 0.661 0.674 0.684 0.671 0.676 0.686 0.686 0.691 0.702 0.699\n [13] 0.705 0.702 0.707 0.695 0.699 0.702 0.700 0.693 0.695 0.695 0.697 0.695\n [25] 0.699 0.697 0.696 0.700 0.693 0.688 0.697 0.692 0.697 0.698 0.697 0.692\n [37] 0.695 0.695 0.695 0.696 0.696 0.701 0.699 0.701 0.699 0.699 0.699 0.698\n [49] 0.699 0.696 0.698 0.699 0.699 0.696 0.695 0.699 0.696 0.697 0.696 0.698\n [61] 0.697 0.696 0.699 0.697 0.697 0.697 0.697 0.696 0.698 0.699 0.699 0.696\n [73] 0.698 0.697 0.698 0.696 0.698 0.698 0.698 0.699 0.699 0.698 0.698 0.700\n [85] 0.698 0.698 0.700 0.700 0.699 0.700 0.700 0.701 0.701 0.701 0.700 0.700\n [97] 0.699 0.698 0.699 0.699\n\n\n(g) Compare the 100 classifiers above, which few values of K give the best average accuracy?\n\nlibrary(glue)\nindex = which(ave.accuracy == max(ave.accuracy))\nplot(x = 1:k, ave.accuracy)\nabline(v = index, col = \"red\")\n\n\n\n\n\n\n\nglue(\"most accurate k = {index} with average accuracy = {ave.accuracy[index]}\")\n\nmost accurate k = 15 with average accuracy = 0.707\n\n\n(Decision Trees)\nConsider the famous Iris Flower Data set which was rst introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation: the length and the width of the sepals and petals (in cm).\n\n(a) Use decision tree method to predict Iris species based on all four features.\n\niris &lt;- read.csv(\"Iris dataset.csv\")\nhead(iris)\n\n  sepal.length sepal.width petal.length petal.width       class\n1          5.1         3.5          1.4         0.2 Iris-setosa\n2          4.9         3.0          1.4         0.2 Iris-setosa\n3          4.7         3.2          1.3         0.2 Iris-setosa\n4          4.6         3.1          1.5         0.2 Iris-setosa\n5          5.0         3.6          1.4         0.2 Iris-setosa\n6          5.4         3.9          1.7         0.4 Iris-setosa\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ sepal.length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ sepal.width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ petal.length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal.width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\ndim(iris)\n\n[1] 150   5\n\nlibrary(rpart)\nfit &lt;- rpart(class ~ sepal.length + sepal.width + petal.length + petal.width,\n              data = iris,\n              method=\"class\",\n              parms=list(split='information'),\n              control = rpart.control( minsplit =1))\n\n(b) Visualize the decision tree above, using the rpart.plot function.\n\nlibrary(rpart.plot)\nrpart.plot(fit, type=4, extra=2)\n\n\n\n\n\n\n\n# The fitted tree is given in the figure below.\n# If the measurement of petal length is less than 2.5 cm \n# then the flower is of Iris-setosa\n# If the petal length is â‰¥ 2.5 cm with the petal width is â‰¥ 1.8 cm \n# then high chance (45/46) it will be an Iris-virginica.\n# If the petal length is â‰¥ 2.5 cm with the petal width is &lt; 1.8 cm\n# then we continue to check if the petal length is in the interval\n# [2.5, 5]. If yes, then high chance (47/48) it is Iris-versicolor.\n\n(c) What are the more important features in the fitted tree above?\n\n# It seems the sepal length and sepal width \n# are not important in the classification while\n# the petal length and petal width are more important."
  }
]