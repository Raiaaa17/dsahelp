[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "",
    "text": "Hi! I’m Ray! Welcome to your ultimate DSA1101 survival guide! 🎉 Whether you’re new to data science or leveling up for finals, this helpsheet blog has bite-sized tips, easy code, and all the cheat codes you need. 🚀 From data wrangling to machine learning, I’ve got you covered. Ready to dive in? Let’s go! 😎"
  },
  {
    "objectID": "index.html#working-directory",
    "href": "index.html#working-directory",
    "title": "DSA1101: Everything You Need",
    "section": "Working Directory",
    "text": "Working Directory\n\n# setwd(\"/Users/rayana/Documents/DSA1101/\")\n# getwd()"
  },
  {
    "objectID": "index.html#reading-file",
    "href": "index.html#reading-file",
    "title": "DSA1101: Everything You Need",
    "section": "Reading File",
    "text": "Reading File\n\ndata &lt;- read.table(\"crab.csv\", header = FALSE, sep = \"\")\ndata &lt;- read.csv(\"crab.csv\", header = TRUE, sep = \",\") \n\n# Reading a tab-separated file\ndata &lt;- read.delim(\"crab.csv\")\ndata &lt;- read.csv(\"crab.csv\", sep = \"\\t\")"
  },
  {
    "objectID": "index.html#common-plots",
    "href": "index.html#common-plots",
    "title": "DSA1101: Everything You Need",
    "section": "Common Plots",
    "text": "Common Plots\n\n# pie.chart &lt;- pie(x, labels, radius, main, col, clockwise)\n# bar.chart &lt;- barplot(H, xlab, ylab, main, names.arg, col)\n# box.chart &lt;- boxplot(x, data, notch, varwidth, names, main)\n# histogram &lt;- hist(v, main, xlab, xlim, ylim, breaks, col, border)\n# linegraph &lt;- plot(v, type, col, xlab, ylab)\n# scatterplot &lt;- plot(x, y, xlab, ylab, xlim, ylim, axes)"
  },
  {
    "objectID": "index.html#uselful-functions",
    "href": "index.html#uselful-functions",
    "title": "DSA1101: Everything You Need",
    "section": "Uselful Functions",
    "text": "Uselful Functions\nHere are some functions that will make your life so much easier—but let’s be real, you probably forget how to use them half the time! 😅\n\nFunction: ifelse()\n\nx &lt;- 1:10\nx &lt;- ifelse(x %% 2 == 0, \"even\", \"odd\")\nx\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n\n\nFunction: which()\n\ndf &lt;- data.frame(\nx = c(1, 2, 3, 4, 5), \ny = c(5, 4, 3, 2, 1))\n\nrows &lt;- which(df$x &gt; 3)\nrows\n\n[1] 4 5\n\n\n\n\nFunction: append()\n\nvec &lt;- c(1, 2)\ncombined_vec &lt;- append(vec, 3)\ncombined_vec\n\n[1] 1 2 3"
  },
  {
    "objectID": "index.html#interpreting-scatterplots",
    "href": "index.html#interpreting-scatterplots",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Scatterplots",
    "text": "Interpreting Scatterplots\n\nIs there any relationship? Is it strong?\nIf there is, is it positive or negative?\nRelationship is linear or non-linear?\nSpecial observations?\nIs the variability of the response stable when x changes?\n\n\ndata &lt;- read.csv(\"house_selling_prices_FL.csv\")\nplot(data$price ~ data$size, pch = 20)\n\n\n\n\n\n\n\n# Comments:\n# 1. There is a clear relationship.\n# 2. It is a positive association.\n# 3. It seems linear.\n# 4. Variability of price is quite stable as size changes.\n\n\nTwo Categorical Scatterplots\n\nfev &lt;- read.csv(\"FEV.csv\")\nfemale = fev$FEV[which(fev$Sex==0)]\n# Alternative: fev$FEV[fev$Sex==0]\nmale = fev$FEV[which(fev$Sex==1)] \n# Alternative: fev$FEV[fev$Sex==1]\n\nplot(fev$height, fev$FEV, type = \"n\")\npoints(female ~ fev$height[which(fev$Sex==0)], \ncol = \"red\", pch = 20)\npoints(male ~ fev$height[which(fev$Sex==1)], \ncol = \"darkblue\", pch = 20)\nlegend(1.2, 5, legend = c(\"Female\", \"Male\"), \ncol = c(\"red\",\"darkblue\"), pch=c(20,20))\n\n\n\n\n\n\n\n# Comments:\n# Computed correlation is quite high.\n# It is clear from the plot that there is a strong \n# positive linear association between FEV and height. \n# The range of FEV and height for males appears larger than for females. \n# The variability of FEV at lower heights seems slightly \n# less than the variability of FEV at greater heights."
  },
  {
    "objectID": "index.html#interpreting-histograms",
    "href": "index.html#interpreting-histograms",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Histograms",
    "text": "Interpreting Histograms\n\nRange\nBimodal/Unimodal\nSymmetric/Skewed\nData has Gaps/is Clustered\nSuspected outliers\n\n\nhdb &lt;- read.csv(\"hdbresale_reg.csv\")\nhist(hdb$resale_price)\n\n\n\n\n\n\n\n# Comments:\n# range: 200k to ~1 mil\n# unimodal\n# clearly right skewed\n# suspected outliers\n\nfev &lt;- read.csv(\"FEV.csv\")\nhist(fev$FEV)\n\n\n\n\n\n\n\n# Comments:\n# range: 0.5 to 6\n# unimodal\n# slightly right skewed\n# may have outliers\n\n\nCreating Separates Histograms\n\nfev &lt;- read.csv(\"FEV.csv\")\nfemale = fev$FEV[which(fev$Sex==0)]\n# Alternative: fev$FEV[fev$Sex==0]\nmale = fev$FEV[which(fev$Sex==1)] \n# Alternative: fev$FEV[fev$Sex==1]\n\nopar &lt;- par(mfrow=c(1,2)) \nhist(female, col = 2, freq= FALSE, \nmain = \"Histogram of Female FEV\", ylim = c(0,0.52))\nhist(male, col = 4, freq= FALSE, \nmain = \"Histogram of Male FEV\", ylim = c(0,0.52))\n\n\n\n\n\n\n\n# Remember to use par(mfrow=c(1,1)) to reset the diagram layout!\n\n# Comments:\n# Both histogram are unimodal but have different shapes. \n# It is almost symmetrical for females, but quite \n# right-skewed for males.\n# Median FEV for females is much lower than males,\n# 2.49 compared to 2.605.\n# Variability for males is higher than females.\n# The respective IQR are 1.54 and 1.05."
  },
  {
    "objectID": "index.html#interpreting-boxplots",
    "href": "index.html#interpreting-boxplots",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Boxplots",
    "text": "Interpreting Boxplots\n\nHow many outliers\nMedian\nDistribution\nAny visible pattern? (Optional)\n\n\nhbd &lt;- read.csv(\"hdbresale_reg.csv\")\nbp &lt;- boxplot(hdb$resale_price)\n\n\n\n\n\n\n\noutliers &lt;- bp$out\nhead(outliers)\n\n[1] 680000 728000 680000 707000 640000 659500"
  },
  {
    "objectID": "index.html#interpreting-q-q-plot",
    "href": "index.html#interpreting-q-q-plot",
    "title": "DSA1101: Everything You Need",
    "section": "Interpreting Q-Q Plot",
    "text": "Interpreting Q-Q Plot\n\nfev &lt;- read.csv(\"FEV.csv\")\nqqnorm(fev$FEV, pch = 20)\nqqline(fev$FEV, col = \"red\")\n\n\n\n\n\n\n\n# Comments:\n# Left tail sample quantiles are larger than expected,\n# hence left tail shorter than normal. \n# Right tail sample quantiles larger than expected, hence # right tail longer than normal.\n\n# Combined with the histogram of FEV, it is clear that the\n# sample of FEV is not normally distributed and quite \n# right skewed."
  },
  {
    "objectID": "index.html#linear-regression-model",
    "href": "index.html#linear-regression-model",
    "title": "DSA1101: Everything You Need",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nThere are assumptions have to be fulfilled before using linear model:\n\nQuantitative response (check through str() function)\nRemember to as.factor() categorical features\nSymmetric (check through histogram and/or qqplot)\nVariability of y is stable when x changes (check through scatterplot)\n\nIf data is found to be asymmetric, can do transformation: log(Y), sqrt(Y), 1/Y\n\n\n\n\n\n\n\n\nTransformation\nRestriction\nReverse Transformation\n\n\n\n\nnew_Y &lt;- log(Y)\nY can’t be 0 nor negative\npred &lt;- exp(pred)\n\n\nnew_Y &lt;- sqrt(Y)\nY can’t be negative\npred &lt;- pred**2\n\n\nnew_Y &lt;- 1/Y\nY can’t be 0\npred &lt;- 1/pred\n\n\n\nTo evaluate goodness of fit, we consider F-test and \\(R^2\\). When comparing models however, use adjusted \\(R^2\\)"
  },
  {
    "objectID": "index.html#k-nearest-neighbor-knn-model",
    "href": "index.html#k-nearest-neighbor-knn-model",
    "title": "DSA1101: Everything You Need",
    "section": "K-Nearest Neighbor (KNN) Model",
    "text": "K-Nearest Neighbor (KNN) Model\nThere are some important steps when doing KNN:\n\nImporting the required library\n\nlibrary(class)\n\nStandardizing NUMERIC input features.\n\nVery important, especially when they are in different magnitudes.\nUse scale() Function\n\nRandomly split original data to train and test set.\n\nUsually 80% train and 20% split.\nUse shuffle() function\nRemember to set.seed()\n\n\nTo evaluate goodness of fit, N-fold cross validation is usually do to. It’s a fair way to evaluate most of classifier models. Confusion matrix is also used to check accuracy, precision, etc.\nBayes decision boundary (the gold standard decision boundary) is also often used to check the goodness of fit, especially in choosing the right K.\n\nSmall K (more flexibility): When K is small, like K = 1, the KNN model is more flexible, fitting the data closely and capturing non-linear patterns. This allows it to adapt better to local variations in the data, which is important for non-linear decision boundaries. However, small KKK values can also make the model more sensitive to noise and prone to overfitting.\nLarge K (More Smoothing): A large K smooths out the decision boundary because the prediction is averaged over more neighbors. This can be useful for linear or simple decision boundaries, but in the case of highly non-linear boundaries, a large K would result in underfitting, where the model oversimplifies the decision boundary and fails to capture the complex patterns."
  },
  {
    "objectID": "index.html#decision-tree-model",
    "href": "index.html#decision-tree-model",
    "title": "DSA1101: Everything You Need",
    "section": "Decision Tree Model",
    "text": "Decision Tree Model\nThere are some important steps when doing Decision Tree:\n\nimporting the required library\n\nlibrary(rpart) : minsplit, maxdepth, cp\nlibrary(rpart.plot) : varlen, faclen\n\nUnderstanding the fitted tree. It’s crucial to know the how and why of the fitted tree to avoid bias\n\nTo evaluate goodness of fit, N-fold cross validation can do. But most of the times confusion matrix is used."
  },
  {
    "objectID": "index.html#confusion-matrix",
    "href": "index.html#confusion-matrix",
    "title": "DSA1101: Everything You Need",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nNot a ML model, but a way to evaluate most of classifiers\n\n\n\n\n\nSome other formulas:"
  },
  {
    "objectID": "index.html#tutorial-1-introduction-to-r",
    "href": "index.html#tutorial-1-introduction-to-r",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 1 (Introduction to R)",
    "text": "Tutorial 1 (Introduction to R)\n\nOffsite Questions\nGeneral idea of this problem: You have just graduated from NUS and just started your first job. You plan to buy a flat on your own which has price = $1,200,000 (1.2 million dollars). You need to save money for several years before you can afford to make the down payment which is 25% of the flat’s price.\n\nCall the amount that you have saved thus far: saved. You start the very first month with a savings of $10,000 that your parents gave you.\nCall your monthly salary as salary which is paid at the end of every month. Each month, you are going to dedicate 40% of your salary to save for the down payment.\nAssume that you invest your savings wisely, with a monthly average return of 2%. That means: at the end of each month, you receive an additional of saved ×0.02 funds where saved is the amount you have from end of previous month to put into your savings.\nAt the end of each month, your savings will be increased by the return on your investment, plus 40% of your monthly salary.\n\nNote: In your code for the questions below, you MUST use the names as given in bold above.\n\nWrite the code to calculate how many months it will take you to save up enough money for the down payment for two persons of of different salary: (i) salary = $7,000; and (ii) salary = $10,000.\n\n# Solution 1.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 7000\nsaved &lt;- 10000\nmonth &lt;- 0\n\nwhile(saved &lt; cost){\nmonth &lt;- month + 1\nsaved &lt;- saved + 0.4 * salary + 0.02 * saved }\nprint(month) # 55\n\n[1] 55\n\n# Solution 2.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 10000\nsaved &lt;- 10000\nmonth &lt;- 0\n\nwhile(saved &lt; cost){\nmonth &lt;- month + 1\nsaved &lt;- saved + 0.4 * salary + 0.02 * saved }\nprint(month) # 44\n\n[1] 44\n\n\nIn question above, we unrealistically assumed that the salary doesn’t change over the years. However, now we consider that the salary will be raised every 4 months by a rate named rate, this variable should be in decimal form (i.e. 0.03 for 3%). The new salary will be applied for the month after every batch of 4 months.\nWith this further assumption, write the code to calculate how many months it will take a person to save up enough money for the down payment if that person has (i) (salary = $7,000 and rate = 0.02); (ii) (salary = $10,000 and rate = 0.01).\n\n# Solution 2.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 7000\nsaved &lt;- 10000\nmonth &lt;- 0\nrate &lt;- 0.02\n\nwhile (saved &lt; cost) {\n  month &lt;- month + 1\n  saved &lt;- saved + 0.4 * salary + 0.02 * saved\n  if (month %% 4 == 0) {\n    salary &lt;-  salary * (1 + rate)\n  }\n}\nprint(month) #52\n\n[1] 52\n\n# Solution 2.ii\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 10000\nsaved &lt;- 10000\nmonth &lt;- 0\nrate &lt;- 0.01\n\nwhile (saved &lt; cost) {\n  month &lt;- month + 1\n  saved &lt;- saved + 0.4 * salary + 0.02 * saved\n  if (month %% 4 == 0) {\n    salary &lt;-  salary * (1 + rate)\n  }\n}\nprint(month) #43\n\n[1] 43\n\n\n\n\n\nOnsite Questions\n\nA sequence is generated using the following recursive relation\n\\(x_n\\) = \\(2x_{n-1}\\) \\(-\\) \\(x_{n-2}\\) \\(+\\) \\(5\\), \\(\\text{ for } n \\geq 3\\)\nwith \\(x_1\\) = 0 and \\(x_2\\) = 1.\n(a) Use for loop in R to find the 30th term of the series.\n\nx &lt;- numeric(30)\nx[1] &lt;- 0\nx[2] &lt;- 1\n\nfor (i in 3:30) {\n  x[i] &lt;- 2*x[i-1] - x[i-2] + 5\n}\nx[30]\n\n[1] 2059\n\n\n\n\n\nFind the smallest value of n such that \\(x_n\\) ≥ 1,000\n\n\nx &lt;- numeric(10) \nx[1] &lt;- 0\nx[2] &lt;- 1\ni &lt;- 3\n\nwhile (x[i-1] &lt;= 1000) {\n  x[i] &lt;- 2 * x[i-1] - x[i-2] + 5\n  i &lt;- i + 1\n}\n\nmax(x)\n\n[1] 1071\n\nwhich(x == max(x))\n\n[1] 22\n\n\nConsider another sequence which is generated using the following recursive relation\n\\(y_1 = 2800 + 1.02\\times y_0, \\text{ with } y_0 = 1000\\) and\n\\(y_n = 2800 + 1.02\\times y_{n-1}, \\text{ for } n \\geq 2\\)\nfind the smallest value of \\(n\\) such as \\(y_n\\) \\(\\geq\\) \\(300,000\\)\n\ny &lt;- numeric()  # Initialize an empty numeric vector\ny[1] &lt;- 2800 + 1.02 * 10000  # y_1\n\nn &lt;- 2\nwhile (y[n-1] &lt; 300000) {\n  y[n] &lt;- 2800 + 1.02 * y[n-1]\n  n &lt;- n + 1\n}\n\nmax(y)\n\n[1] 305759.6\n\nwhich(y == max(y))\n\n[1] 55"
  },
  {
    "objectID": "index.html#tutorial-2-basic-prob-and-stats",
    "href": "index.html#tutorial-2-basic-prob-and-stats",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 2 (Basic Prob and Stats)",
    "text": "Tutorial 2 (Basic Prob and Stats)\n\nOffsite Questions\nForced Expiratory Volume (FEV) is an index of pulmonary function that measures the volume of air expelled after 1 second of constant effort. The dataset FEV.csv contains measurements for 654 children aged 3 to 19 years of age. The purpose of the data collection was to study how FEV is affected by certain other variables. The variables that we shall work with are\nAge: Age in years.\nFEV: FEV measurement.\nHgt: Height in inches.\nheight: Height in meters\nSex: 0 = female, 1 = male.\nSmoking status: 0 = current non-smoker, 1 = current smoker.\n(a) What is the response variable in this study?\n\n# FEV is the response variable in this study\n\n(b) Create a histogram of FEV and comment on it.\n\nfev &lt;- read.csv(\"FEV.csv\")\nhist(fev$FEV)\n\n\n\n\n\n\n\n# Comments:\n# range: 0.5 to 6\n# unimodal\n# slightly right skewed\n# may have outliers\n\n(c) Create a boxplot of FEV and identify how many outliers there are. Investigate your data and comment on these outliers.\n\nbp &lt;- boxplot(fev$FEV)\n\n\n\n\n\n\n\noutliers &lt;- bp$out\nlength(outliers)\n\n[1] 9\n\n# There are 9 outliers\n\nindex &lt;- which(fev$FEV %in% outliers)\nfev[index,]\n\n       ID Age  FEV Hgt Sex Smoke height\n321  2142  14 4.84  72   1     0   1.83\n452 33041  12 5.22  70   1     0   1.78\n464 37241  13 4.88  73   1     0   1.85\n517 49541  13 5.08  74   1     0   1.88\n609  6144  19 5.10  72   1     0   1.83\n624 25941  15 5.79  69   1     0   1.75\n632 37441  17 5.63  73   1     0   1.85\n648 71141  17 5.64  70   1     0   1.78\n649 71142  16 4.87  72   1     1   1.83\n\n# Comments:\n# 1. All outliers are male\n# 2. Most (8/9) are\n# non-smokers\n# 3. They are rather\n# tall\n\n(d) Generally, is the sample of FEV normally distributed?\n\nqqnorm(fev$FEV, pch = 20)\nqqline(fev$FEV, col = \"red\")\n\n\n\n\n\n\n\n# Comments:\n# Left tail sample quantiles are larger than expected,\n# hence left tail shorter than normal. \n# Right tail sample quantiles larger than expected, hence # right tail longer than normal.\n\n# Combined with the histogram of FEV, it is clear that the\n# sample of FEV is not normally distributed and quite \n# right skewed.\n\n(e) Create separate histograms for male and female FEV, then obtain separate numerical summaries for males and female FEV. Comment on what you observe.\n\n# First, obtain the male and female FEV values separately\n# to plot onto two different histograms.\n\nfemale = fev$FEV[which(fev$Sex==0)]\n# Alternative: fev$FEV[fev$Sex==0]\nmale = fev$FEV[which(fev$Sex==1)] \n# Alternative: fev$FEV[fev$Sex==1]\n\n# Now plot the two histograms side-by-side\n\nopar &lt;- par(mfrow=c(1,2)) \nhist(female, col = 2, freq= FALSE, \nmain = \"Histogram of Female FEV\", ylim = c(0,0.52))\nhist(male, col = 4, freq= FALSE, \nmain = \"Histogram of Male FEV\", ylim = c(0,0.52))\n\n\n\n\n\n\n\n# Remember to use par(mfrow=c(1,1))  to reset the diagram layout!\n\n# obtaining separate numerical summaries for male and female\nIQR(female) # 1.04\n\n[1] 1.04\n\nsummary(female) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.790   1.950   2.490   2.452   2.990   3.840 \n\nvar(female) # 0.4169424\n\n[1] 0.4169424\n\nIQR(male) # 1.5275\n\n[1] 1.5275\n\nsummary(male)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.800   2.007   2.605   2.813   3.535   5.790 \n\nvar(male) # 1.006866\n\n[1] 1.006866\n\n# Comments:\n# Both histogram are unimodal but have different shapes. \n# It is almost symmetrical for females, but quite right-skewed for males.\n# Median FEV for females is much lower than males, 2.49 compared to 2.605.\n# Variability for males is higher than females.\n# The respective IQR are 1.54 and 1.05.\n\n(f) Create a scatterplot with height (in metres) on the x-axis and FEV on the y-axis.\n\nplot(fev$height, fev$FEV)\npoints(female ~ fev$height[which(fev$Sex==0)], \ncol = \"red\", pch = 20)\npoints(male ~ fev$height[which(fev$Sex==1)], \ncol = \"darkblue\", pch = 20)\nlegend(1.2, 5, legend = c(\"Female\", \"Male\"), \ncol = c(\"red\",\"darkblue\"), pch=c(20,20))\n\n\n\n\n\n\n\n\n(g) Compute the correlation between FEV and height and comment on your results.\n\ncor(fev$FEV, fev$height) # 0.8675619\n\n[1] 0.8675619\n\n# Computed correlation is quite high.\n# It is clear from the plot that there is a strong \n# positive linear association between FEV and height. \n# The range of FEV and height for males appears larger than for females. \n# The variability of FEV at lower heights seems slightly \n# less than the variability of FEV at greater heights.\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-3-linear-regression-1",
    "href": "index.html#tutorial-3-linear-regression-1",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 3 (Linear Regression 1)",
    "text": "Tutorial 3 (Linear Regression 1)\n\nOffsite Questions\nConsider the question given in Tutorial 1.\n(a) For the first question in Tutorial 1, use the code to define a function, called F1, where the argument of F1 is salary. Run function F1 for the two cases mentioned.\n\ncost &lt;- 1200000 * 0.25 \nF1 &lt;- function(salary) {\n  saved &lt;- 10000\n  month &lt;- 0\n  while(saved &lt; cost) {\n    month &lt;- month + 1\n    saved &lt;- saved + 0.4 * salary + 0.02 * saved\n    }\n  return(month)\n}\n\n(b) For the second question in Tutorial 1, use the code to define a function, called F2, where F2 has two arguments: salary and rate. Run function F2 for the two cases mentioned to obtain the results.\n\nF2 &lt;- function(salary, price = 1200000, rate = 0.01, portion_save = 0.4) {\n  r = 0.02 # monthly rate return from investment\n  saved &lt;- 10000 # savings given by parents initially\n  month &lt;- 0\n  cost = 0.25*price\n  while(saved &lt; cost) {\n    month = month +1\n    saved = saved + portion_save * salary + saved * r\n    if (month %% 4 ==0) {\n      salary = salary*(1+rate)\n    }\n  }\n  return(month)\n}\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-4-linear-regression-2",
    "href": "index.html#tutorial-4-linear-regression-2",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 4 (Linear Regression 2)",
    "text": "Tutorial 4 (Linear Regression 2)\n\nOffsite Questions\nConsider data set given in the file hdbresale_reg.csv on Canvas, which has the information of 6055 HDB resale flats in Singapore. We would want to form a linear model that helps to predict the resale price of HDB flats, based on the floor area in square meters and the type of the flats.\n(a) Consider the resale price, plot a histogram of it and give your comments. Is it suitable to fit a\nlinear model for this response variable? Explain.\n\ndata &lt;- read.csv(\"hdbresale_reg.csv\")\nstr(data)\n\n'data.frame':   6055 obs. of  11 variables:\n $ X                  : int  580 581 582 583 584 585 586 587 588 589 ...\n $ month              : chr  \"2012-03\" \"2012-03\" \"2012-03\" \"2012-03\" ...\n $ town               : chr  \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" ...\n $ flat_type          : chr  \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" ...\n $ block              : chr  \"640\" \"640\" \"668\" \"5\" ...\n $ street_name        : chr  \"ROWELL RD\" \"ROWELL RD\" \"CHANDER RD\" \"TG PAGAR PLAZA\" ...\n $ storey_range       : chr  \"01 TO 05\" \"06 TO 10\" \"01 TO 05\" \"11 TO 15\" ...\n $ floor_area_sqm     : num  74 74 73 59 68 75 68 77 82 105 ...\n $ flat_model         : chr  \"Model A\" \"Model A\" \"Model A\" \"Improved\" ...\n $ lease_commence_date: int  1984 1984 1984 1977 1979 2003 1980 1974 1978 2002 ...\n $ resale_price       : num  380000 388000 400000 460000 488000 ...\n\nhist(data$resale_price)\n\n\n\n\n\n\n\n# Comments: \n# Right-skewed histogram\n# Hence, resale price is NOT suitable to be the response \n# as assumption of linear model (symmetric) is violated.\n# For a right skewed variable, it is suggested to try \n# transforming the response by taking its logarithm.\n\n(b) Consider the resale price, plot a histogram of log_e of it and give your comments. Is it more\nsuitable to fit a linear model for this response variable than the original resale price?\n\nhist(log(data$resale_price))\n\n\n\n\n\n\n\n# Comments: \n# The histogram of the log of the resale price \n# is more symmetric, hence it is more suitable than \n# the original resale price as a response for our linear model.\n\n(c) Derive a scatter plot of the log_e of the resale price against the floor area in square meters. Give your comments.\n\n# creates a new column for the log(price)\ndata$log.price = log(data$resale_price) \nplot(data$log.price ~ data$floor_area_sqm)\n\n\n\n\n\n\n\n# Comments:\n# There seems to be a strong, positive, linear relationship\n# between log(price) and floor area.\n# The variability of the log(price) seems fairly stable \n# when the floor area changes.***\n\n(d) Fit a linear model where the log of the resale price be the response. Write down the fitted equation.\n\nstr(data)\n\n'data.frame':   6055 obs. of  12 variables:\n $ X                  : int  580 581 582 583 584 585 586 587 588 589 ...\n $ month              : chr  \"2012-03\" \"2012-03\" \"2012-03\" \"2012-03\" ...\n $ town               : chr  \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" ...\n $ flat_type          : chr  \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" ...\n $ block              : chr  \"640\" \"640\" \"668\" \"5\" ...\n $ street_name        : chr  \"ROWELL RD\" \"ROWELL RD\" \"CHANDER RD\" \"TG PAGAR PLAZA\" ...\n $ storey_range       : chr  \"01 TO 05\" \"06 TO 10\" \"01 TO 05\" \"11 TO 15\" ...\n $ floor_area_sqm     : num  74 74 73 59 68 75 68 77 82 105 ...\n $ flat_model         : chr  \"Model A\" \"Model A\" \"Model A\" \"Improved\" ...\n $ lease_commence_date: int  1984 1984 1984 1977 1979 2003 1980 1974 1978 2002 ...\n $ resale_price       : num  380000 388000 400000 460000 488000 ...\n $ log.price          : num  12.8 12.9 12.9 13 13.1 ...\n\nunique(data$flat_type)\n\n[1] \"3 ROOM\"    \"4 ROOM\"    \"5 ROOM\"    \"EXECUTIVE\" \"2 ROOM\"   \n\nM = lm(log.price ~ floor_area_sqm + flat_type, data = data)\nsummary(M)\n\n\nCall:\nlm(formula = log.price ~ floor_area_sqm + flat_type, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.28208 -0.07054 -0.01515  0.04323  0.79797 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.235e+01  1.891e-02 653.013  &lt; 2e-16 ***\nfloor_area_sqm     3.712e-03  1.817e-04  20.429  &lt; 2e-16 ***\nflat_type3 ROOM    1.190e-01  1.714e-02   6.944  4.2e-12 ***\nflat_type4 ROOM    2.093e-01  1.869e-02  11.196  &lt; 2e-16 ***\nflat_type5 ROOM    2.762e-01  2.099e-02  13.160  &lt; 2e-16 ***\nflat_typeEXECUTIVE 4.302e-01  2.527e-02  17.023  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1156 on 6049 degrees of freedom\nMultiple R-squared:  0.7116,    Adjusted R-squared:  0.7114 \nF-statistic:  2986 on 5 and 6049 DF,  p-value: &lt; 2.2e-16\n\n# fitted_log(price) = \n# 12.35 + \n# 0.003712 * floor_area_sqm +\n# 0.119 ∗ I(flat type = 3 ROOM) +\n# 0.2093 ∗ I(flat type = 4 ROOM) +\n# 0.2762 ∗ I(flat type = 5 ROOM) +\n# 0.4302 ∗ I(flat type = Executive)\n\n(e) Report the coefficient of the floor area in square meters and interpret it.\n\n# The coefficient of it is 0.003712. \n# Meaning when comparing two flats of the same type, \n# then an increase of 1 square meter will increase \n# the predicted log(price) by 0.003712.\n# Equivalently, the price will \n# increase by e^0.003712 = 1.003719 TIMES.\n\n(f) Predict the resale price of a 4-room HDB flat that is of 100 square meters.\n\nnew = data.frame(\n  floor_area_sqm = 100, \n  flat_type = \"4 ROOM\"\n  )\n\npredicted_log.price = predict(M, new)\npredicted_price = exp(predicted_log.price)\nprint(predicted_price) # 412807.6\n\n       1 \n412807.6 \n\n\n(g) Report \\(R^2\\) of the model and interpret it.\n\nsummary(M)$r.squared\n\n[1] 0.7116378\n\n# The R_squared value is 0.712. \n# That means model M can explain 71.2% \n# of the variability of the response in the sample.\n\n\n\nOnsite Questions\nA dataset on house selling price was randomly collected, house_selling_prices_FL.csv. It’s our interest to model how \\(y\\) = selling price (dollar) is dependent on \\(x\\) = the size of the house (square feet). A simple linear regression model (\\(y\\) regress on \\(x\\)) was fitted, called Model 1.\nThe given data has another variable, NW, which specifies if a house is in the part of the town considered\nless desirable (NW = 0).\n\nhouse &lt;- read.csv(\"house_selling_prices_FL.csv\")\ndim(house)\n\n[1] 100   9\n\nhouse$NW &lt;- as.factor(house$NW)\nstr(house)\n\n'data.frame':   100 obs. of  9 variables:\n $ House   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes   : int  1360 1050 1010 830 2150 1230 150 1470 1850 320 ...\n $ Bedrooms: int  3 1 3 3 3 3 2 3 3 3 ...\n $ Baths   : num  2 1 1.5 2 2 2 2 2 2 2 ...\n $ Quadrant: chr  \"NW\" \"NW\" \"NW\" \"SW\" ...\n $ NW      : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 2 2 2 2 ...\n $ price   : int  145000 68000 115000 69000 163000 69900 50000 137000 121300 70000 ...\n $ size    : int  1240 370 1130 1120 1710 1010 860 1420 1270 1160 ...\n $ lot     : int  18000 25000 25000 17000 14000 8000 15300 18000 16000 8000 ...\n\n\n(a) Derive the correlation between \\(x\\) and \\(y\\).\n\ncor(house$size, house$price)\n\n[1] 0.7612621\n\n\n(b) Derive a scatter plot of \\(y\\) against \\(x\\). Give your comments on the association of y and x.\n\nplot(house$price ~ house$size, pch = 20)\n\n\n\n\n\n\n\n# Comments:\n# There is a clear (obvious) association shown.\n# The association is positive.\n# The association is quite linear.\n# The variability of y (the price) is quite stable when x (the size) changes.\n\n(c) Derive \\(R^2\\) of Model 1. Verify that \\(\\sqrt{R^2}\\) = \\(|cor(y, x)|\\). In which situation we can have \\(\\sqrt{R^2}\\) = \\(cor(y, x)\\)?\n\nM1 = lm(price ~ size, data = house)\nsummary(M1)$r.squared\n\n[1] 0.57952\n\nsqrt(summary(M1)$r.squared ) # same as cor(price, size) # 0.7612621\n\n[1] 0.7612621\n\n# From the code above, we can see that \n# √R^2 = |cor(y, x)|: R^2 = 0.5795, hence, √0.5795 = 0.761 = |cor(y, x)|.\n# When cor(y, x) &gt; 0 then in a simple model y ∼ x, we always have √R2 = cor(y, x)\n\n(d) Form a model (called Model 2) which has two regressors (x and NW). Report the coefficient of variable NW in Model 2. Interpret it.\n\nM2 = lm(price ~ size + NW, data = house)\nsummary(M2)\n\n\nCall:\nlm(formula = price ~ size + NW, data = house)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-83207 -22968    215  14135 109149 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -15257.514  11908.297  -1.281 0.203160    \nsize            77.985      6.209  12.560  &lt; 2e-16 ***\nNW1          30569.087   7948.742   3.846 0.000215 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34390 on 97 degrees of freedom\nMultiple R-squared:  0.6352,    Adjusted R-squared:  0.6276 \nF-statistic: 84.43 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n# The fitted equation of Model 2:\n# y_pred = −15257.5 + 77.99x + 30569.1 × I(NW = 1).\n\n# The estimated coefficient of NW in Model 2 is 30569.1\n# This value means: for two houses of the same size (fix x), \n# the house in the more desirable part (NW = 1) is \n# $30569.1 more than the one in the less desirable part (NW = 0)\n\n(e) Estimate and report the price of a house where its size is 4000 square feet and is located at the more\ndesirable part of the town.\n\npredict(M2, newdata=data.frame(size=4000, NW = \"1\"))\n\n       1 \n327252.1 \n\n# The mean price of a house with size x = 4000 and NW = 1 is $327252.1."
  },
  {
    "objectID": "index.html#tutorial-5-k-nearest-neighbor",
    "href": "index.html#tutorial-5-k-nearest-neighbor",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 5 (K-Nearest Neighbor)",
    "text": "Tutorial 5 (K-Nearest Neighbor)\n\nOffsite Questions\n\n(MLR) Consider the horseshoe female crab data given in the csv file crab.csv. We would want to form a model for the weight of the female crabs (kg), which depends on its width (cm) and its spine condition (1 = both good, 2 = one worn or broken, 3 = both worn or broken).\n\ndata &lt;- read.csv(\"crab.csv\")\nhead(data)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : int  3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\ndata$spine &lt;- as.factor(data$spine)\n# 1 = both good\n# 2 = one worn or broken\n# 3 = both worn or broken\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\n\n(a) Produce a scatter plot of variable weight against width for different condition of spine.\n\nattach(data)\nplot(weight ~ width, pch = 20, main = \"weight vs width for each spine class\",\n     xlab = \"width\", ylab = \"weight\")\npoints(weight[spine == \"1\"] ~ width[spine ==\"1\"], pch = 15, col = \"red\")\npoints(weight[spine == \"2\"] ~ width[spine ==\"2\"], pch = 16, col = \"blue\")\npoints(weight[spine == \"3\"] ~ width[spine ==\"3\"], pch = 17, col = \"green\")\nlegend(22,4,legend=c(\"1\", \"2\", \"3\"),col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Fit a linear regression model for weight which has two explanatories, width and spine.\n\nmodel &lt;- lm(weight ~ width + spine, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n\n(c) Is the fitted model signicant?\n\n# the model f statistic showing p value is less than 2.2 x 10^-16.\n# Since this is far below 0.05, the model can be considered significant\n\n(d) Derive \\(R^2\\) and adjusted \\(R^2\\) of the fitted model.\n\nsummary(model)$r.squared\n\n[1] 0.7917598\n\n# 0.7917598\nsummary(model)$adj.r.squared\n\n[1] 0.7880632\n\n# 0.7880632\n\n(e) Write down the fitted model.\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) -0.06969*I(spine=3)\n\n(f) Two female crabs of the same width, and the diference of their weight if one has spines are of good condition and another one with broken spines.\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) + -0.06969*I(spine=3)\n# weight1 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*0\n# weight2 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*1\n\nweight.dif &lt;- (0.05544*0 + -0.06969*0) - (0.05544*0 + -0.06969*1)\nweight.dif\n\n[1] 0.06969\n\n\n(g) Predict the weight of a female crab that has width of 27 cm and has both spines worn or broken.\n\nnewdata &lt;- data.frame(width = 27, spine = \"3\")\npredict(model, newdata = newdata)\n\n       1 \n2.582352 \n\n\nMeasures of classier performance\nSuppose we have developed a K-nearest neighbors classier for predicting diabetes status. The following table shows the actual response \\(Y\\) (1 = yes, 0 =n o) and fitted value \\(\\hat Y\\) using the classier for 10 test data points. A test data point is predicted to be \\(\\hat G\\)= 1 if \\(\\hat Y\\) &gt; δ, for a specied threshold value δ (Recall that we use δ = 0.5 in class, also known as the majority rule).\n\n\nWe define: \\(TPR = \\frac {TP}{TP +FN}\\), \\(FPR = \\frac {FP}{FP +TN}\\); For each of the thresholds δ = 0.3, 0.6 and 0.8, derive \\(TPR\\) and \\(FPR\\) in making predictions with the K-nearest neighbors classier for the 10 test data points. Plot \\(TPR\\) against \\(FPR\\) for the three thresholds.\n\n\ny &lt;- c(1,1,0,1,1,0,0,1,0,0)\nycap &lt;- c(0.9,0.5,0.7,0.4,0.5,0.2,0.7,0.9,0.1,0.1)\ntpr.all &lt;- numeric(0)\nfpr.all &lt;- numeric(0)\n\nsigma.list &lt;- c(0.3,0.6,0.8)\nfor (sigma in sigma.list) {\n  pred &lt;- ifelse(ycap &gt;= sigma, 1 ,0)\n  confusion.matrix &lt;- table(y, pred)\n  tpr &lt;- confusion.matrix[2,2]/sum(confusion.matrix[2,])\n  fpr &lt;- confusion.matrix[1,2]/sum(confusion.matrix[1,])\n  tpr.all &lt;- append(tpr.all, tpr)\n  fpr.all &lt;- append(fpr.all, fpr)\n}\n\nplot(tpr.all ~ fpr.all, pch = 20,\n     xlim = c(0,1),\n     ylim = c(0,1))\npoints(tpr.all[1] ~ fpr.all[1], pch = 15, col = \"red\")\npoints(tpr.all[2] ~ fpr.all[2], pch = 16, col = \"blue\")\npoints(tpr.all[3] ~ fpr.all[3], pch = 17, col = \"green\")\nlegend(0.7,0.4,legend=c(\"sigma = 0.3\", \"sigma = 0.6\", \"sigma = 0.8\"),\n       col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Can we add the two points (0,0) and (1,1) to the plot of \\(TPR\\) against \\(FPR\\) in part (a). Explain why or why not.\n\n# If σ &gt; 0.9 then all test points have predicted ˆG = 0 (predicted as negative), \n# so TPR = FPR = 0.\n# If σ &lt; 0.1, then all test points have predicted ˆG = 1 (predicted as positive),\n# so TPR = FPR = 1.\n# Since there exist σ within the range from 0 to 1 for the two points to happen, \n# these two points can be added to the plot.\n\nThe CSV file Caravan.csv contains data on 5822 real customer records on caravan insurance purchase. This data set is owned and supplied by the Dutch data mining company, Sentient Machine Research, and is based on real world business data. Each record consists of 86 variables, containing socio-demographic data (variables 1-43) and product ownership (variables 44-86). Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy. For this business, assume that the overall error rate (equivalently, the accuracy) is not of interest. Instead, the company wants to use the classier to predict who are the potential customers likely to purchase insurance. Then the metric precision will be important, since it relates the proportion of Individuals who will actually purchase the insurance, among the group of individuals who are predicted to purchase insurance.\n\ncara &lt;- read.csv(\"Caravan data.csv\")\nhead(cara, n = 2)\n\n  X MOSTYPE MAANTHUI MGEMOMV MGEMLEEF MOSHOOFD MGODRK MGODPR MGODOV MGODGE\n1 1      33        1       3        2        8      0      5      1      3\n2 2      37        1       2        2        8      1      4      1      4\n  MRELGE MRELSA MRELOV MFALLEEN MFGEKIND MFWEKIND MOPLHOOG MOPLMIDD MOPLLAAG\n1      7      0      2        1        2        6        1        2        7\n2      6      2      2        0        4        5        0        5        4\n  MBERHOOG MBERZELF MBERBOER MBERMIDD MBERARBG MBERARBO MSKA MSKB1 MSKB2 MSKC\n1        1        0        1        2        5        2    1     1     2    6\n2        0        0        0        5        0        4    0     2     3    5\n  MSKD MHHUUR MHKOOP MAUT1 MAUT2 MAUT0 MZFONDS MZPART MINKM30 MINK3045 MINK4575\n1    1      1      8     8     0     1       8      1       0        4        5\n2    0      2      7     7     1     2       6      3       2        0        5\n  MINK7512 MINK123M MINKGEM MKOOPKLA PWAPART PWABEDR PWALAND PPERSAUT PBESAUT\n1        0        0       4        3       0       0       0        6       0\n2        2        0       5        4       2       0       0        0       0\n  PMOTSCO PVRAAUT PAANHANG PTRACTOR PWERKT PBROM PLEVEN PPERSONG PGEZONG\n1       0       0        0        0      0     0      0        0       0\n2       0       0        0        0      0     0      0        0       0\n  PWAOREG PBRAND PZEILPL PPLEZIER PFIETS PINBOED PBYSTAND AWAPART AWABEDR\n1       0      5       0        0      0       0        0       0       0\n2       0      2       0        0      0       0        0       2       0\n  AWALAND APERSAUT ABESAUT AMOTSCO AVRAAUT AAANHANG ATRACTOR AWERKT ABROM\n1       0        1       0       0       0        0        0      0     0\n2       0        0       0       0       0        0        0      0     0\n  ALEVEN APERSONG AGEZONG AWAOREG ABRAND AZEILPL APLEZIER AFIETS AINBOED\n1      0        0       0       0      1       0        0      0       0\n2      0        0       0       0      1       0        0      0       0\n  ABYSTAND Purchase\n1        0       No\n2        0       No\n\nstr(cara)\n\n'data.frame':   5822 obs. of  87 variables:\n $ X       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MOSTYPE : int  33 37 37 9 40 23 39 33 33 11 ...\n $ MAANTHUI: int  1 1 1 1 1 1 2 1 1 2 ...\n $ MGEMOMV : int  3 2 2 3 4 2 3 2 2 3 ...\n $ MGEMLEEF: int  2 2 2 3 2 1 2 3 4 3 ...\n $ MOSHOOFD: int  8 8 8 3 10 5 9 8 8 3 ...\n $ MGODRK  : int  0 1 0 2 1 0 2 0 0 3 ...\n $ MGODPR  : int  5 4 4 3 4 5 2 7 1 5 ...\n $ MGODOV  : int  1 1 2 2 1 0 0 0 3 0 ...\n $ MGODGE  : int  3 4 4 4 4 5 5 2 6 2 ...\n $ MRELGE  : int  7 6 3 5 7 0 7 7 6 7 ...\n $ MRELSA  : int  0 2 2 2 1 6 2 2 0 0 ...\n $ MRELOV  : int  2 2 4 2 2 3 0 0 3 2 ...\n $ MFALLEEN: int  1 0 4 2 2 3 0 0 3 2 ...\n $ MFGEKIND: int  2 4 4 3 4 5 3 5 3 2 ...\n $ MFWEKIND: int  6 5 2 4 4 2 6 4 3 6 ...\n $ MOPLHOOG: int  1 0 0 3 5 0 0 0 0 0 ...\n $ MOPLMIDD: int  2 5 5 4 4 5 4 3 1 4 ...\n $ MOPLLAAG: int  7 4 4 2 0 4 5 6 8 5 ...\n $ MBERHOOG: int  1 0 0 4 0 2 0 2 1 2 ...\n $ MBERZELF: int  0 0 0 0 5 0 0 0 1 0 ...\n $ MBERBOER: int  1 0 0 0 4 0 0 0 0 0 ...\n $ MBERMIDD: int  2 5 7 3 0 4 4 2 1 3 ...\n $ MBERARBG: int  5 0 0 1 0 2 1 5 8 3 ...\n $ MBERARBO: int  2 4 2 2 0 2 5 2 1 3 ...\n $ MSKA    : int  1 0 0 3 9 2 0 2 1 1 ...\n $ MSKB1   : int  1 2 5 2 0 2 1 1 1 2 ...\n $ MSKB2   : int  2 3 0 1 0 2 4 2 0 1 ...\n $ MSKC    : int  6 5 4 4 0 4 5 5 8 4 ...\n $ MSKD    : int  1 0 0 0 0 2 0 2 1 2 ...\n $ MHHUUR  : int  1 2 7 5 4 9 6 0 9 0 ...\n $ MHKOOP  : int  8 7 2 4 5 0 3 9 0 9 ...\n $ MAUT1   : int  8 7 7 9 6 5 8 4 5 6 ...\n $ MAUT2   : int  0 1 0 0 2 3 0 4 2 1 ...\n $ MAUT0   : int  1 2 2 0 1 3 1 2 3 2 ...\n $ MZFONDS : int  8 6 9 7 5 9 9 6 7 6 ...\n $ MZPART  : int  1 3 0 2 4 0 0 3 2 3 ...\n $ MINKM30 : int  0 2 4 1 0 5 4 2 7 2 ...\n $ MINK3045: int  4 0 5 5 0 2 3 5 2 3 ...\n $ MINK4575: int  5 5 0 3 9 3 3 3 1 3 ...\n $ MINK7512: int  0 2 0 0 0 0 0 0 0 1 ...\n $ MINK123M: int  0 0 0 0 0 0 0 0 0 0 ...\n $ MINKGEM : int  4 5 3 4 6 3 3 3 2 4 ...\n $ MKOOPKLA: int  3 4 4 4 3 3 5 3 3 7 ...\n $ PWAPART : int  0 2 2 0 0 0 0 0 0 2 ...\n $ PWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSAUT: int  6 0 6 6 0 6 6 0 5 0 ...\n $ PBESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PTRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBROM   : int  0 0 0 0 0 0 0 3 0 0 ...\n $ PLEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBRAND  : int  5 2 2 2 6 0 0 0 0 3 ...\n $ PZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAPART : int  0 2 1 0 0 0 0 0 0 1 ...\n $ AWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSAUT: int  1 0 1 1 0 1 1 0 1 0 ...\n $ ABESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ ATRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABROM   : int  0 0 0 0 0 0 0 1 0 0 ...\n $ ALEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABRAND  : int  1 1 1 1 1 0 0 0 0 1 ...\n $ AZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Purchase: chr  \"No\" \"No\" \"No\" \"No\" ...\n\ndim(cara)\n\n[1] 5822   87\n\n\n(a) Without any classier, if the company tries to sell insurance to a random selection of customers, what is the success rate?\n\ntable(cara$Purchase)\n\n\n  No  Yes \n5474  348 \n\nsuccesRate &lt;- table(cara$Purchase)[2]/sum(table(cara$Purchase))\nsuccesRate\n\n       Yes \n0.05977327 \n\n# data set shows almost 6% of people purchased insurance\n\n(b) Standardize the input features. \\(Hint\\): Use scale() command in R.\n\ncara &lt;- cara[,-1] #throwing the first column since it provides no information\nscaled.X &lt;- scale(cara[,-86]) #throwing the response column\n\n(c) Randomly select 1000 observations to form the test data, and the remaining observations will be the training data.\n\nset.seed(5)\nn &lt;- dim(cara)[1] #train data number\nshuffled.index &lt;- sample(c(1:n), size = 1000)\nX.train &lt;- scaled.X[-shuffled.index,]\nX.test&lt;- scaled.X[shuffled.index,]\nY.train &lt;- as.factor(cara$Purchase[-shuffled.index])\nY.test &lt;- as.factor(cara$Purchase[shuffled.index])\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a customer will purchase insurance. Compute the precision of the classier.\n\nlibrary(class)\n\npred &lt;- knn(X.train, X.test, Y.train, k=1)\nconfusion.matrix &lt;- table(Y.test, pred) \n# Precision = TP/TP+FP\nprecision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\nconfusion.matrix\n\n      pred\nY.test  No Yes\n   No  892  55\n   Yes  44   9\n\nprecision\n\n[1] 0.140625\n\n\n(e) Repeat question 3d, for k-nearest neighbor classier where k = 3,5,10. Which value of k gives the best precision?\n\nprecision.all &lt;- data.frame()\nk.list &lt;- c(3, 5, 10)\nfor (k in k.list) {\n  pred &lt;- knn(X.train, X.test, Y.train, k = k)\n  confusion.matrix &lt;- table(Y.test, pred) \n  # Precision = TP/TP+FP\n  precision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\n  precision.all &lt;- rbind(precision.all, data.frame(k, precision))\n}\n\nprecision.all\n\n   k  precision\n1  3 0.05263158\n2  5 0.14285714\n3 10        NaN\n\n# So far, k = 5 gives the best precision.\n# However, one might use N -fold cross validation \n# to have the average precision for each k.\n# With that, the value of k that gives largest average precision is chosen.\n\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-6-decision-tree",
    "href": "index.html#tutorial-6-decision-tree",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 6 (Decision Tree)",
    "text": "Tutorial 6 (Decision Tree)\n\nOffsite Questions\n\n(KNN and N -fold Cross Validation)\nLoan managers often need to take into account an applicant’s demographic and socioeconomic proles in deciding whether to approve a loan to the applicant, to minimize losses due to defaults. In this exercise we will build and evaluate a classier based on the German Credit Data to predict whether an applicant is considered as having good or bad credit risk. The features or predictors include (1) loan duration (in months), (2) credit amount, (3) Installment rate in percentage of disposable income and (4) age in years.\n(a) Read and explore the data from the file German_credit.csv.\n\ndata &lt;- read.csv(\"German credit.csv\")\nhead(data)\n\n  Creditability Duration Amount Instalment Age\n1             1       18   1049          4  21\n2             1        9   2799          2  36\n3             1       12    841          2  23\n4             1       12   2122          3  39\n5             1       12   2171          4  38\n6             1       10   2241          1  48\n\nstr(data)\n\n'data.frame':   1000 obs. of  5 variables:\n $ Creditability: int  1 1 1 1 1 1 1 1 1 1 ...\n $ Duration     : int  18 9 12 12 12 10 8 6 18 24 ...\n $ Amount       : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n $ Instalment   : int  4 2 2 3 4 1 1 2 4 1 ...\n $ Age          : int  21 36 23 39 38 48 39 40 65 23 ...\n\n\n(b) Standardize the input features.\n\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\nhead(X)\n\n       Duration     Amount  Instalment         Age\n[1,] -0.2407368 -0.7872630  0.91801781 -1.28093214\n[2,] -0.9870788 -0.1673006 -0.86974813  0.04034293\n[3,] -0.7382981 -0.8609500 -0.86974813 -1.10476213\n[4,] -0.7382981 -0.4071375  0.02413484  0.30459795\n[5,] -0.7382981 -0.3897785  0.91801781  0.21651294\n[6,] -0.9041519 -0.3649800 -1.76363111  1.09736299\n\n\n(c) Randomly select 800 customer records to form the training data, and the remaining 200 records will be the test data.\n\nset.seed(100)\nn &lt;- dim(data)[1]\nindex &lt;- sample(1:n, 800)\n\ntraining.X &lt;- X[index, ]\ntest.X &lt;- X[-index, ]\ntraining.Y &lt;- Y[index]\ntest.Y &lt;- Y[-index]\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a loan applicant is credible for the 200 test points. Compute the accuracy of the classier.\n\nlibrary(class)\npred &lt;- knn(training.X, test.X, training.Y, k=1)\nconfusion.matrix &lt;- table(test.Y, pred)\nconfusion.matrix\n\n      pred\ntest.Y  0  1\n     0 24 35\n     1 47 94\n\naccuracy &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\naccuracy\n\n[1] 0.59\n\n\n(e) Use N -folds cross validation with N = 5 to find the average accuracy for the 1-nearest neighbor classifier.\n\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\n\naccuracy &lt;- numeric(n_folds)\nfor (j in 1:n_folds) {\n  test_j &lt;- which(folds_j == j) \n  test.y &lt;- Y[test_j] \n  train.X &lt;- X[-test_j, ]  \n  test.X &lt;- X[test_j, ]  \n  train.y &lt;- Y[-test_j] \n\n  knn.pred &lt;- knn(train.X, test.X, train.y, k = 1) \n\n  confusion.matrix &lt;- table(test.y, knn.pred)\n  accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n}\n\nave.accuracy &lt;- round(mean(accuracy), digits = 3)\nave.accuracy\n\n[1] 0.622\n\n\n(f) Repeat question 1e for K-nearest neighbor classifiers where K = 1, 2, …100.\n\n### FULL KNN WITH N-FOLDS VALIDATION\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nk &lt;- 100\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\nave.accuracy &lt;- numeric(length(k))\n\nfor (i in 1:k) {\n  accuracy &lt;- numeric(n_folds)\n  for (j in 1:n_folds) {\n    test_j &lt;- which(folds_j == j) \n    test.y &lt;- Y[test_j] \n    train.X &lt;- X[-test_j, ]  \n    test.X &lt;- X[test_j, ]  \n    train.y &lt;- Y[-test_j] \n\n    knn.pred &lt;- knn(train.X, test.X, train.y, k = i) \n\n    confusion.matrix=table(test.y, knn.pred)\n    accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n  }\n  ave.accuracy[i] &lt;- round(mean(accuracy), digits = 3)\n}\n\nave.accuracy\n\n  [1] 0.622 0.597 0.661 0.674 0.684 0.671 0.676 0.686 0.686 0.691 0.702 0.699\n [13] 0.705 0.702 0.707 0.695 0.699 0.702 0.700 0.693 0.695 0.695 0.697 0.695\n [25] 0.699 0.697 0.696 0.700 0.693 0.688 0.697 0.692 0.697 0.698 0.697 0.692\n [37] 0.695 0.695 0.695 0.696 0.696 0.701 0.699 0.701 0.699 0.699 0.699 0.698\n [49] 0.699 0.696 0.698 0.699 0.699 0.696 0.695 0.699 0.696 0.697 0.696 0.698\n [61] 0.697 0.696 0.699 0.697 0.697 0.697 0.697 0.696 0.698 0.699 0.699 0.696\n [73] 0.698 0.697 0.698 0.696 0.698 0.698 0.698 0.699 0.699 0.698 0.698 0.700\n [85] 0.698 0.698 0.700 0.700 0.699 0.700 0.700 0.701 0.701 0.701 0.700 0.700\n [97] 0.699 0.698 0.699 0.699\n\n\n(g) Compare the 100 classifiers above, which few values of K give the best average accuracy?\n\nlibrary(glue)\nindex = which(ave.accuracy == max(ave.accuracy))\nplot(x = 1:k, ave.accuracy)\nabline(v = index, col = \"red\")\n\n\n\n\n\n\n\nglue(\"most accurate k = {index} with average accuracy = {ave.accuracy[index]}\")\n\nmost accurate k = 15 with average accuracy = 0.707\n\n\n(Decision Trees)\nConsider the famous Iris Flower Data set which was rst introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation: the length and the width of the sepals and petals (in cm).\n\n(a) Use decision tree method to predict Iris species based on all four features.\n\niris &lt;- read.csv(\"Iris dataset.csv\")\nhead(iris)\n\n  sepal.length sepal.width petal.length petal.width       class\n1          5.1         3.5          1.4         0.2 Iris-setosa\n2          4.9         3.0          1.4         0.2 Iris-setosa\n3          4.7         3.2          1.3         0.2 Iris-setosa\n4          4.6         3.1          1.5         0.2 Iris-setosa\n5          5.0         3.6          1.4         0.2 Iris-setosa\n6          5.4         3.9          1.7         0.4 Iris-setosa\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ sepal.length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ sepal.width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ petal.length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal.width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\ndim(iris)\n\n[1] 150   5\n\nlibrary(rpart)\nfit &lt;- rpart(class ~ sepal.length + sepal.width + petal.length + petal.width,\n              data = iris,\n              method=\"class\",\n              parms=list(split='information'),\n              control = rpart.control( minsplit =1))\n\n(b) Visualize the decision tree above, using the rpart.plot function.\n\nlibrary(rpart.plot)\nrpart.plot(fit, type=4, extra=2)\n\n\n\n\n\n\n\n# The fitted tree is given in the figure below.\n# If the measurement of petal length is less than 2.5 cm \n# then the flower is of Iris-setosa\n# If the petal length is ≥ 2.5 cm with the petal width is ≥ 1.8 cm \n# then high chance (45/46) it will be an Iris-virginica.\n# If the petal length is ≥ 2.5 cm with the petal width is &lt; 1.8 cm\n# then we continue to check if the petal length is in the interval\n# [2.5, 5]. If yes, then high chance (47/48) it is Iris-versicolor.\n\n(c) What are the more important features in the fitted tree above?\n\n# It seems the sepal length and sepal width \n# are not important in the classification while\n# the petal length and petal width are more important."
  },
  {
    "objectID": "index.html#y2223-s2",
    "href": "index.html#y2223-s2",
    "title": "DSA1101: Everything You Need",
    "section": "Y(22/23) S2",
    "text": "Y(22/23) S2\n\nQuestions 1 (50 points)\nA study of nesting horseshoe crabs (J. Rrockmann, Ethology, 102: 1-21, 1996) collected data on 173 female horseshoe crabs. Explanatory variables included the female crab’s color (2,3,4 and 5 which increases when the darkness increases), spine condition, weight (kg), and carapace width (cm).\nVariable satell indicates the number of male crab (called satellite) attached to the female crabs (for example, the first crab has 8 satellites attached to it). In this problem, we are interested in the possible factors like color, weight and width that may affect the number of satellites attach to a female crab.\nData are given in the file data1.txt.\nImport the given data into R and keep all the variable names as original.\n\ndata&lt;-read.table('data1.txt', header=T) \nhead(data)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\ndim(data)\n\n[1] 173   5\n\nnames(data)\n\n[1] \"color\"  \"spine\"  \"width\"  \"satell\" \"weight\"\n\nattach(data)\n\nThe following objects are masked from data (pos = 7):\n\n    color, satell, spine, weight, width\n\n\nPart I: Exploring the response variable - satell\n\nCreate a histogram of this variable with a normal density curve overlaying. Give your comment about this plot.\n\nhist(satell, probability = TRUE, \n     main = \"Histogram of Satell with Normal Density Curve\",\n     xlab = \"Number of Satellites\", col = \"lightblue\", border = \"black\")\n\nx_values &lt;- seq(min(satell), max(satell), length = 100)\ny_values &lt;- dnorm(x_values, mean = mean(satell), sd = sd(satell))\nlines(x_values, y_values, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Comments:\n# The histogram of the satell variable shows values ranging from 0 to 15, \n# with a single peak (unimodal) where most of the data is concentrated around 1. \n# The distribution is clearly right-skewed, no gap.\n# There are a few suspected outliers on the far right, with values around 15.\n\nCreate a box plot of this variable. Does it show any outliers? If yes, retrieve the full information (full row) of the crabs that are outliers in number of satellites. Copy the information and paste into R code file as comments.\n\nboxplot(satell)\nout &lt;- boxplot(satell)$out; out\n\n\n\n\n\n\n\n\n[1] 14 15\n\nindex &lt;- which(satell %in% out)\ndata[index,]\n\n   color spine width satell weight\n15     3     1  26.0     14    2.3\n56     3     3  28.3     15    3.0\n\n# There are two outliers, as following:\n# color spine width satell weight\n# 15     3     1  26.0     14    2.3\n# 56     3     3  28.3     15    3.0\n\nCreate a qq plot of this variable. Give your comment about this plot.\n\nqqnorm(satell,  pch = 20)\nqqline(satell, col = \"red\")\n\n\n\n\n\n\n\n# Comments:\n# Observed values on the left are clearly higher then the expected values,\n# meaning the data has shorter left tail.\n# Hence, satell data is not normally distributed\n\n\nPart II: Variable color\n\nWrite the code to create a new categorical variable, col, which equals to light if the crab has color 2 or 3, and equals to dark if the crab has color 4 or 5.\n\ndata$col &lt;- numeric(length(satell))\ndata$col[which(color &lt;= 3)] &lt;- \"light\"\ndata$col[which(color &gt; 3)] &lt;- \"dark\"\n\nhead(data)\n\n  color spine width satell weight   col\n1     3     3  28.3      8   3.05 light\n2     4     3  22.5      0   1.55  dark\n3     2     1  26.0      9   2.30 light\n4     4     3  24.8      0   2.10  dark\n5     4     3  26.0      4   2.60  dark\n6     3     3  23.8      0   2.10 light\n\nattach(data)\n\nThe following objects are masked from data (pos = 3):\n\n    color, satell, spine, weight, width\n\n\nThe following objects are masked from data (pos = 8):\n\n    color, satell, spine, weight, width\n\n\nCreate a frequency table for variable col created above. How many crabs are of light color and how many crabs are of dark color?\n\ntable(col)\n\ncol\n dark light \n   66   107 \n\n\nPlot a scatter plot of weight and satell, classified by col. Add a legend box for this plot. Give your comment about this plot.\n\nplot(satell ~ weight, \n     type = \"n\",\n     xlab=\"Weight\",\n     ylab=\"Satell\", \n     main = \"Crab Data\")\n\npoints(satell[col == \"light\"] ~ weight[col == \"light\"], pch = 20, col = \"red\")\npoints(satell[col == \"dark\"] ~ weight[col == \"dark\"], pch = 20, col = \"blue\")\nlegend(4.5, 15, legend=c(\"light\", \"dark\"),col = c(\"red\", \"blue\"), pch = 20)\n\n\n\n\n\n\n\n# Comments:\n# Overall, weight and satell has (quiet weak) POSITIVE\n# and possibily LINEAR relationship.\n# For light color or for dark color, the relationship between satell and weight\n# are about similar, no clear difference\n# as weight changes, the variability of satell gets larger\n\n\nPart III: Modelling\n\nFit a linear regression model for satell using three features color, weight and width.\n\ndata$color = as.factor(data$color)\nM = lm(satell ~ color + weight + width, data = data)\n\nReport the value of \\(R^2\\) of the model above. Give your comments on the goodness-of-fit of the model.\n\nsummary(M)\n\n\nCall:\nlm(formula = satell ~ color + weight + width, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5238 -2.1270 -0.6765  1.5393 11.1459 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -1.85827    4.51379  -0.412    0.681  \ncolor3      -0.61976    0.90368  -0.686    0.494  \ncolor4      -1.23133    0.96969  -1.270    0.206  \ncolor5      -1.17540    1.07476  -1.094    0.276  \nweight       1.68797    0.84358   2.001    0.047 *\nwidth        0.05576    0.23188   0.240    0.810  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.947 on 167 degrees of freedom\nMultiple R-squared:  0.1492,    Adjusted R-squared:  0.1237 \nF-statistic: 5.857 on 5 and 167 DF,  p-value: 5.168e-05\n\n# Comments:\n# R^2 = 0.1492\n# Though F-test for the significance of the overall  model: model is significant\n# However, R^2 is too low. It means model doesn't fit the data well.\n\n\n\n\nQuestions 2 (30 points)\nA data set data2.csv contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\nA table of variable description is given below. status is the response in this study.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nage\nage of patient at which they undergone surgery\n\n\nyear\nyear in which patient was undergone surgery (1958 - 1969)\n\n\nnode\nnumber of lymph nodes that have cancer cells detected\n\n\nstatus\n1 = the patient survived 5 years or longer (negative); and\n2 = the patient died within 5 years (positive)\n\n\n\nFor this problem, we will not consider the year when patient was undergone the surgery, year, as a feature for classification.\nIn R, use set.seed(999).\nWe would:\n\nUse KNN algorithm where K can be any positive integers, from 1 up to 50, to form classifiers to predict the outcome.\nFor each classier, evaluate it’s performance by 3-fold cross validation.\nUse type I error rate and type II error rate as the measures to evaluate each classifiers.\nSelect the best K.\n\nset.seed(999)\n\nhab &lt;- read.csv('data2.csv')\nhead(hab)\n\n  age year node survival.status\n1  30   64    1               1\n2  30   62    3               1\n3  30   65    0               1\n4  31   59    2               1\n5  31   65    4               1\n6  33   58   10               1\n\ndim(hab)\n\n[1] 306   4\n\nnames(hab)[4] = \"status\"\n\nhab$status = as.factor(hab$status)\nhab[, 1:3] = lapply(hab[ , 1:3],scale) # SCALING THE FEATURES\n\nattach(hab)\ntable(status)\n\nstatus\n  1   2 \n225  81 \n\n\n\n\nWrite the code for the purposes above. For each classier, the average of Type I error rates from 3-fold cross validation is saved in a vector, (named ave.type1); and the average of Type II error rates is saved in another vector (named ave.type2).\n\nlibrary(class)\n\nX &lt;- hab[, c(1,3)] # we do not use the 2nd column, year.\nY &lt;- hab[, 4] # response\n\nn &lt;- length(Y) # sample size\nn_folds &lt;- 3\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\ntable(folds_j)\n\nfolds_j\n  1   2   3 \n102 102 102 \n\nK = 50\nave.type1 = numeric(K) # to store the accuracy for each k of KNN, k is from 1 to K = 50.\nave.type2 = numeric(K) # to store the precision for each k of KNN\ntype1=numeric(n_folds)\ntype2=numeric(n_folds)\n\nfor(i in 1:K) {\n  for (j in 1:n_folds) {\n    test_j &lt;- which(folds_j == j) # get the index of the points that will be in the test set\n    test.y = Y[test_j] # response for the test points\n    knn.pred &lt;- knn(train=X[ -test_j, ], test=X[test_j, ], cl=Y[-test_j ], k=i) \n    confusion.matrix=table(test.y, knn.pred) # 2 = positive and 1 = negative\n\n    type1[j] = confusion.matrix[1,2]/sum(confusion.matrix[1,]) # row 1 means actual negative\n    type2[j] = confusion.matrix[2,1]/sum(confusion.matrix[2,]) # row 2 means actual positive\n  }\n        ave.type1[i] = round(mean(type1), digits = 3)\n        ave.type2[i] = round(mean(type2), digits = 3)\n}\n\nave.type1\n\n [1] 0.164 0.142 0.098 0.102 0.111 0.089 0.085 0.093 0.089 0.085 0.084 0.093\n[13] 0.085 0.080 0.085 0.080 0.080 0.080 0.080 0.063 0.067 0.067 0.067 0.071\n[25] 0.071 0.080 0.071 0.067 0.067 0.071 0.058 0.049 0.049 0.044 0.044 0.049\n[37] 0.040 0.035 0.035 0.031 0.035 0.036 0.031 0.027 0.027 0.022 0.022 0.022\n[49] 0.022 0.022\n\nave.type2\n\n [1] 0.634 0.630 0.707 0.654 0.640 0.655 0.652 0.688 0.713 0.727 0.690 0.715\n[13] 0.678 0.691 0.678 0.700 0.715 0.739 0.690 0.715 0.691 0.703 0.703 0.727\n[25] 0.703 0.728 0.764 0.764 0.753 0.776 0.752 0.753 0.779 0.790 0.814 0.789\n[37] 0.801 0.835 0.837 0.826 0.850 0.850 0.862 0.863 0.889 0.887 0.887 0.899\n[49] 0.899 0.912\n\n\nReport the length of vector ave.type1 and the length of vector ave.type2.\n\nlength(ave.type1) # 50, same as K\n\n[1] 50\n\nlength(ave.type2) # 50, same as K\n\n[1] 50\n\n\nWrite the code to produce a scatter plot where ave.type1 is in X-axis and ave.type2 is in Y-axis.\n\nplot(ave.type1, ave.type2, pch = 20)\n\n\n\n\n\n\n\n\nFor this study, we assume that type I error can be tolerated while type II error is not. Which value of K would you choose among the three smallest type II error rate, yet the type I error rate is not larger than 15%? Report the type I and type II error rate for that value of K.\n\nsort(ave.type2)[1:3] \n\n[1] 0.630 0.634 0.640\n\n# index of K that produced three smallest type 2 error\nindex = which(ave.type2 %in% c(sort(ave.type2)[1:3])) \nindex\n\n[1] 1 2 5\n\n# values of type 1 error for those K with 3 smallest type 2 error\nave.type1[index] \n\n[1] 0.164 0.142 0.111\n\n# among those values of K, we choose the one that gives types 1 error smaller than 15%\n# values of K could be chosen are: 2, 5\n# since K = 2 gives second smallest type 2 error = 63.4% with type 1 error = 14.2%,\n# and K = 5 gives third smalles type 2 error = 64% with type 1 error = 11.1%."
  },
  {
    "objectID": "index.html#y2324-s1",
    "href": "index.html#y2324-s1",
    "title": "DSA1101: Everything You Need",
    "section": "Y(23/24) S1",
    "text": "Y(23/24) S1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#tutorial-5-linreg-knn",
    "href": "index.html#tutorial-5-linreg-knn",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 5 (Linreg & KNN)",
    "text": "Tutorial 5 (Linreg & KNN)\n\nOffsite Questions\n\n(MLR) Consider the horseshoe female crab data given in the csv file crab.csv. We would want to form a model for the weight of the female crabs (kg), which depends on its width (cm) and its spine condition (1 = both good, 2 = one worn or broken, 3 = both worn or broken).\n\ndata &lt;- read.csv(\"crab.csv\")\nhead(data)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : int  3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\ndata$spine &lt;- as.factor(data$spine)\n# 1 = both good\n# 2 = one worn or broken\n# 3 = both worn or broken\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\n\n(a) Produce a scatter plot of variable weight against width for different condition of spine.\n\nattach(data)\nplot(weight ~ width, pch = 20, main = \"weight vs width for each spine class\",\n     xlab = \"width\", ylab = \"weight\")\npoints(weight[spine == \"1\"] ~ width[spine ==\"1\"], pch = 15, col = \"red\")\npoints(weight[spine == \"2\"] ~ width[spine ==\"2\"], pch = 16, col = \"blue\")\npoints(weight[spine == \"3\"] ~ width[spine ==\"3\"], pch = 17, col = \"green\")\nlegend(22,5,\n       legend=c(\"Spine = 1\", \"Spine = 2\", \"Spine = 3\"),\n       col=c(\"red\", \"blue\", \"green\"), \n       pch=c(15,16,17)\n       )\n\n\n\n\n\n\n\n\n(b) Fit a linear regression model for weight which has two explanatories, width and spine.\n\nmodel &lt;- lm(weight ~ width + spine, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n\n(c) Is the fitted model significant?\n\n# the model f statistic showing p value is less than 2.2 x 10^-16.\n# Since this is far below 0.05, the model can be considered significant\n\n(d) Derive \\(R^2\\) and adjusted \\(R^2\\) of the fitted model.\n\nsummary(model)$r.squared\n\n[1] 0.7917598\n\n# 0.7917598\nsummary(model)$adj.r.squared\n\n[1] 0.7880632\n\n# 0.7880632\n\n(e) Write down the fitted model.\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) - 0.06969*I(spine=3)\n\n(f) Two female crabs of the same width, and the difference of their weight if one has spines are of good condition and another one with broken spines.\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) - 0.06969*I(spine=3)\n# weight1 = -3.92955 + 0.24376*width + 0.05544*0 - 0.06969*0\n# weight2 = -3.92955 + 0.24376*width + 0.05544*0 - 0.06969*1\n\nweight.dif &lt;- (0.05544*0 - 0.06969*0) - (0.05544*0 - 0.06969*1)\nweight.dif\n\n[1] 0.06969\n\n# When they have the same width, then on average the one that has spines\n# of good condition is heavier than the one with broken spines by 0.06969 kg.\n\n(g) Predict the weight of a female crab that has width of 27 cm and has both spines worn or broken.\n\nnewdata &lt;- data.frame(width = 27, spine = \"3\")\npredict(model, newdata = newdata)\n\n       1 \n2.582352 \n\n\nMeasures of classier performance\nSuppose we have developed a K-nearest neighbors classier for predicting diabetes status. The following table shows the actual response \\(Y\\) (1 = yes, 0 =n o) and fitted value \\(\\hat Y\\) using the classier for 10 test data points. A test data point is predicted to be \\(\\hat G\\)= 1 if \\(\\hat Y\\) &gt; δ, for a specied threshold value δ (Recall that we use δ = 0.5 in class, also known as the majority rule).\n\n\nWe define: \\(TPR = \\frac {TP}{TP +FN}\\), \\(FPR = \\frac {FP}{FP +TN}\\); For each of the thresholds δ = 0.3, 0.6 and 0.8, derive \\(TPR\\) and \\(FPR\\) in making predictions with the K-nearest neighbors classier for the 10 test data points. Plot \\(TPR\\) against \\(FPR\\) for the three thresholds.\n\n\n\ny &lt;- c(1,1,0,1,1,0,0,1,0,0)\nycap &lt;- c(0.9,0.5,0.7,0.4,0.5,0.2,0.7,0.9,0.1,0.1)\ntpr.all &lt;- numeric(0)\nfpr.all &lt;- numeric(0)\n\ndelta.list &lt;- c(0.3,0.6,0.8)\nfor (delta in delta.list) {\n  pred &lt;- ifelse(ycap &gt;= delta, 1 ,0)\n  confusion.matrix &lt;- table(y, pred)\n  tpr &lt;- confusion.matrix[2,2]/sum(confusion.matrix[2,])\n  fpr &lt;- confusion.matrix[1,2]/sum(confusion.matrix[1,])\n  tpr.all &lt;- append(tpr.all, tpr)\n  fpr.all &lt;- append(fpr.all, fpr)\n}\n\nplot(tpr.all ~ fpr.all, type = \"n\", pch = 20,\n     xlim = c(0,1),\n     ylim = c(0,1))\npoints(tpr.all[1] ~ fpr.all[1], pch = 15, col = \"red\")\npoints(tpr.all[2] ~ fpr.all[2], pch = 16, col = \"blue\")\npoints(tpr.all[3] ~ fpr.all[3], pch = 17, col = \"green\")\nlegend(0.7,0.4,legend=c(\"sigma = 0.3\", \"sigma = 0.6\", \"sigma = 0.8\"),\n       col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Can we add the two points (0,0) and (1,1) to the plot of \\(TPR\\) against \\(FPR\\) in part (a). Explain why or why not.\n\n# If σ &gt; 0.9 then all test points have predicted ˆG = 0 (predicted as negative), \n# so TPR = FPR = 0.\n# If σ &lt; 0.1, then all test points have predicted ˆG = 1 (predicted as positive),\n# so TPR = FPR = 1.\n# Since there exist σ within the range from 0 to 1 for the two points to happen, \n# these two points can be added to the plot.\n\nThe CSV file Caravan.csv contains data on 5822 real customer records on caravan insurance purchase. This data set is owned and supplied by the Dutch data mining company, Sentient Machine Research, and is based on real world business data. Each record consists of 86 variables, containing socio-demographic data (variables 1-43) and product ownership (variables 44-86). Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy. For this business, assume that the overall error rate (equivalently, the accuracy) is not of interest. Instead, the company wants to use the classier to predict who are the potential customers likely to purchase insurance. Then the metric precision will be important, since it relates the proportion of Individuals who will actually purchase the insurance, among the group of individuals who are predicted to purchase insurance.\n\ncara &lt;- read.csv(\"Caravan data.csv\")\nhead(cara, n = 2)\n\n  X MOSTYPE MAANTHUI MGEMOMV MGEMLEEF MOSHOOFD MGODRK MGODPR MGODOV MGODGE\n1 1      33        1       3        2        8      0      5      1      3\n2 2      37        1       2        2        8      1      4      1      4\n  MRELGE MRELSA MRELOV MFALLEEN MFGEKIND MFWEKIND MOPLHOOG MOPLMIDD MOPLLAAG\n1      7      0      2        1        2        6        1        2        7\n2      6      2      2        0        4        5        0        5        4\n  MBERHOOG MBERZELF MBERBOER MBERMIDD MBERARBG MBERARBO MSKA MSKB1 MSKB2 MSKC\n1        1        0        1        2        5        2    1     1     2    6\n2        0        0        0        5        0        4    0     2     3    5\n  MSKD MHHUUR MHKOOP MAUT1 MAUT2 MAUT0 MZFONDS MZPART MINKM30 MINK3045 MINK4575\n1    1      1      8     8     0     1       8      1       0        4        5\n2    0      2      7     7     1     2       6      3       2        0        5\n  MINK7512 MINK123M MINKGEM MKOOPKLA PWAPART PWABEDR PWALAND PPERSAUT PBESAUT\n1        0        0       4        3       0       0       0        6       0\n2        2        0       5        4       2       0       0        0       0\n  PMOTSCO PVRAAUT PAANHANG PTRACTOR PWERKT PBROM PLEVEN PPERSONG PGEZONG\n1       0       0        0        0      0     0      0        0       0\n2       0       0        0        0      0     0      0        0       0\n  PWAOREG PBRAND PZEILPL PPLEZIER PFIETS PINBOED PBYSTAND AWAPART AWABEDR\n1       0      5       0        0      0       0        0       0       0\n2       0      2       0        0      0       0        0       2       0\n  AWALAND APERSAUT ABESAUT AMOTSCO AVRAAUT AAANHANG ATRACTOR AWERKT ABROM\n1       0        1       0       0       0        0        0      0     0\n2       0        0       0       0       0        0        0      0     0\n  ALEVEN APERSONG AGEZONG AWAOREG ABRAND AZEILPL APLEZIER AFIETS AINBOED\n1      0        0       0       0      1       0        0      0       0\n2      0        0       0       0      1       0        0      0       0\n  ABYSTAND Purchase\n1        0       No\n2        0       No\n\nstr(cara)\n\n'data.frame':   5822 obs. of  87 variables:\n $ X       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MOSTYPE : int  33 37 37 9 40 23 39 33 33 11 ...\n $ MAANTHUI: int  1 1 1 1 1 1 2 1 1 2 ...\n $ MGEMOMV : int  3 2 2 3 4 2 3 2 2 3 ...\n $ MGEMLEEF: int  2 2 2 3 2 1 2 3 4 3 ...\n $ MOSHOOFD: int  8 8 8 3 10 5 9 8 8 3 ...\n $ MGODRK  : int  0 1 0 2 1 0 2 0 0 3 ...\n $ MGODPR  : int  5 4 4 3 4 5 2 7 1 5 ...\n $ MGODOV  : int  1 1 2 2 1 0 0 0 3 0 ...\n $ MGODGE  : int  3 4 4 4 4 5 5 2 6 2 ...\n $ MRELGE  : int  7 6 3 5 7 0 7 7 6 7 ...\n $ MRELSA  : int  0 2 2 2 1 6 2 2 0 0 ...\n $ MRELOV  : int  2 2 4 2 2 3 0 0 3 2 ...\n $ MFALLEEN: int  1 0 4 2 2 3 0 0 3 2 ...\n $ MFGEKIND: int  2 4 4 3 4 5 3 5 3 2 ...\n $ MFWEKIND: int  6 5 2 4 4 2 6 4 3 6 ...\n $ MOPLHOOG: int  1 0 0 3 5 0 0 0 0 0 ...\n $ MOPLMIDD: int  2 5 5 4 4 5 4 3 1 4 ...\n $ MOPLLAAG: int  7 4 4 2 0 4 5 6 8 5 ...\n $ MBERHOOG: int  1 0 0 4 0 2 0 2 1 2 ...\n $ MBERZELF: int  0 0 0 0 5 0 0 0 1 0 ...\n $ MBERBOER: int  1 0 0 0 4 0 0 0 0 0 ...\n $ MBERMIDD: int  2 5 7 3 0 4 4 2 1 3 ...\n $ MBERARBG: int  5 0 0 1 0 2 1 5 8 3 ...\n $ MBERARBO: int  2 4 2 2 0 2 5 2 1 3 ...\n $ MSKA    : int  1 0 0 3 9 2 0 2 1 1 ...\n $ MSKB1   : int  1 2 5 2 0 2 1 1 1 2 ...\n $ MSKB2   : int  2 3 0 1 0 2 4 2 0 1 ...\n $ MSKC    : int  6 5 4 4 0 4 5 5 8 4 ...\n $ MSKD    : int  1 0 0 0 0 2 0 2 1 2 ...\n $ MHHUUR  : int  1 2 7 5 4 9 6 0 9 0 ...\n $ MHKOOP  : int  8 7 2 4 5 0 3 9 0 9 ...\n $ MAUT1   : int  8 7 7 9 6 5 8 4 5 6 ...\n $ MAUT2   : int  0 1 0 0 2 3 0 4 2 1 ...\n $ MAUT0   : int  1 2 2 0 1 3 1 2 3 2 ...\n $ MZFONDS : int  8 6 9 7 5 9 9 6 7 6 ...\n $ MZPART  : int  1 3 0 2 4 0 0 3 2 3 ...\n $ MINKM30 : int  0 2 4 1 0 5 4 2 7 2 ...\n $ MINK3045: int  4 0 5 5 0 2 3 5 2 3 ...\n $ MINK4575: int  5 5 0 3 9 3 3 3 1 3 ...\n $ MINK7512: int  0 2 0 0 0 0 0 0 0 1 ...\n $ MINK123M: int  0 0 0 0 0 0 0 0 0 0 ...\n $ MINKGEM : int  4 5 3 4 6 3 3 3 2 4 ...\n $ MKOOPKLA: int  3 4 4 4 3 3 5 3 3 7 ...\n $ PWAPART : int  0 2 2 0 0 0 0 0 0 2 ...\n $ PWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSAUT: int  6 0 6 6 0 6 6 0 5 0 ...\n $ PBESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PTRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBROM   : int  0 0 0 0 0 0 0 3 0 0 ...\n $ PLEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBRAND  : int  5 2 2 2 6 0 0 0 0 3 ...\n $ PZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAPART : int  0 2 1 0 0 0 0 0 0 1 ...\n $ AWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSAUT: int  1 0 1 1 0 1 1 0 1 0 ...\n $ ABESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ ATRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABROM   : int  0 0 0 0 0 0 0 1 0 0 ...\n $ ALEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABRAND  : int  1 1 1 1 1 0 0 0 0 1 ...\n $ AZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Purchase: chr  \"No\" \"No\" \"No\" \"No\" ...\n\ndim(cara)\n\n[1] 5822   87\n\n\n(a) Without any classier, if the company tries to sell insurance to a random selection of customers, what is the success rate?\n\ntable(cara$Purchase)\n\n\n  No  Yes \n5474  348 \n\nsuccesRate &lt;- table(cara$Purchase)[2]/sum(table(cara$Purchase))\nsuccesRate\n\n       Yes \n0.05977327 \n\n# data set shows almost 6% of people purchased insurance\n\n(b) Standardize the input features. \\(Hint\\): Use scale() command in R.\n\ncara &lt;- cara[,-1] #throwing the first column since it provides no information\nscaled.X &lt;- scale(cara[,-86]) #throwing the response column\n\n(c) Randomly select 1000 observations to form the test data, and the remaining observations will be the training data.\n\nset.seed(99)\nn &lt;- dim(cara)[1] #train data number\nshuffled.index &lt;- sample(c(1:n), size = 2000)\nX.train &lt;- scaled.X[-shuffled.index,]\nX.test&lt;- scaled.X[shuffled.index,]\nY.train &lt;- as.factor(cara$Purchase[-shuffled.index])\nY.test &lt;- as.factor(cara$Purchase[shuffled.index])\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a customer will purchase insurance. Compute the precision of the classier.\n\nlibrary(class)\n\npred &lt;- knn(X.train, X.test, Y.train, k=1)\nconfusion.matrix &lt;- table(Y.test, pred) \n# Precision = TP/TP+FP\nprecision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\nconfusion.matrix\n\n      pred\nY.test   No  Yes\n   No  1761  121\n   Yes  101   17\n\nprecision\n\n[1] 0.1231884\n\n\n(e) Repeat question 3d, for k-nearest neighbor classier where k = 3,5,10. Which value of k gives the best precision?\n\nprecision.all &lt;- data.frame()\nk.list &lt;- c(3, 5, 10)\nfor (k in k.list) {\n  pred &lt;- knn(X.train, X.test, Y.train, k = k)\n  confusion.matrix &lt;- table(Y.test, pred) \n  # Precision = TP/TP+FP\n  precision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\n  precision.all &lt;- rbind(precision.all, data.frame(k, precision))\n}\n\nprecision.all\n\n   k precision\n1  3 0.2285714\n2  5 0.3571429\n3 10 0.0000000\n\n# So far, k = 5 gives the best precision.\n# However, one might use N -fold cross validation \n# to have the average precision for each k.\n# With that, the value of k that gives largest average precision is chosen.\n\n\n\n\nOnsite Questions\nCOMING SOON"
  },
  {
    "objectID": "index.html#tutorial-6-knn-decision-tree",
    "href": "index.html#tutorial-6-knn-decision-tree",
    "title": "DSA1101: Everything You Need",
    "section": "Tutorial 6 (KNN & Decision Tree)",
    "text": "Tutorial 6 (KNN & Decision Tree)\n\nOffsite Questions\n\n(KNN and N -fold Cross Validation)\nLoan managers often need to take into account an applicant’s demographic and socioeconomic proles in deciding whether to approve a loan to the applicant, to minimize losses due to defaults. In this exercise we will build and evaluate a classier based on the German Credit Data to predict whether an applicant is considered as having good or bad credit risk. The features or predictors include (1) loan duration (in months), (2) credit amount, (3) Installment rate in percentage of disposable income and (4) age in years.\n(a) Read and explore the data from the file German_credit.csv.\n\ndata &lt;- read.csv(\"German credit.csv\")\nhead(data)\n\n  Creditability Duration Amount Instalment Age\n1             1       18   1049          4  21\n2             1        9   2799          2  36\n3             1       12    841          2  23\n4             1       12   2122          3  39\n5             1       12   2171          4  38\n6             1       10   2241          1  48\n\nstr(data)\n\n'data.frame':   1000 obs. of  5 variables:\n $ Creditability: int  1 1 1 1 1 1 1 1 1 1 ...\n $ Duration     : int  18 9 12 12 12 10 8 6 18 24 ...\n $ Amount       : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n $ Instalment   : int  4 2 2 3 4 1 1 2 4 1 ...\n $ Age          : int  21 36 23 39 38 48 39 40 65 23 ...\n\n\n(b) Standardize the input features.\n\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\nhead(X)\n\n       Duration     Amount  Instalment         Age\n[1,] -0.2407368 -0.7872630  0.91801781 -1.28093214\n[2,] -0.9870788 -0.1673006 -0.86974813  0.04034293\n[3,] -0.7382981 -0.8609500 -0.86974813 -1.10476213\n[4,] -0.7382981 -0.4071375  0.02413484  0.30459795\n[5,] -0.7382981 -0.3897785  0.91801781  0.21651294\n[6,] -0.9041519 -0.3649800 -1.76363111  1.09736299\n\n\n(c) Randomly select 800 customer records to form the training data, and the remaining 200 records will be the test data.\n\nset.seed(100)\nn &lt;- dim(data)[1]\nindex &lt;- sample(1:n, 800)\n\ntraining.X &lt;- X[index, ]\ntest.X &lt;- X[-index, ]\ntraining.Y &lt;- Y[index]\ntest.Y &lt;- Y[-index]\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a loan applicant is credible for the 200 test points. Compute the accuracy of the classier.\n\nlibrary(class)\npred &lt;- knn(training.X, test.X, training.Y, k=1)\nconfusion.matrix &lt;- table(test.Y, pred)\nconfusion.matrix\n\n      pred\ntest.Y  0  1\n     0 24 35\n     1 47 94\n\naccuracy &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\naccuracy\n\n[1] 0.59\n\n\n(e) Use N -folds cross validation with N = 5 to find the average accuracy for the 1-nearest neighbor classifier.\n\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\n\naccuracy &lt;- numeric(n_folds)\nfor (j in 1:n_folds) {\n  test_j &lt;- which(folds_j == j) \n  test.y &lt;- Y[test_j] \n  train.X &lt;- X[-test_j, ]  \n  test.X &lt;- X[test_j, ]  \n  train.y &lt;- Y[-test_j] \n\n  knn.pred &lt;- knn(train.X, test.X, train.y, k = 1) \n\n  confusion.matrix &lt;- table(test.y, knn.pred)\n  accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n}\n\nave.accuracy &lt;- round(mean(accuracy), digits = 3)\nave.accuracy\n\n[1] 0.622\n\n\n(f) Repeat question 1e for K-nearest neighbor classifiers where K = 1, 2, …100.\n\n### FULL KNN WITH N-FOLDS VALIDATION\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nk &lt;- 100\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\nave.accuracy &lt;- numeric(length(k))\n\nfor (i in 1:k) {\n  accuracy &lt;- numeric(n_folds)\n  for (j in 1:n_folds) {\n    test_j &lt;- which(folds_j == j) \n    test.y &lt;- Y[test_j] \n    train.X &lt;- X[-test_j, ]  \n    test.X &lt;- X[test_j, ]  \n    train.y &lt;- Y[-test_j] \n\n    knn.pred &lt;- knn(train.X, test.X, train.y, k = i) \n\n    confusion.matrix=table(test.y, knn.pred)\n    accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n  }\n  ave.accuracy[i] &lt;- round(mean(accuracy), digits = 3)\n}\n\nave.accuracy\n\n  [1] 0.622 0.597 0.661 0.674 0.684 0.671 0.676 0.686 0.686 0.691 0.702 0.699\n [13] 0.705 0.702 0.707 0.695 0.699 0.702 0.700 0.693 0.695 0.695 0.697 0.695\n [25] 0.699 0.697 0.696 0.700 0.693 0.688 0.697 0.692 0.697 0.698 0.697 0.692\n [37] 0.695 0.695 0.695 0.696 0.696 0.701 0.699 0.701 0.699 0.699 0.699 0.698\n [49] 0.699 0.696 0.698 0.699 0.699 0.696 0.695 0.699 0.696 0.697 0.696 0.698\n [61] 0.697 0.696 0.699 0.697 0.697 0.697 0.697 0.696 0.698 0.699 0.699 0.696\n [73] 0.698 0.697 0.698 0.696 0.698 0.698 0.698 0.699 0.699 0.698 0.698 0.700\n [85] 0.698 0.698 0.700 0.700 0.699 0.700 0.700 0.701 0.701 0.701 0.700 0.700\n [97] 0.699 0.698 0.699 0.699\n\n\n(g) Compare the 100 classifiers above, which few values of K give the best average accuracy?\n\nlibrary(glue)\nindex = which(ave.accuracy == max(ave.accuracy))\nplot(x = 1:k, ave.accuracy)\nabline(v = index, col = \"red\")\n\n\n\n\n\n\n\nglue(\"most accurate k = {index} with average accuracy = {ave.accuracy[index]}\")\n\nmost accurate k = 15 with average accuracy = 0.707\n\n\n(Decision Trees)\nConsider the famous Iris Flower Data set which was rst introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation: the length and the width of the sepals and petals (in cm).\n\n(a) Use decision tree method to predict Iris species based on all four features.\n\niris &lt;- read.csv(\"Iris dataset.csv\")\nhead(iris)\n\n  sepal.length sepal.width petal.length petal.width       class\n1          5.1         3.5          1.4         0.2 Iris-setosa\n2          4.9         3.0          1.4         0.2 Iris-setosa\n3          4.7         3.2          1.3         0.2 Iris-setosa\n4          4.6         3.1          1.5         0.2 Iris-setosa\n5          5.0         3.6          1.4         0.2 Iris-setosa\n6          5.4         3.9          1.7         0.4 Iris-setosa\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ sepal.length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ sepal.width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ petal.length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal.width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\ndim(iris)\n\n[1] 150   5\n\nlibrary(rpart)\nfit &lt;- rpart(class ~ sepal.length + sepal.width + petal.length + petal.width,\n              data = iris,\n              method=\"class\",\n              parms=list(split='information'),\n              control = rpart.control( minsplit =1))\n\n(b) Visualize the decision tree above, using the rpart.plot function.\n\nlibrary(rpart.plot)\nrpart.plot(fit, type=4, extra=2)\n\n\n\n\n\n\n\n# The fitted tree is given in the figure below.\n# If the measurement of petal length is less than 2.5 cm \n# then the flower is of Iris-setosa\n# If the petal length is ≥ 2.5 cm with the petal width is ≥ 1.8 cm \n# then high chance (45/46) it will be an Iris-virginica.\n# If the petal length is ≥ 2.5 cm with the petal width is &lt; 1.8 cm\n# then we continue to check if the petal length is in the interval\n# [2.5, 5]. If yes, then high chance (47/48) it is Iris-versicolor.\n\n(c) What are the more important features in the fitted tree above?\n\n# It seems the sepal length and sepal width \n# are not important in the classification while\n# the petal length and petal width are more important."
  },
  {
    "objectID": "index.html#t1-introduction-to-r",
    "href": "index.html#t1-introduction-to-r",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T1 (Introduction to R)",
    "text": "T1 (Introduction to R)\n\nOffsite Questions (Functions)\nGeneral idea of this problem: You have just graduated from NUS and just started your first job. You plan to buy a flat on your own which has price = $1,200,000 (1.2 million dollars). You need to save money for several years before you can afford to make the down payment which is 25% of the flat’s price.\n\nCall the amount that you have saved thus far: saved. You start the very first month with a savings of $10,000 that your parents gave you.\nCall your monthly salary as salary which is paid at the end of every month. Each month, you are going to dedicate 40% of your salary to save for the down payment.\nAssume that you invest your savings wisely, with a monthly average return of 2%. That means: at the end of each month, you receive an additional of saved ×0.02 funds where saved is the amount you have from end of previous month to put into your savings.\nAt the end of each month, your savings will be increased by the return on your investment, plus 40% of your monthly salary.\n\nNote: In your code for the questions below, you MUST use the names as given in bold above.\n\nWrite the code to calculate how many months it will take you to save up enough money for the down payment for two persons of of diferent salary: (i) salary = $7,000; and (ii) salary = $10,000.\n\n# Solution 1.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 7000\nsaved &lt;- 10000\nmonth &lt;- 0\n\nwhile(saved &lt; cost){\nmonth &lt;- month + 1\nsaved &lt;- saved + 0.4 * salary + 0.02 * saved }\nprint(month) # 55\n\n[1] 55\n\n# Solution 2.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 10000\nsaved &lt;- 10000\nmonth &lt;- 0\n\nwhile(saved &lt; cost){\nmonth &lt;- month + 1\nsaved &lt;- saved + 0.4 * salary + 0.02 * saved }\nprint(month) # 44\n\n[1] 44\n\n\nIn question above, we unrealistically assumed that the salary doesn’t change over the years. However, now we consider that the salary will be raised every 4 months by a rate named rate, this variable should be in decimal form (i.e. 0.03 for 3%). The new salary will be applied for the month after every batch of 4 months.\nWith this further assumption, write the code to calculate how many months it will take a person to save up enough money for the down payment if that person has (i) (salary = $7,000 and rate = 0.02); (ii) (salary = $10,000 and rate = 0.01).\n\n# Solution 2.i\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 7000\nsaved &lt;- 10000\nmonth &lt;- 0\nrate &lt;- 0.02\n\nwhile (saved &lt; cost) {\n  month &lt;- month + 1\n  saved &lt;- saved + 0.4 * salary + 0.02 * saved\n  if (month %% 4 == 0) {\n    salary &lt;-  salary * (1 + rate)\n  }\n}\nprint(month) #52\n\n[1] 52\n\n# Solution 2.ii\ncost &lt;- 1200000 * 0.25 \nsalary &lt;- 10000\nsaved &lt;- 10000\nmonth &lt;- 0\nrate &lt;- 0.01\n\nwhile (saved &lt; cost) {\n  month &lt;- month + 1\n  saved &lt;- saved + 0.4 * salary + 0.02 * saved\n  if (month %% 4 == 0) {\n    salary &lt;-  salary * (1 + rate)\n  }\n}\nprint(month) #43\n\n[1] 43\n\n\n\n\n\nOnsite Questions (Loops & Which)\n\nA sequence is generated using the following recursive relation:\n\n\\(x_n = 2x_{n-1} - x_{n-2} + 5, \\quad \\text{for } n \\geq 3,\\)\nwith \\(x_1 = 0\\) and \\(x_2 = 1\\).\n\nUse a “for” loop in R to find the 30th term of the series.\n\nx &lt;- numeric(30)\nx[1] &lt;- 0\nx[2] &lt;- 1 \nfor(i in 3:30) {x[i] &lt;- 2*x[i-1] - x[i-2] + 5}\nx[30] # 2059\n\n[1] 2059\n\n\nFind the smallest value of (n) such that \\(x_n \\geq 1,000\\).\n\nprint(which(x &gt;= 1000))\n\n[1] 22 23 24 25 26 27 28 29 30\n\n# all elements from x_22 onwards are larger than 1000\n# x_22 is the smallest, thus answer is n = 22\n\n\n\n\nConsider another sequence which is generated using the following recursive relation:\n\n\\(y_1 = 2800 + 1.02 \\times y_0, \\quad \\text{with } y_0 = 10000\\),\n\\(y_n = 2800 + 1.02 \\times y_{n-1}, \\quad \\text{for } n \\geq 2\\).\nFind the smallest value of \\(n\\) such that \\(y_n \\geq 300,000\\).\n\ny &lt;- numeric()\ny &lt;- append(y, 2800 + 1.02*10000) \nwhile (max(y) &lt; 300000) {\n  y &lt;- c(y, 2800 + 1.02*max(y))\n}\nlength(y) # 55\n\n[1] 55\n\ny[55] # 305759.6\n\n[1] 305759.6"
  },
  {
    "objectID": "index.html#t2-basic-prob-and-stats",
    "href": "index.html#t2-basic-prob-and-stats",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T2 (Basic Prob and Stats)",
    "text": "T2 (Basic Prob and Stats)\n\n#df = read.csv(...) \n# Imports the dataframe\n# BY DEFAULT, READ.CSV:\n# 1. Assumes values are separated by commas\n# 2. Assumes the data contains headers\n\ndf = read.csv(\n    \"ex_1.txt\",\n    sep = \" \",\n    header = FALSE\n) \n# for tab use \"\\t\"\n\n\n### Interpreting Scatterplots\n# Is there any relationship? Is it strong?\n# If there is, is it positive or negative?\n# Relationship is linear or non-linear?\n# Special observations?\n# Is the variability of the response stable when x changes? ***\n\n\n### Interpreting Boxplots\n# How many outliers\n# Median\n# Distribution\n# Any visible pattern? (Optional)\n\n\nOffsite Questions (Box, Scatter, QQ, Hist)\nForced Expiratory Volume (FEV) is an index of pulmonary function that measures the volume of air expelled after 1 second of constant effort. The dataset FEV.csv contains measurements for 654 children aged 3 to 19 years of age. The purpose of the data collection was to study how FEV is affected by certain other variables. The variables that we shall work with are\nAge: Age in years.\nFEV: FEV measurement.\nHgt: Height in inches.\nheight: Height in meters\nSex: 0 = female, 1 = male.\nSmoking status: 0 = current non-smoker, 1 = current smoker.\n(a) What is the response variable in this study?\n\n# FEV is the response variable in this study\n\n(b) Create a histogram of FEV and comment on it.\n\nfev &lt;- read.csv(\"FEV.csv\")\nhist(fev$FEV)\n\n\n\n\n\n\n\n# Comments:\n# range: 0.5 to 6\n# unimodal\n# slightly right skewed\n# may have outliers\n\n(c) Create a boxplot of FEV and identify how many outliers there are. Investigate your data and comment on these outliers.\n\nbp &lt;- boxplot(fev$FEV)\n\n\n\n\n\n\n\noutliers &lt;- bp$out\nlength(outliers)\n\n[1] 9\n\n# There are 9 outliers\n\nindex &lt;- which(fev$FEV %in% outliers)\nfev[index,]\n\n       ID Age  FEV Hgt Sex Smoke height\n321  2142  14 4.84  72   1     0   1.83\n452 33041  12 5.22  70   1     0   1.78\n464 37241  13 4.88  73   1     0   1.85\n517 49541  13 5.08  74   1     0   1.88\n609  6144  19 5.10  72   1     0   1.83\n624 25941  15 5.79  69   1     0   1.75\n632 37441  17 5.63  73   1     0   1.85\n648 71141  17 5.64  70   1     0   1.78\n649 71142  16 4.87  72   1     1   1.83\n\n# Comments:\n# 1. All outliers are male\n# 2. Most (8/9) are\n# non-smokers\n# 3. They are rather\n# tall\n\n(d) Generally, is the sample of FEV normally distributed?\n\nqqnorm(fev$FEV, pch = 20)\nqqline(fev$FEV, col = \"red\")\n\n\n\n\n\n\n\n# Comments:\n# Left tail sample quantiles are larger than expected,\n# hence left tail shorter than normal. \n# Right tail sample quantiles larger than expected, hence # right tail longer than normal.\n\n# Combined with the histogram of FEV, it is clear that the\n# sample of FEV is not normally distributed and quite \n# right skewed.\n\n(e) Create separate histograms for male and female FEV, then obtain separate numerical summaries for males and female FEV. Comment on what you observe.\n\n# First, obtain the male and female FEV values separately\n# to plot onto two different histograms.\n\nfemale = fev$FEV[which(fev$Sex==0)]\n# Alternative: fev$FEV[fev$Sex==0]\nmale = fev$FEV[which(fev$Sex==1)] \n# Alternative: fev$FEV[fev$Sex==1]\n\n# Now plot the two histograms side-by-side\n\nopar &lt;- par(mfrow=c(1,2)) \nhist(female, col = 2, freq= FALSE, \nmain = \"Histogram of Female FEV\", ylim = c(0,0.52))\nhist(male, col = 4, freq= FALSE, \nmain = \"Histogram of Male FEV\", ylim = c(0,0.52))\n\n\n\n\n\n\n\n# Remember to use par(mfrow=c(1,1))  to reset the diagram layout!\n\n# obtaining separate numerical summaries for male and female\nIQR(female) # 1.04\n\n[1] 1.04\n\nsummary(female) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.790   1.950   2.490   2.452   2.990   3.840 \n\nvar(female) # 0.4169424\n\n[1] 0.4169424\n\nIQR(male) # 1.5275\n\n[1] 1.5275\n\nsummary(male)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.800   2.007   2.605   2.813   3.535   5.790 \n\nvar(male) # 1.006866\n\n[1] 1.006866\n\n# Comments:\n# Both histogram are unimodal but have different shapes. \n# It is almost symmetrical for females, but quite right-skewed for males.\n# Median FEV for females is much lower than males, 2.49 compared to 2.605.\n# Variability for males is higher than females.\n# The respective IQR are 1.54 and 1.05.\n\n(f) Create a scatterplot with height (in metres) on the x-axis and FEV on the y-axis.\n\nplot(fev$height, fev$FEV)\npoints(female ~ fev$height[which(fev$Sex==0)], \ncol = \"red\", pch = 20)\npoints(male ~ fev$height[which(fev$Sex==1)], \ncol = \"darkblue\", pch = 20)\nlegend(1.2, 5, legend = c(\"Female\", \"Male\"), \ncol = c(\"red\",\"darkblue\"), pch=c(20,20))\n\n\n\n\n\n\n\n\n(g) Compute the correlation between FEV and height and comment on your results.\n\ncor(fev$FEV, fev$height) # 0.8675619\n\n[1] 0.8675619\n\n# Computed correlation is quite high.\n# It is clear from the plot that there is a strong \n# positive linear association between FEV and height. \n# The range of FEV and height for males appears larger than for females. \n# The variability of FEV at lower heights seems slightly \n# less than the variability of FEV at greater heights.\n\n\n\nOnsite Questions (Box)\nConsider a dataset about HDB resale flats in Singapore given in hdbresale_reg.csv, which helps to investigate the factors that affect the resale price of the flats.\n\nImport the dataset into R.\n\nhdb = read.csv(\"hdbresale_reg.csv\")\nnames(hdb)\n\n [1] \"X\"                   \"month\"               \"town\"               \n [4] \"flat_type\"           \"block\"               \"street_name\"        \n [7] \"storey_range\"        \"floor_area_sqm\"      \"flat_model\"         \n[10] \"lease_commence_date\" \"resale_price\"       \n\n\nHow many flats are in the sample given?\n\ndim(hdb)\n\n[1] 6055   11\n\n# the dataframe has 6055 rows and 11 columns\n# thus there are 6055 flats\n\nExploring the variable resale_price:\n\nCreate a histogram. Give your comments. Is the sample of resale prices normally distributed?\n\n\nhist(hdb$resale_price)\n\n\n\n\n\n\n\n# range: 200k to ~1 mil\n# unimodal\n# clearly right skewed\n# suspected outliers\n\n\nCreate a box plot.\n\n\nbp &lt;- boxplot(hdb$resale_price)\n\n\n\n\n\n\n\noutliers &lt;- bp$out\nhead(outliers)\n\n[1] 680000 728000 680000 707000 640000 659500\n\nlength(outliers) # 284 outliers\n\n[1] 284"
  },
  {
    "objectID": "index.html#t3-linear-regression-1",
    "href": "index.html#t3-linear-regression-1",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T3 (Linear Regression 1)",
    "text": "T3 (Linear Regression 1)\n\nOffsite Questions (Functions)\nConsider the question given in Tutorial 1.\n(a) For the first question in Tutorial 1, use the code to define a function, called F1, where the argument of F1 is salary. Run function F1 for the two cases mentioned.\n\ncost &lt;- 1200000 * 0.25 \nF1 &lt;- function(salary) {\n  saved &lt;- 10000\n  month &lt;- 0\n  while(saved &lt; cost) {\n    month &lt;- month + 1\n    saved &lt;- saved + 0.4 * salary + 0.02 * saved\n    }\n  return(month)\n}\n\n(b) For the second question in Tutorial 1, use the code to define a function, called F2, where F2 has two arguments: salary and rate. Run function F2 for the two cases mentioned to obtain the results.\n\nF2 &lt;- function(salary, price = 1200000, rate = 0.01, portion_save = 0.4) {\n  r = 0.02 # monthly rate return from investment\n  saved &lt;- 10000 # savings given by parents initially\n  month &lt;- 0\n  cost = 0.25*price\n  while(saved &lt; cost) {\n    month = month +1\n    saved = saved + portion_save * salary + saved * r\n    if (month %% 4 ==0) {\n      salary = salary*(1+rate)\n    }\n  }\n  return(month)\n}\n\n\n\nOnsite Questions (LM Mathematics)\n\nRead the data from the file Colleges.txt. Consider a simple linear regression of the percentage of applicants accepted (Acceptance) on the median combined math and verbal SAT score of students (SAT), called Model M1.\n(a) Write your own function in R, name the function as simple, to derive the intercept β0 and the slope β1 of Model M1. Hint: Use the formula of the estimated coefficients, β̂1 and β̂0, given in slide 31/52 of Topic 3.\n\nsimple &lt;- function(x, y) {\n    beta1 = (sum(y*x) - sum(y)*mean(x)) /\n(sum(x^2) - sum(x)*mean(x))\n    beta0 = mean(y)- beta1 * mean(x)\n    return(c(beta0, beta1))\n}\n\ndata = read.csv(\"Colleges.txt\", sep = \"\\t\")\nhead(data)\n\n      School School_Type  SAT Acceptance DPerStudent Top.10p PerPhD GradPer\n1    Amherst    Lib Arts 1315         22       26636      85     81      93\n2 Swarthmore    Lib Arts 1310         24       27487      78     93      88\n3   Williams    Lib Arts 1336         28       23772      86     90      93\n4    Bowdoin    Lib Arts 1300         24       25703      78     95      90\n5  Wellesley    Lib Arts 1250         49       27879      76     91      86\n6     Pomona    Lib Arts 1320         33       26668      79     98      80\n\nsimple(data$SAT, data$Acceptance)\n\n[1] 202.2677440  -0.1300894\n\n\n(b) Use function lm() in R to derive the coefficients of Model M1. Compare with your answer in part (a).\n\nlm(Acceptance ~ SAT , data = data)\n\n\nCall:\nlm(formula = Acceptance ~ SAT, data = data)\n\nCoefficients:\n(Intercept)          SAT  \n   202.2677      -0.1301  \n\n# Same coefficients from different functions\n# Helps verify implementation of 'simple' function\n\n(If time permits), consider a dataset about HDB resale flats in Singapore given in hdbresale_reg.csv. Consider a simple model (Model M2) where the resale price is the response and the floor area in square meters is the only regressor.\n(a) Use the function simple you formed in the question above to find the coefficients of Model M2.\n\ndf &lt;- read.csv(\"hdbresale_reg.csv\")\nnames(df)\n\n [1] \"X\"                   \"month\"               \"town\"               \n [4] \"flat_type\"           \"block\"               \"street_name\"        \n [7] \"storey_range\"        \"floor_area_sqm\"      \"flat_model\"         \n[10] \"lease_commence_date\" \"resale_price\"       \n\nsimple(x = df$floor_area_sqm, y = df$resale_price)\n\n[1] 115145.730   3117.212\n\n\n(b) Use the function lm() in R to derive the coefficients of Model M2.\n\nlm(df$resale_price ~ df$floor_area_sqm)\n\n\nCall:\nlm(formula = df$resale_price ~ df$floor_area_sqm)\n\nCoefficients:\n      (Intercept)  df$floor_area_sqm  \n           115146               3117"
  },
  {
    "objectID": "index.html#t4-linear-regression-2",
    "href": "index.html#t4-linear-regression-2",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T4 (Linear Regression 2)",
    "text": "T4 (Linear Regression 2)\nLinear Model Assumptions:\n\nQuantitative\nSymmetric - Histogram\nVariability of y is stable when x changes - Scatterplot\n\nTransformations:\ny -&gt; log(y), sqrt(y), 1/y\nEvaluating Goodness-Of-Fit:\n\nF-Test (p-value)\nR-squared\n\n\nOffsite Questions (Hist, Linear Regression)\nConsider data set given in the file hdbresale_reg.csv on Canvas, which has the information of 6055 HDB resale flats in Singapore. We would want to form a linear model that helps to predict the resale price of HDB flats, based on the floor area in square meters and the type of the flats.\n(a) Consider the resale price, plot a histogram of it and give your comments. Is it suitable to fit a\nlinear model for this response variable? Explain.\n\ndata &lt;- read.csv(\"hdbresale_reg.csv\")\nstr(data)\n\n'data.frame':   6055 obs. of  11 variables:\n $ X                  : int  580 581 582 583 584 585 586 587 588 589 ...\n $ month              : chr  \"2012-03\" \"2012-03\" \"2012-03\" \"2012-03\" ...\n $ town               : chr  \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" ...\n $ flat_type          : chr  \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" ...\n $ block              : chr  \"640\" \"640\" \"668\" \"5\" ...\n $ street_name        : chr  \"ROWELL RD\" \"ROWELL RD\" \"CHANDER RD\" \"TG PAGAR PLAZA\" ...\n $ storey_range       : chr  \"01 TO 05\" \"06 TO 10\" \"01 TO 05\" \"11 TO 15\" ...\n $ floor_area_sqm     : num  74 74 73 59 68 75 68 77 82 105 ...\n $ flat_model         : chr  \"Model A\" \"Model A\" \"Model A\" \"Improved\" ...\n $ lease_commence_date: int  1984 1984 1984 1977 1979 2003 1980 1974 1978 2002 ...\n $ resale_price       : num  380000 388000 400000 460000 488000 ...\n\nhist(data$resale_price)\n\n\n\n\n\n\n\n# Comments: \n# Right-skewed histogram\n# Hence, resale price is NOT suitable to be the response \n# as assumption of linear model (symmetric) is violated.\n# For a right skewed variable, it is suggested to try \n# transforming the response by taking its logarithm.\n\n(b) Consider the resale price, plot a histogram of log_e of it and give your comments. Is it more\nsuitable to fit a linear model for this response variable than the original resale price?\n\nhist(log(data$resale_price))\n\n\n\n\n\n\n\n# Comments: \n# The histogram of the log of the resale price \n# is more symmetric, hence it is more suitable than \n# the original resale price as a response for our linear model.\n\n(c) Derive a scatter plot of the log_e of the resale price against the floor area in square meters. Give your comments.\n\n# creates a new column for the log(price)\ndata$log.price = log(data$resale_price) \nplot(data$log.price ~ data$floor_area_sqm)\n\n\n\n\n\n\n\n# Comments:\n# There seems to be a strong, positive, linear relationship\n# between log(price) and floor area.\n# The variability of the log(price) seems fairly stable \n# when the floor area changes.***\n\n(d) Fit a linear model where the log of the resale price be the response. Write down the fitted equation.\n\nstr(data)\n\n'data.frame':   6055 obs. of  12 variables:\n $ X                  : int  580 581 582 583 584 585 586 587 588 589 ...\n $ month              : chr  \"2012-03\" \"2012-03\" \"2012-03\" \"2012-03\" ...\n $ town               : chr  \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" \"CENTRAL AREA\" ...\n $ flat_type          : chr  \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" ...\n $ block              : chr  \"640\" \"640\" \"668\" \"5\" ...\n $ street_name        : chr  \"ROWELL RD\" \"ROWELL RD\" \"CHANDER RD\" \"TG PAGAR PLAZA\" ...\n $ storey_range       : chr  \"01 TO 05\" \"06 TO 10\" \"01 TO 05\" \"11 TO 15\" ...\n $ floor_area_sqm     : num  74 74 73 59 68 75 68 77 82 105 ...\n $ flat_model         : chr  \"Model A\" \"Model A\" \"Model A\" \"Improved\" ...\n $ lease_commence_date: int  1984 1984 1984 1977 1979 2003 1980 1974 1978 2002 ...\n $ resale_price       : num  380000 388000 400000 460000 488000 ...\n $ log.price          : num  12.8 12.9 12.9 13 13.1 ...\n\nunique(data$flat_type)\n\n[1] \"3 ROOM\"    \"4 ROOM\"    \"5 ROOM\"    \"EXECUTIVE\" \"2 ROOM\"   \n\nM = lm(log.price ~ floor_area_sqm + flat_type, data = data)\nsummary(M)\n\n\nCall:\nlm(formula = log.price ~ floor_area_sqm + flat_type, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.28208 -0.07054 -0.01515  0.04323  0.79797 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.235e+01  1.891e-02 653.013  &lt; 2e-16 ***\nfloor_area_sqm     3.712e-03  1.817e-04  20.429  &lt; 2e-16 ***\nflat_type3 ROOM    1.190e-01  1.714e-02   6.944  4.2e-12 ***\nflat_type4 ROOM    2.093e-01  1.869e-02  11.196  &lt; 2e-16 ***\nflat_type5 ROOM    2.762e-01  2.099e-02  13.160  &lt; 2e-16 ***\nflat_typeEXECUTIVE 4.302e-01  2.527e-02  17.023  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1156 on 6049 degrees of freedom\nMultiple R-squared:  0.7116,    Adjusted R-squared:  0.7114 \nF-statistic:  2986 on 5 and 6049 DF,  p-value: &lt; 2.2e-16\n\n# fitted_log(price) = \n# 12.35 + \n# 0.003712 * floor_area_sqm +\n# 0.119 ∗ I(flat type = 3 ROOM) +\n# 0.2093 ∗ I(flat type = 4 ROOM) +\n# 0.2762 ∗ I(flat type = 5 ROOM) +\n# 0.4302 ∗ I(flat type = Executive)\n\n(e) Report the coeficient of the floor area in square meters and interpret it.\n\n# The coefficient of it is 0.003712. \n# Meaning when comparing two flats of the same type, \n# then an increase of 1 square meter will increase \n# the predicted log(price) by 0.003712.\n# Equivalently, the price will \n# increase by e^0.003712 = 1.003719 TIMES.\n\n(f) Predict the resale price of a 4-room HDB at that is of 100 square meters.\n\nnew = data.frame(\n  floor_area_sqm = 100, \n  flat_type = \"4 ROOM\"\n  )\n\npredicted_log.price = predict(M, new)\npredicted_price = exp(predicted_log.price)\nprint(predicted_price) # 412807.6\n\n       1 \n412807.6 \n\n\n(g) Report \\(R^2\\) of the model and interpret it.\n\nsummary(M)$r.squared\n\n[1] 0.7116378\n\n# The R_squared value is 0.712. \n# That means model M can explain 71.2% \n# of the variability of the response in the sample.\n\n\n\nOnsite Questions (Linear Regression)\n\nA dataset on house selling price was randomly collected (house_selling_prices_FL.csv). It’s our interest to model how y = selling price (dollar) is dependent on x = the size of the house (square feet). A simple linear regression model (y regress on x) was fitted, called Model 1.\n\nThe given data has another variable, NW, which specifies if a house is in the part of the town considered less desirable (NW=0).\n(a) Derive the correlation between x and y.\n\ndata &lt;- read.csv(\"house_selling_prices_FL.csv\")\nstr(data)\n\n'data.frame':   100 obs. of  9 variables:\n $ House   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes   : int  1360 1050 1010 830 2150 1230 150 1470 1850 320 ...\n $ Bedrooms: int  3 1 3 3 3 3 2 3 3 3 ...\n $ Baths   : num  2 1 1.5 2 2 2 2 2 2 2 ...\n $ Quadrant: chr  \"NW\" \"NW\" \"NW\" \"SW\" ...\n $ NW      : int  1 1 1 0 1 1 1 1 1 1 ...\n $ price   : int  145000 68000 115000 69000 163000 69900 50000 137000 121300 70000 ...\n $ size    : int  1240 370 1130 1120 1710 1010 860 1420 1270 1160 ...\n $ lot     : int  18000 25000 25000 17000 14000 8000 15300 18000 16000 8000 ...\n\ncor(data$price, data$size)\n\n[1] 0.7612621\n\n# Correlation between\n# 'price' and 'size' is 0.761\n\n(b) Derive a scatter plot of y against x. Give your comments on the association of y and x.\n\nplot(data$price ~ data$size, pch = 20)\n\n\n\n\n\n\n\nplot(data$size, data$price, pch = 20)\n\n\n\n\n\n\n\n# 1. There is a clear relationship.\n# 2. It is a positive association.\n# 3. It seems linear.\n# 4. Variability of price is quite stable as size changes.\n\n(c) Derive \\(R^2\\) of Model 1. Verify that \\(\\sqrt{R^2} = |cor(y, x)|\\). In which situation can we have \\(\\sqrt{R^2} = cor(y, x)\\)?\n\nM1 &lt;- lm(price ~ size, data = data)\nr.squared &lt;- summary(M1)$r.squared\nprint(r.squared) # 0.57952\n\n[1] 0.57952\n\nprint(sqrt(r.squared)) # 0.7612621\n\n[1] 0.7612621\n\n# Same as correlation value derived from 1.a.\n\n# Given that we have verified that \n# sqrt(R^2) = abs(cor(y, x)),\n# sqrt(R^2) = cor(y, x) when \n# abs(cor(y, x)) = cor(y, x).\n# Thus, when cor(y, x) &gt; 0,\n# then in a simple model y ∼ x, \n# we always have sqrt(R^2) = cor(y, x).\n\n(d) Form a model (called Model 2) which has two regressors (x and NW). Report the coefficient of variable NW in Model 2. Interpret it.\n\nstr(data)\n\n'data.frame':   100 obs. of  9 variables:\n $ House   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes   : int  1360 1050 1010 830 2150 1230 150 1470 1850 320 ...\n $ Bedrooms: int  3 1 3 3 3 3 2 3 3 3 ...\n $ Baths   : num  2 1 1.5 2 2 2 2 2 2 2 ...\n $ Quadrant: chr  \"NW\" \"NW\" \"NW\" \"SW\" ...\n $ NW      : int  1 1 1 0 1 1 1 1 1 1 ...\n $ price   : int  145000 68000 115000 69000 163000 69900 50000 137000 121300 70000 ...\n $ size    : int  1240 370 1130 1120 1710 1010 860 1420 1270 1160 ...\n $ lot     : int  18000 25000 25000 17000 14000 8000 15300 18000 16000 8000 ...\n\ndata$NW = as.factor(data$NW)\nstr(data)\n\n'data.frame':   100 obs. of  9 variables:\n $ House   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes   : int  1360 1050 1010 830 2150 1230 150 1470 1850 320 ...\n $ Bedrooms: int  3 1 3 3 3 3 2 3 3 3 ...\n $ Baths   : num  2 1 1.5 2 2 2 2 2 2 2 ...\n $ Quadrant: chr  \"NW\" \"NW\" \"NW\" \"SW\" ...\n $ NW      : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 2 2 2 2 ...\n $ price   : int  145000 68000 115000 69000 163000 69900 50000 137000 121300 70000 ...\n $ size    : int  1240 370 1130 1120 1710 1010 860 1420 1270 1160 ...\n $ lot     : int  18000 25000 25000 17000 14000 8000 15300 18000 16000 8000 ...\n\nM2 = lm(price ~ size + NW, data = data)\nsummary(M2)\n\n\nCall:\nlm(formula = price ~ size + NW, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-83207 -22968    215  14135 109149 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -15257.514  11908.297  -1.281 0.203160    \nsize            77.985      6.209  12.560  &lt; 2e-16 ***\nNW1          30569.087   7948.742   3.846 0.000215 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34390 on 97 degrees of freedom\nMultiple R-squared:  0.6352,    Adjusted R-squared:  0.6276 \nF-statistic: 84.43 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n# Fitted equation of linear model:\n# fitted_price = -15257.514 + 77.985 * size + \n#                   30569.087 * I(NW = 1)\n# The estimated coefficient of NW in Model 2 is 30569.1 \n# This means that for houses of same size, those with \n# NW = 1 (Those located at NorthWest area) has predicted \n# price higher than that in the area of NW = 0 by $30569.1\n\n(e) Estimate and report the price of a house where its size is 4000 square feet and is located at the more desirable part of the town.\n\nnew_data &lt;- data.frame(size = 4000, NW = \"1\")\npredict(M2, newdata = new_data)\n\n       1 \n327252.1 \n\n# The predicted price of a house with size x = 4000 and \n# NW = 1 is $327252.1."
  },
  {
    "objectID": "index.html#t5-k-nearest-neighbor",
    "href": "index.html#t5-k-nearest-neighbor",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T5 (K-Nearest Neighbor)",
    "text": "T5 (K-Nearest Neighbor)\n\nOffsite Questions (Multiple Scater, Linear Regression)\n\n(MLR) Consider the horseshoe female crab data given in the csv file crab.csv. We would want to form a model for the weight of the female crabs (kg), which depends on its width (cm) and its spine condition (1 = both good, 2 = one worn or broken, 3 = both worn or broken).\n\ndata &lt;- read.csv(\"crab.csv\")\nhead(data)\n\n  color spine width satell weight\n1     3     3  28.3      8   3.05\n2     4     3  22.5      0   1.55\n3     2     1  26.0      9   2.30\n4     4     3  24.8      0   2.10\n5     4     3  26.0      4   2.60\n6     3     3  23.8      0   2.10\n\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : int  3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\ndata$spine &lt;- as.factor(data$spine)\n# 1 = both good\n# 2 = one worn or broken\n# 3 = both worn or broken\nstr(data)\n\n'data.frame':   173 obs. of  5 variables:\n $ color : int  3 4 2 4 4 3 2 4 3 4 ...\n $ spine : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 1 3 3 3 1 2 1 3 ...\n $ width : num  28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ...\n $ satell: int  8 0 9 0 4 0 0 0 0 0 ...\n $ weight: num  3.05 1.55 2.3 2.1 2.6 2.1 2.35 1.9 1.95 2.15 ...\n\n\n(a) Produce a scatter plot of variable weight against width for different condition of spine.\n\nattach(data)\nplot(weight ~ width, pch = 20, main = \"weight vs width for each spine class\",\n     xlab = \"width\", ylab = \"weight\")\npoints(weight[spine == \"1\"] ~ width[spine ==\"1\"], pch = 15, col = \"red\")\npoints(weight[spine == \"2\"] ~ width[spine ==\"2\"], pch = 16, col = \"blue\")\npoints(weight[spine == \"3\"] ~ width[spine ==\"3\"], pch = 17, col = \"green\")\nlegend(22,4,legend=c(\"1\", \"2\", \"3\"),col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Fit a linear regression model for weight which has two explanatories, width and spine.\n\nmodel &lt;- lm(weight ~ width + spine, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n\n(c) Is the fitted model signicant?\n\n# the model f statistic showing p value is less than 2.2 x 10^-16.\n# Since this is far below 0.05, the model can be considered significant\n\n(d) Derive \\(R^2\\) and adjusted \\(R^2\\) of the fitted model.\n\nsummary(model)$r.squared\n\n[1] 0.7917598\n\n# 0.7917598\nsummary(model)$adj.r.squared\n\n[1] 0.7880632\n\n# 0.7880632\n\n(e) Write down the fitted model.\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ width + spine, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23016 -0.10828  0.01016  0.13356  0.96350 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.92955    0.27506 -14.286   &lt;2e-16 ***\nwidth        0.24376    0.01002  24.335   &lt;2e-16 ***\nspine2       0.05544    0.08475   0.654    0.514    \nspine3      -0.06969    0.05065  -1.376    0.171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2656 on 169 degrees of freedom\nMultiple R-squared:  0.7918,    Adjusted R-squared:  0.7881 \nF-statistic: 214.2 on 3 and 169 DF,  p-value: &lt; 2.2e-16\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) -0.06969*I(spine=3)\n\n(f) Two female crabs of the same width, and the diference of their weight if one has spines are of good condition and another one with broken spines.\n\n# weight = -3.92955 + 0.24376*width + 0.05544*I(spine=2) + -0.06969*I(spine=3)\n# weight1 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*0\n# weight2 = -3.92955 + 0.24376*width + 0.05544*0 + -0.06969*1\n\nweight.dif &lt;- (0.05544*0 + -0.06969*0) - (0.05544*0 + -0.06969*1)\nweight.dif\n\n[1] 0.06969\n\n\n(g) Predict the weight of a female crab that has width of 27 cm and has both spines worn or broken.\n\nnewdata &lt;- data.frame(width = 27, spine = \"3\")\npredict(model, newdata = newdata)\n\n       1 \n2.582352 \n\n\nMeasures of classier performance\nSuppose we have developed a K-nearest neighbors classier for predicting diabetes status. The following table shows the actual response \\(Y\\) (1 = yes, 0 =n o) and fitted value \\(\\hat Y\\) using the classier for 10 test data points. A test data point is predicted to be \\(\\hat G\\)= 1 if \\(\\hat Y\\) &gt; δ, for a specied threshold value δ (Recall that we use δ = 0.5 in class, also known as the majority rule).\n\nWe define: \\(TPR = \\frac {TP}{TP +FN}\\), \\(FPR = \\frac {FP}{FP +TN}\\); For each of the thresholds δ = 0.3, 0.6 and 0.8, derive \\(TPR\\) and \\(FPR\\) in making predictions with the K-nearest neighbors classier for the 10 test data points. Plot \\(TPR\\) against \\(FPR\\) for the three thresholds.\n\n\ny &lt;- c(1,1,0,1,1,0,0,1,0,0)\nycap &lt;- c(0.9,0.5,0.7,0.4,0.5,0.2,0.7,0.9,0.1,0.1)\ntpr.all &lt;- numeric(0)\nfpr.all &lt;- numeric(0)\n\nsigma.list &lt;- c(0.3,0.6,0.8)\nfor (sigma in sigma.list) {\n  pred &lt;- ifelse(ycap &gt;= sigma, 1 ,0)\n  confusion.matrix &lt;- table(y, pred)\n  tpr &lt;- confusion.matrix[2,2]/sum(confusion.matrix[2,])\n  fpr &lt;- confusion.matrix[1,2]/sum(confusion.matrix[1,])\n  tpr.all &lt;- append(tpr.all, tpr)\n  fpr.all &lt;- append(fpr.all, fpr)\n}\n\nplot(tpr.all ~ fpr.all, pch = 20,\n     xlim = c(0,1),\n     ylim = c(0,1))\npoints(tpr.all[1] ~ fpr.all[1], pch = 15, col = \"red\")\npoints(tpr.all[2] ~ fpr.all[2], pch = 16, col = \"blue\")\npoints(tpr.all[3] ~ fpr.all[3], pch = 17, col = \"green\")\nlegend(0.7,0.4,legend=c(\"sigma = 0.3\", \"sigma = 0.6\", \"sigma = 0.8\"),\n       col=c(\"red\", \"blue\", \"green\"), pch=c(15,16,17))\n\n\n\n\n\n\n\n\n(b) Can we add the two points (0,0) and (1,1) to the plot of \\(TPR\\) against \\(FPR\\) in part (a). Explain why or why not.\n\n# If σ &gt; 0.9 then all test points have predicted ˆG = 0 (predicted as negative), \n# so TPR = FPR = 0.\n# If σ &lt; 0.1, then all test points have predicted ˆG = 1 (predicted as positive),\n# so TPR = FPR = 1.\n# Since there exist σ within the range from 0 to 1 for the two points to happen, \n# these two points can be added to the plot.\n\nThe CSV file Caravan.csv contains data on 5822 real customer records on caravan insurance purchase. This data set is owned and supplied by the Dutch data mining company, Sentient Machine Research, and is based on real world business data. Each record consists of 86 variables, containing sociodemographic data (variables 1-43) and product ownership (variables 44-86). Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy. For this business, assume that the overall error rate (equivalently, the accuracy) is not of interest. Instead, the company wants to use the classier to predict who are the potential customers likely to purchase insurance. Then the metric precision will be important, since it relates the proportion of Individuals who will actually purchase the insurance, among the group of individuals who are predicted to purchase insurance.\n\ncara &lt;- read.csv(\"Caravan.csv\")\nhead(cara, n = 2)\n\n  X MOSTYPE MAANTHUI MGEMOMV MGEMLEEF MOSHOOFD MGODRK MGODPR MGODOV MGODGE\n1 1      33        1       3        2        8      0      5      1      3\n2 2      37        1       2        2        8      1      4      1      4\n  MRELGE MRELSA MRELOV MFALLEEN MFGEKIND MFWEKIND MOPLHOOG MOPLMIDD MOPLLAAG\n1      7      0      2        1        2        6        1        2        7\n2      6      2      2        0        4        5        0        5        4\n  MBERHOOG MBERZELF MBERBOER MBERMIDD MBERARBG MBERARBO MSKA MSKB1 MSKB2 MSKC\n1        1        0        1        2        5        2    1     1     2    6\n2        0        0        0        5        0        4    0     2     3    5\n  MSKD MHHUUR MHKOOP MAUT1 MAUT2 MAUT0 MZFONDS MZPART MINKM30 MINK3045 MINK4575\n1    1      1      8     8     0     1       8      1       0        4        5\n2    0      2      7     7     1     2       6      3       2        0        5\n  MINK7512 MINK123M MINKGEM MKOOPKLA PWAPART PWABEDR PWALAND PPERSAUT PBESAUT\n1        0        0       4        3       0       0       0        6       0\n2        2        0       5        4       2       0       0        0       0\n  PMOTSCO PVRAAUT PAANHANG PTRACTOR PWERKT PBROM PLEVEN PPERSONG PGEZONG\n1       0       0        0        0      0     0      0        0       0\n2       0       0        0        0      0     0      0        0       0\n  PWAOREG PBRAND PZEILPL PPLEZIER PFIETS PINBOED PBYSTAND AWAPART AWABEDR\n1       0      5       0        0      0       0        0       0       0\n2       0      2       0        0      0       0        0       2       0\n  AWALAND APERSAUT ABESAUT AMOTSCO AVRAAUT AAANHANG ATRACTOR AWERKT ABROM\n1       0        1       0       0       0        0        0      0     0\n2       0        0       0       0       0        0        0      0     0\n  ALEVEN APERSONG AGEZONG AWAOREG ABRAND AZEILPL APLEZIER AFIETS AINBOED\n1      0        0       0       0      1       0        0      0       0\n2      0        0       0       0      1       0        0      0       0\n  ABYSTAND Purchase\n1        0       No\n2        0       No\n\nstr(cara)\n\n'data.frame':   5822 obs. of  87 variables:\n $ X       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ MOSTYPE : int  33 37 37 9 40 23 39 33 33 11 ...\n $ MAANTHUI: int  1 1 1 1 1 1 2 1 1 2 ...\n $ MGEMOMV : int  3 2 2 3 4 2 3 2 2 3 ...\n $ MGEMLEEF: int  2 2 2 3 2 1 2 3 4 3 ...\n $ MOSHOOFD: int  8 8 8 3 10 5 9 8 8 3 ...\n $ MGODRK  : int  0 1 0 2 1 0 2 0 0 3 ...\n $ MGODPR  : int  5 4 4 3 4 5 2 7 1 5 ...\n $ MGODOV  : int  1 1 2 2 1 0 0 0 3 0 ...\n $ MGODGE  : int  3 4 4 4 4 5 5 2 6 2 ...\n $ MRELGE  : int  7 6 3 5 7 0 7 7 6 7 ...\n $ MRELSA  : int  0 2 2 2 1 6 2 2 0 0 ...\n $ MRELOV  : int  2 2 4 2 2 3 0 0 3 2 ...\n $ MFALLEEN: int  1 0 4 2 2 3 0 0 3 2 ...\n $ MFGEKIND: int  2 4 4 3 4 5 3 5 3 2 ...\n $ MFWEKIND: int  6 5 2 4 4 2 6 4 3 6 ...\n $ MOPLHOOG: int  1 0 0 3 5 0 0 0 0 0 ...\n $ MOPLMIDD: int  2 5 5 4 4 5 4 3 1 4 ...\n $ MOPLLAAG: int  7 4 4 2 0 4 5 6 8 5 ...\n $ MBERHOOG: int  1 0 0 4 0 2 0 2 1 2 ...\n $ MBERZELF: int  0 0 0 0 5 0 0 0 1 0 ...\n $ MBERBOER: int  1 0 0 0 4 0 0 0 0 0 ...\n $ MBERMIDD: int  2 5 7 3 0 4 4 2 1 3 ...\n $ MBERARBG: int  5 0 0 1 0 2 1 5 8 3 ...\n $ MBERARBO: int  2 4 2 2 0 2 5 2 1 3 ...\n $ MSKA    : int  1 0 0 3 9 2 0 2 1 1 ...\n $ MSKB1   : int  1 2 5 2 0 2 1 1 1 2 ...\n $ MSKB2   : int  2 3 0 1 0 2 4 2 0 1 ...\n $ MSKC    : int  6 5 4 4 0 4 5 5 8 4 ...\n $ MSKD    : int  1 0 0 0 0 2 0 2 1 2 ...\n $ MHHUUR  : int  1 2 7 5 4 9 6 0 9 0 ...\n $ MHKOOP  : int  8 7 2 4 5 0 3 9 0 9 ...\n $ MAUT1   : int  8 7 7 9 6 5 8 4 5 6 ...\n $ MAUT2   : int  0 1 0 0 2 3 0 4 2 1 ...\n $ MAUT0   : int  1 2 2 0 1 3 1 2 3 2 ...\n $ MZFONDS : int  8 6 9 7 5 9 9 6 7 6 ...\n $ MZPART  : int  1 3 0 2 4 0 0 3 2 3 ...\n $ MINKM30 : int  0 2 4 1 0 5 4 2 7 2 ...\n $ MINK3045: int  4 0 5 5 0 2 3 5 2 3 ...\n $ MINK4575: int  5 5 0 3 9 3 3 3 1 3 ...\n $ MINK7512: int  0 2 0 0 0 0 0 0 0 1 ...\n $ MINK123M: int  0 0 0 0 0 0 0 0 0 0 ...\n $ MINKGEM : int  4 5 3 4 6 3 3 3 2 4 ...\n $ MKOOPKLA: int  3 4 4 4 3 3 5 3 3 7 ...\n $ PWAPART : int  0 2 2 0 0 0 0 0 0 2 ...\n $ PWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSAUT: int  6 0 6 6 0 6 6 0 5 0 ...\n $ PBESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PTRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBROM   : int  0 0 0 0 0 0 0 3 0 0 ...\n $ PLEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBRAND  : int  5 2 2 2 6 0 0 0 0 3 ...\n $ PZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PPLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ PFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PBYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAPART : int  0 2 1 0 0 0 0 0 0 1 ...\n $ AWABEDR : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWALAND : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSAUT: int  1 0 1 1 0 1 1 0 1 0 ...\n $ ABESAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AMOTSCO : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AVRAAUT : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AAANHANG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ ATRACTOR: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWERKT  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABROM   : int  0 0 0 0 0 0 0 1 0 0 ...\n $ ALEVEN  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APERSONG: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AGEZONG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AWAOREG : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABRAND  : int  1 1 1 1 1 0 0 0 0 1 ...\n $ AZEILPL : int  0 0 0 0 0 0 0 0 0 0 ...\n $ APLEZIER: int  0 0 0 0 0 0 0 0 0 0 ...\n $ AFIETS  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ AINBOED : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ABYSTAND: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Purchase: chr  \"No\" \"No\" \"No\" \"No\" ...\n\ndim(cara)\n\n[1] 5822   87\n\n\n(a) Without any classier, if the company tries to sell insurance to a random selection of customers, what is the success rate?\n\ntable(cara$Purchase)\n\n\n  No  Yes \n5474  348 \n\nsuccesRate &lt;- table(cara$Purchase)[2]/sum(table(cara$Purchase))\nsuccesRate\n\n       Yes \n0.05977327 \n\n# data set shows almost 6% of people purchased insurance\n\n(b) Standardize the input features. \\(Hint\\): Use scale() command in R.\n\ncara &lt;- cara[,-1] #throwing the first column since it provides no information\nscaled.X &lt;- scale(cara[,-86]) #throwing the response column\n\n(c) Randomly select 1000 observations to form the test data, and the remaining observations will be the training data.\n\nset.seed(5)\nn &lt;- dim(cara)[1] #train data number\nshuffled.index &lt;- sample(c(1:n), size = 1000)\nX.train &lt;- scaled.X[-shuffled.index,]\nX.test&lt;- scaled.X[shuffled.index,]\nY.train &lt;- as.factor(cara$Purchase[-shuffled.index])\nY.test &lt;- as.factor(cara$Purchase[shuffled.index])\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a customer will purchase insurance. Compute the precision of the classier.\n\nlibrary(class)\n\npred &lt;- knn(X.train, X.test, Y.train, k=1)\nconfusion.matrix &lt;- table(Y.test, pred) \n# Precision = TP/TP+FP\nprecision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\nconfusion.matrix\n\n      pred\nY.test  No Yes\n   No  892  55\n   Yes  44   9\n\nprecision\n\n[1] 0.140625\n\n\n(e) Repeat question 3d, for k-nearest neighbor classier where k = 3,5,10. Which value of k gives the best precision?\n\nprecision.all &lt;- data.frame()\nk.list &lt;- c(3, 5, 10)\nfor (k in k.list) {\n  pred &lt;- knn(X.train, X.test, Y.train, k = k)\n  confusion.matrix &lt;- table(Y.test, pred) \n  # Precision = TP/TP+FP\n  precision &lt;- confusion.matrix[2,2]/sum(confusion.matrix[,2])\n  precision.all &lt;- rbind(precision.all, data.frame(k, precision))\n}\n\nprecision.all\n\n   k  precision\n1  3 0.05263158\n2  5 0.14285714\n3 10        NaN\n\n# So far, k = 5 gives the best precision.\n# However, one might use N -fold cross validation \n# to have the average precision for each k.\n# With that, the value of k that gives largest average precision is chosen.\n\n\n\n\nOnsite Questions (KNN Mathematics)\n\nThe k-nearest neighbor classifier\n\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable, y.\n\n\n\nObs\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\nY\n\n\n\n\n1\n0\n3\n0\nRed\n\n\n2\n2\n0\n0\nRed\n\n\n3\n0\n1\n3\nRed\n\n\n4\n0\n1\n2\nGreen\n\n\n5\n-1\n0\n1\nGreen\n\n\n6\n1\n1\n1\nRed\n\n\n\nSuppose we wish to use this data set to make a prediction for a new observation with response Y given that it has \\(X_1 = X_2 = X_3 = 0\\) using K-nearest neighbors.\n(a) Compute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\)\n\nX1 &lt;- c(0, 2, 0, 0, -1, 1)\nX2 &lt;- c(3, 0, 1, 1, 0, 1)\nX3 &lt;- c(0, 0, 3, 2, 1, 1)\nY &lt;- c(\"Red\", \"Red\", \"Red\", \"Green\", \"Green\", \"Red\")\ndataframe &lt;- data.frame(X1, X2, X3, Y)\n\ndataframe$distance &lt;- \n    sqrt(\n        dataframe$X1^2 + \n        dataframe$X2^2 + \n        dataframe$X3^2\n    )\n\ndataframe &lt;- dataframe[order(dataframe$distance),]\n\n(b) What is our prediction with K=1? Why?\n\n# Prediction for K = 1 is \n# Green as the one nearest \n# point is also Green. Thus,\n# by majority vote, KNN \n# classifies the new data\n# point as Green.\n\n\n(c) What is our prediction with K=3? Why?\n\n# Prediction for K = 3 is \n# Red as 2/3 of the nearest\n# points are Red, with one \n# Green point. Thus,\n# by majority vote, KNN \n# classifies the new data\n# point as Red.\n\n\n(d) If the Bayes decision boundary (the gold standard decision boundary) in this problem is highly non-linear, then would we expect the best value for KKK to be large or small? Why?\n\n# We would expect the best\n# value of K to be small, \n# since it translates to \n# a more flexible \n# classification method."
  },
  {
    "objectID": "index.html#t6-decision-trees",
    "href": "index.html#t6-decision-trees",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T6 (Decision Trees)",
    "text": "T6 (Decision Trees)\n\n# fit &lt;- rpart(...)\n\n# Predict using decision trees\n#predict(fit, newdata, type = \"class\") # Classification\n#predict(fit, newdata, type = \"prob\") # Raw probability\n# **No need to remove features\n\n\nOffsite Questions (KNN Tuning, N-fold CV, Decision Trees)\n\n(KNN and N -fold Cross Validation)\nLoan managers often need to take into account an applicant’s demographic and socioeconomic proles in deciding whether to approve a loan to the applicant, to minimize losses due to defaults. In this exercise we will build and evaluate a classier based on the German Credit Data to predict whether an applicant is considered as having good or bad credit risk. The features or predictors include (1) loan duration (in months), (2) credit amount, (3) Installment rate in percentage of disposable income and (4) age in years.\n(a) Read and explore the data from the file German_credit.csv.\n\ndata &lt;- read.csv(\"German_credit.csv\")\nhead(data)\n\n  Creditability Duration Amount Instalment Age\n1             1       18   1049          4  21\n2             1        9   2799          2  36\n3             1       12    841          2  23\n4             1       12   2122          3  39\n5             1       12   2171          4  38\n6             1       10   2241          1  48\n\nstr(data)\n\n'data.frame':   1000 obs. of  5 variables:\n $ Creditability: int  1 1 1 1 1 1 1 1 1 1 ...\n $ Duration     : int  18 9 12 12 12 10 8 6 18 24 ...\n $ Amount       : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n $ Instalment   : int  4 2 2 3 4 1 1 2 4 1 ...\n $ Age          : int  21 36 23 39 38 48 39 40 65 23 ...\n\n\n(b) Standardize the input features.\n\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\nhead(X)\n\n       Duration     Amount  Instalment         Age\n[1,] -0.2407368 -0.7872630  0.91801781 -1.28093214\n[2,] -0.9870788 -0.1673006 -0.86974813  0.04034293\n[3,] -0.7382981 -0.8609500 -0.86974813 -1.10476213\n[4,] -0.7382981 -0.4071375  0.02413484  0.30459795\n[5,] -0.7382981 -0.3897785  0.91801781  0.21651294\n[6,] -0.9041519 -0.3649800 -1.76363111  1.09736299\n\n\n(c) Randomly select 800 customer records to form the training data, and the remaining 200 records will be the test data.\n\nset.seed(100)\nn &lt;- dim(data)[1]\nindex &lt;- sample(1:n, 800)\n\ntraining.X &lt;- X[index, ]\ntest.X &lt;- X[-index, ]\ntraining.Y &lt;- Y[index]\ntest.Y &lt;- Y[-index]\n\n(d) Use 1-nearest neighbor classier for the training data to predict if a loan applicant is credible for the 200 test points. Compute the accuracy of the classier.\n\nlibrary(class)\npred &lt;- knn(training.X, test.X, training.Y, k=1)\nconfusion.matrix &lt;- table(test.Y, pred)\nconfusion.matrix\n\n      pred\ntest.Y  0  1\n     0 24 35\n     1 47 94\n\naccuracy &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\naccuracy\n\n[1] 0.59\n\n\n(e) Use N -folds cross validation with N = 5 to find the average accuracy for the 1-nearest neighbor classifier.\n\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German_credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\n\naccuracy &lt;- numeric(n_folds)\nfor (j in 1:n_folds) {\n  test_j &lt;- which(folds_j == j) \n  test.y &lt;- Y[test_j] \n  train.X &lt;- X[-test_j, ]  \n  test.X &lt;- X[test_j, ]  \n  train.y &lt;- Y[-test_j] \n\n  knn.pred &lt;- knn(train.X, test.X, train.y, k = 1) \n\n  confusion.matrix &lt;- table(test.y, knn.pred)\n  accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n}\n\nave.accuracy &lt;- round(mean(accuracy), digits = 3)\nave.accuracy\n\n[1] 0.622\n\n\n(f) Repeat question 1e for K-nearest neighbor classifiers where K = 1, 2, …100.\n\n### FULL KNN WITH N-FOLDS VALIDATION\nlibrary(class)\nset.seed(100)\n\ndata &lt;- read.csv(\"German_credit.csv\")\nX &lt;- scale(data[-1])\nY &lt;- data$Creditability\n\nk &lt;- 100\nn_folds &lt;- 5\nn &lt;- length(Y)\nfolds_j &lt;- sample(rep(1:n_folds, length.out = n ))\nave.accuracy &lt;- numeric(length(k))\n\nfor (i in 1:k) {\n  accuracy &lt;- numeric(n_folds)\n  for (j in 1:n_folds) {\n    test_j &lt;- which(folds_j == j) \n    test.y &lt;- Y[test_j] \n    train.X &lt;- X[-test_j, ]  \n    test.X &lt;- X[test_j, ]  \n    train.y &lt;- Y[-test_j] \n\n    knn.pred &lt;- knn(train.X, test.X, train.y, k = i) \n\n    confusion.matrix=table(test.y, knn.pred)\n    accuracy[j] &lt;- sum(diag(confusion.matrix))/sum(confusion.matrix)\n  }\n  ave.accuracy[i] &lt;- round(mean(accuracy), digits = 3)\n}\n\nave.accuracy\n\n  [1] 0.622 0.597 0.661 0.674 0.684 0.671 0.676 0.686 0.686 0.691 0.702 0.699\n [13] 0.705 0.702 0.707 0.695 0.699 0.702 0.700 0.693 0.695 0.695 0.697 0.695\n [25] 0.699 0.697 0.696 0.700 0.693 0.688 0.697 0.692 0.697 0.698 0.697 0.692\n [37] 0.695 0.695 0.695 0.696 0.696 0.701 0.699 0.701 0.699 0.699 0.699 0.698\n [49] 0.699 0.696 0.698 0.699 0.699 0.696 0.695 0.699 0.696 0.697 0.696 0.698\n [61] 0.697 0.696 0.699 0.697 0.697 0.697 0.697 0.696 0.698 0.699 0.699 0.696\n [73] 0.698 0.697 0.698 0.696 0.698 0.698 0.698 0.699 0.699 0.698 0.698 0.700\n [85] 0.698 0.698 0.700 0.700 0.699 0.700 0.700 0.701 0.701 0.701 0.700 0.700\n [97] 0.699 0.698 0.699 0.699\n\n\n(g) Compare the 100 classifiers above, which few values of K give the best average accuracy?\n\nlibrary(glue)\nindex = which(ave.accuracy == max(ave.accuracy))\nplot(x = 1:k, ave.accuracy)\nabline(v = index, col = \"red\")\n\n\n\n\n\n\n\nglue(\"most accurate k = {index} with average accuracy = {ave.accuracy[index]}\")\n\nmost accurate k = 15 with average accuracy = 0.707\n\n\n(Decision Trees)\nConsider the famous Iris Flower Data set which was rst introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation: the length and the width of the sepals and petals (in cm).\n(a) Use decision tree method to predict Iris species based on all four features.\n\niris &lt;- read.csv(\"iris.csv\")\nhead(iris)\n\n  sepal.length sepal.width petal.length petal.width       class\n1          5.1         3.5          1.4         0.2 Iris-setosa\n2          4.9         3.0          1.4         0.2 Iris-setosa\n3          4.7         3.2          1.3         0.2 Iris-setosa\n4          4.6         3.1          1.5         0.2 Iris-setosa\n5          5.0         3.6          1.4         0.2 Iris-setosa\n6          5.4         3.9          1.7         0.4 Iris-setosa\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ sepal.length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ sepal.width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ petal.length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal.width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\ndim(iris)\n\n[1] 150   5\n\nlibrary(rpart)\nfit &lt;- rpart(class ~ sepal.length + sepal.width + petal.length + petal.width,\n              data = iris,\n              method=\"class\",\n              parms=list(split='information'),\n              control = rpart.control( minsplit =1))\n\n(b) Visualize the decision tree above, using the rpart.plot function.\n\nlibrary(rpart.plot)\nrpart.plot(fit, type=4, extra=2)\n\n\n\n\n\n\n\n# The fitted tree is given in the figure below.\n# If the measurement of petal length is less than 2.5 cm \n# then the flower is of Iris-setosa\n# If the petal length is ≥ 2.5 cm with the petal width is ≥ 1.8 cm \n# then high chance (45/46) it will be an Iris-virginica.\n# If the petal length is ≥ 2.5 cm with the petal width is &lt; 1.8 cm\n# then we continue to check if the petal length is in the interval\n# [2.5, 5]. If yes, then high chance (47/48) it is Iris-versicolor.\n\n(c) What are the more important features in the fitted tree above?\n\n# It seems the sepal length and sepal width \n# are not important in the classification while\n# the petal length and petal width are more important."
  },
  {
    "objectID": "index.html#t7-decision-trees-2",
    "href": "index.html#t7-decision-trees-2",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T7 (Decision Trees 2)",
    "text": "T7 (Decision Trees 2)\n\nOffsite Questions (Decision Trees, N-fold CV)\n(DT and N-fold Cross Validation)\nConsider the famous Iris Flower Data set which was first introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor).\nFour features were measured from each observation: the length and the width of the sepals and petals (in cm).\n\nIn Tutorial 6, we used the decision tree method to predict Iris species based on all four features. We now would want to use N-fold CV to check on how good the method is, based on the accuracy.\nWe’ll use 5-fold CV where we would want to keep the ratio of the three species the same (1:1:1) in both the training set and test set.\nWhat’s the average accuracy of the decision tree method?\n\ndf &lt;- read.csv(\"iris.csv\")\nstr(df)\n\n'data.frame':   150 obs. of  5 variables:\n $ sepal.length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ sepal.width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ petal.length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal.width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\ntable(df$class)\n\n\n    Iris-setosa Iris-versicolor  Iris-virginica \n             50              50              50 \n\n\n\nset.seed(1101)\nn_folds &lt;- 5\nfolds_indexes_1 &lt;- sample(rep(1:n_folds, length.out = 50))\nprint(table(folds_indexes_1))\n\nfolds_indexes_1\n 1  2  3  4  5 \n10 10 10 10 10 \n\nfolds_indexes_2 &lt;- sample(rep(1:n_folds, length.out = 50))\nprint(table(folds_indexes_2))\n\nfolds_indexes_2\n 1  2  3  4  5 \n10 10 10 10 10 \n\nfolds_indexes_3 &lt;- sample(rep(1:n_folds, length.out = 50))\nprint(table(folds_indexes_3))\n\nfolds_indexes_3\n 1  2  3  4  5 \n10 10 10 10 10 \n\n\n\nfolds_indexes = c(folds_indexes_1, folds_indexes_2, folds_indexes_3)\nprint(table(folds_indexes))\n\nfolds_indexes\n 1  2  3  4  5 \n30 30 30 30 30 \n\nprint(folds_indexes)\n\n  [1] 3 5 3 2 3 1 5 4 2 2 1 3 5 1 4 5 2 3 4 4 5 3 5 1 4 1 1 4 4 2 2 5 1 2 3 2 3\n [38] 5 1 4 1 3 5 2 1 2 4 5 3 4 1 5 2 1 3 3 4 5 3 2 3 1 1 5 5 1 1 5 3 5 1 4 2 4\n [75] 3 3 1 4 1 4 4 2 2 2 5 1 2 4 5 3 4 4 3 2 5 2 2 5 4 3 5 5 2 4 2 2 5 4 3 3 1\n[112] 1 5 4 4 5 3 3 3 2 4 1 1 2 1 3 4 2 1 5 4 1 1 5 4 5 3 5 1 2 2 2 4 1 2 4 3 3\n[149] 5 3\n\naccuracy &lt;- numeric(n_folds)\nfor (i in 1:n_folds) {\n  test_indexes &lt;- which(i == folds_indexes)\n  fit &lt;- rpart(class ~ ., method = \"class\",\n               data &lt;- df[-test_indexes,])\n  prediction = predict(fit, df[test_indexes,],\n                       type = \"class\")\n  accuracy[i] &lt;- mean(prediction == df$class[test_indexes])\n}\n\nprint(accuracy)\n\n[1] 0.9666667 0.9000000 1.0000000 0.9333333 0.9000000\n\nprint(mean(accuracy))\n\n[1] 0.94\n\n# The average accuracy of the decision tree method is 0.94\n\n\n\nOnsite Questions (Decision Trees Tuning, N-fold CV)\n(Decision Trees)\nCustomer churn is the loss of clients or customers. Banks, telephone service companies, Internet service providers, pay TV companies, and insurance firms often use customer churn analysis and customer churn rates as one of their key business metrics.\nThis is because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\nIn this problem, a wireless telecommunications company wants to predict whether a customer will churn (switch to a different company) in the next six months. With a reasonably accurate prediction of a person’s churning, the sales and marketing groups can attempt to retain the customer by offering various incentives. Variables of our concern are listed below.\n(i) Age (years)\n(ii) Married (true/false)\n(iii) Duration as a customer (years)\n(iv) Churned contacts - Number of the customer’s contacts that have churned (count)\n(v) Churned (true/false) — Whether the customer churned\n(a) Build a decision tree for predicting customer churn, using the feature variables Age, Married, Cust_years, and Churned_contacts.\n\nset.seed(100)\ndf &lt;- read.csv(\"churn.csv\")\nstr(df)\n\n'data.frame':   8000 obs. of  6 variables:\n $ ID              : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Churned         : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Age             : int  61 50 47 50 29 43 50 29 32 48 ...\n $ Married         : int  1 1 1 1 1 1 0 0 1 1 ...\n $ Cust_years      : int  3 3 2 3 1 4 2 3 3 4 ...\n $ Churned_contacts: int  1 2 0 3 3 3 1 2 3 0 ...\n\ndf$Churned &lt;- as.factor(df$Churned)\ndf$Married &lt;- as.factor(df$Married)\n\n\nlibrary(rpart)\nfit &lt;- rpart(Churned ~ Age + Married + Cust_years + Churned_contacts, \n             data = df,\n             method = \"class\",\n             parms = list(split = 'information'),\n             control = rpart.control(minsplit = 1))\n\nlibrary(rpart.plot)\nrpart.plot(fit, type=4, extra=2)\n\n\n\n\n\n\n\n\n(b) Consider the decision tree in part (a) to predict binary variable Churned. Use the tree to predict customer churn for the following observations.\n\n\nindex &lt;- c(2821, 96, 5085, 758, 487, 987, 6061, 3745, 4709, 2769)\ndf[index,]\n\n       ID Churned Age Married Cust_years Churned_contacts\n2821 2821       0  26       1          2                2\n96     96       1  23       1          3                3\n5085 5085       0  56       1          5                2\n758   758       0  36       1          5                2\n487   487       0  45       0          2                1\n987   987       0  28       0          2                2\n6061 6061       0  22       1          3                0\n3745 3745       0  22       0          3                2\n4709 4709       0  60       1          2                1\n2769 2769       0  32       0          3                1\n\npred &lt;- predict(fit, newdata = df[index,], type = \"class\")\npred\n\n2821   96 5085  758  487  987 6061 3745 4709 2769 \n   1    1    0    0    0    1    1    1    0    0 \nLevels: 0 1\n\nconfusion_matrix &lt;- table(actual = df$Churned[index], predicted = pred)\nconfusion_matrix\n\n      predicted\nactual 0 1\n     0 5 4\n     1 0 1\n\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\naccuracy\n\n[1] 0.6\n\n\nRecall that we studied N-fold cross-validation for the K-nearest neighbor classifier, in which the value of k is varied to control the complexity of the decision surface for the classifier.\nFor decision tree classification, when fitting a tree using function rpart(), we use the argument control = rpart.control(minsplit = 1) where minsplit = 1 is to specify the minimum number of observations that must exist in a node in order for a split to be attempted. By default, minsplit = 20. This minsplit argument helps to draft the complexity of a tree, complex with many layers and branches or simple with few layers and less branches.\nFor this control = rpart.control(), there is a similar complexity parameter exists, which is denoted as cp where by default cp = 0.01:\ncontrol = rpart.control(cp = 0.01)\nHeuristically, smaller values of cp correspond to decision trees of larger sizes, and hence more complex decision surfaces.\nFor this problem, we will investigate n-fold cross-validation for a decision tree classifier.\nConsider the data set “bank-sample.csv” we discussed in the lectures. For this exercise, we will fit a decision tree with subscribed as outcome; and job, marital, education, default, housing, loan, contact, and poutcome as 8 feature variables. We want to find the best cp value in terms of mis-classification error rate.\n\nRandomly split the entire data set into 10 mutually exclusive data sets.\nLet cp take on the values \\(10^k\\) for k=−5,−4,…,0,…,3,4,5.\nAt each cp value, run the following loop for j=1,2,…,10.\n\ni. Set the jth group to be the test set.\nii. Fit a decision tree on the other 9 sets with the value of cp.\niii. Predict the class assignment of subscribed for each observation of the test set.\niv. Calculate the number of mis-classification(s) by comparing predicted versus actual class labels in the test set.\n\nDetermine the best cp value in terms of mis-classification error rate.\n\nNote: mis-classification error rate is the complement of the accuracy.\n\ndf &lt;- read.csv(\"bank-sample.csv\")\nstr(df)\n\n'data.frame':   2000 obs. of  17 variables:\n $ age       : int  31 45 46 35 39 31 34 39 38 50 ...\n $ job       : chr  \"management\" \"entrepreneur\" \"services\" \"management\" ...\n $ marital   : chr  \"single\" \"married\" \"divorced\" \"married\" ...\n $ education : chr  \"tertiary\" \"tertiary\" \"secondary\" \"tertiary\" ...\n $ default   : chr  \"no\" \"no\" \"no\" \"no\" ...\n $ balance   : int  0 1752 4329 1108 1410 499 0 26233 8444 72 ...\n $ housing   : chr  \"yes\" \"yes\" \"no\" \"yes\" ...\n $ loan      : chr  \"no\" \"yes\" \"no\" \"no\" ...\n $ contact   : chr  \"cellular\" \"cellular\" \"cellular\" \"cellular\" ...\n $ day       : int  15 20 21 17 23 9 6 2 17 22 ...\n $ month     : chr  \"apr\" \"nov\" \"nov\" \"nov\" ...\n $ duration  : int  185 56 534 52 55 122 611 462 39 171 ...\n $ campaign  : int  2 2 2 1 1 2 1 1 2 13 ...\n $ pdays     : int  -1 -1 -1 -1 -1 -1 183 -1 324 -1 ...\n $ previous  : int  0 0 0 0 0 0 1 0 1 0 ...\n $ poutcome  : chr  \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ subscribed: chr  \"no\" \"no\" \"yes\" \"no\" ...\n\ncol_names &lt;- c(\"subscribed\", \"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"poutcome\")\ndf_new &lt;- df[,col_names]\nstr(df_new)\n\n'data.frame':   2000 obs. of  9 variables:\n $ subscribed: chr  \"no\" \"no\" \"yes\" \"no\" ...\n $ job       : chr  \"management\" \"entrepreneur\" \"services\" \"management\" ...\n $ marital   : chr  \"single\" \"married\" \"divorced\" \"married\" ...\n $ education : chr  \"tertiary\" \"tertiary\" \"secondary\" \"tertiary\" ...\n $ default   : chr  \"no\" \"no\" \"no\" \"no\" ...\n $ housing   : chr  \"yes\" \"yes\" \"no\" \"yes\" ...\n $ loan      : chr  \"no\" \"yes\" \"no\" \"no\" ...\n $ contact   : chr  \"cellular\" \"cellular\" \"cellular\" \"cellular\" ...\n $ poutcome  : chr  \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n\n\n\nset.seed(100)\ncp_values &lt;- 10^(-5:5)\nprint(cp_values)\n\n [1] 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05\n\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\nindex &lt;- 1\ncp_err &lt;- numeric(11)\nn_folds &lt;- 10\nfolds_indexes &lt;- sample(rep(1:n_folds, length.out = dim(df_new)[1])) #nrow(df_new) the same \nfor (cp_val in cp_values) {\n  err &lt;- numeric(10)\n  for (j in 1:10) {\n    test_indexes &lt;- which(folds_indexes == j)\n    fit &lt;- rpart(subscribed ~ .,\n                 method = \"class\",\n                 data = df_new[-test_indexes,],\n                 control = rpart.control(cp = cp_val))\n    prediction &lt;- predict(fit,\n                          df_new[test_indexes,],\n                          type = \"class\")\n    err[j] &lt;- mean(prediction != df_new$subscribed[test_indexes])\n  }\n  cp_err[index] &lt;- mean(err)\n  index &lt;- index + 1\n}\n\n\nplot(log(cp_values, base = 10), cp_err, type = 'b')\n\n\n\n\n\n\n\nbest.cp &lt;- cp_values[which(cp_err == min(cp_err))]\nprint(best.cp)\n\n[1] 0.01\n\n# From the plot, we can observe that cp = 0.01 gives the lowest misclassification error rate.\n\n\nfit &lt;- rpart(subscribed ~ ., \n             method = \"class\", \n             data = df_new, \n             control = rpart.control(cp = 0.01))\nrpart.plot(fit, type = 4, extra = 2, clip.right.labs = FALSE, varlen = 0, faclen = 0)"
  },
  {
    "objectID": "index.html#t8-naive-bayes",
    "href": "index.html#t8-naive-bayes",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T8 (Naive Bayes)",
    "text": "T8 (Naive Bayes)\n\nOffsite Questions (Naive Bayes, Decision Trees, ROC, AUC)\nConsider the data set “Titanic.csv” again.\n(a) Fit a decision tree on all the three feature variables, called M2, which uses minsplit = 1 and information gain.\n(b) Plot the tree M2.\n(c) Plot the ROC curves and derive the AUC values for the two classifiers (naive Bayes from on-site question and decision tree). Which classifier has a larger AUC value?\n\ndf &lt;- read.csv(\"Titanic.csv\")\nstr(df)\n\n'data.frame':   2201 obs. of  4 variables:\n $ Class   : chr  \"3rd\" \"3rd\" \"3rd\" \"3rd\" ...\n $ Sex     : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ Age     : chr  \"Child\" \"Child\" \"Child\" \"Child\" ...\n $ Survived: chr  \"No\" \"No\" \"No\" \"No\" ...\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(e1071)\nlibrary(ROCR)\n\nset.seed(100)\nM1 &lt;- naiveBayes(Survived ~ Class + Sex + Age, df)\nM2 &lt;- rpart(Survived ~ Class + Sex + Age, \n            data = df,\n            method = \"class\",\n            control = rpart.control(minsplit = 1),\n            parms = list(split = 'information'))\nrpart.plot(M2, type = 4, extra = 2, clip.right.labs = FALSE, varlen = 0, faclen = 0)\n\n\n\n\n\n\n\n# Plot the ROC curves and derive the AUC values for the two classifiers (naive Bayes from on-site question and decision tree). Which classifier has a larger AUC value?\n\npred.M1 &lt;- predict(M1, df, type = 'raw')\nscore &lt;- pred.M1[,2]\nsurvived_numeric &lt;- ifelse(df$Survived == \"Yes\", 1, 0)\npred_nb &lt;- prediction(score, survived_numeric)\nroc_nb &lt;- performance(pred_nb, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc_nb, col = \"red\")\n\n\n\n\n\n\n\npred.M2 &lt;- predict(M2, df, type = 'prob')\nscore2 &lt;- pred.M2[,2]\npred_dt &lt;- prediction(score2, survived_numeric)\nroc_dt &lt;- performance(pred_dt, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc_dt, col = \"blue\")\n\n\n\n\n\n\n\nplot(roc_nb, col = \"red\", lwd = 2, main = \"ROC Curves: Naive Bayes vs Decision Tree\")\n# Add ROC curve for Decision Tree\nplot(roc_dt, col = \"blue\", lwd = 2, add = TRUE)\nlegend(x = \"bottomright\", \n       legend = c(\"Naive Bayes\", \"Decision Tree\"), \n       col = c(\"red\", \"blue\"), \n       lwd = 2,\n       lty = 1)\n\n\n\n\n\n\n\nauc_nb &lt;- performance(pred_nb, measure = \"auc\")\nauc_nb_value &lt;- as.numeric(auc_nb@y.values) # Extract the AUC value\nprint(auc_nb_value)\n\n[1] 0.7164944\n\n# 0.7164944\n\n\nauc_dt &lt;- performance(pred_dt, measure = \"auc\")\nauc_dt_value &lt;- as.numeric(auc_dt@y.values) # Extract the AUC value\nprint(auc_dt_value)\n\n[1] 0.7262628\n\n# 0.7262628\n\n# From AUC values, the decision tree classifier is\n# slightly better than the Naive Bayes classifier\n\n\n\nOnsite Questions (Naive Bayes Mathematics)\nData set “Titanic.csv” provides information on the fate of passengers on the fatal maiden voyage of the ocean liner Titanic. It includes the variables: economic status (class), sex, age, and survival. We will train a naive Bayes classifier using this data set, and predict survival.\n\nCompute the probabilities \\(P(Y=1)\\) (survived) and \\(P(Y=0)\\) (did not survive).\nCompute the conditional probabilities \\(P(X_i = x_i | Y = 1)\\) and \\(P(X_i = x_i | Y = 0)\\), where i=1,2,3,4 for the feature variables X={class,sex,age}\nPredict survival for an adult female passenger in 2nd class cabin.\nCompare your prediction in (c) with the one performed by the naiveBayes() function in package e1071\n\n\ndf &lt;- read.csv(\"Titanic.csv\")\nstr(df)\n\n'data.frame':   2201 obs. of  4 variables:\n $ Class   : chr  \"3rd\" \"3rd\" \"3rd\" \"3rd\" ...\n $ Sex     : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ Age     : chr  \"Child\" \"Child\" \"Child\" \"Child\" ...\n $ Survived: chr  \"No\" \"No\" \"No\" \"No\" ...\n\nprop.table(table(df$Survived))\n\n\n      No      Yes \n0.676965 0.323035 \n\n\n\ntab &lt;- table(df[,c(\"Survived\", \"Class\")])\nprint(tab)\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  122 167 528  673\n     Yes 203 118 178  212\n\nclass.pro &lt;- prop.table(tab, margin = 1)\nprint(class.pro)\n\n        Class\nSurvived        1st        2nd        3rd       Crew\n     No  0.08187919 0.11208054 0.35436242 0.45167785\n     Yes 0.28551336 0.16596343 0.25035162 0.29817159\n\n# P (Class = 1st | Y = 1) = 0.286\n# P (Class = 2nd | Y = 1) = 0.166\n# P (Class = 3rd | Y = 1) = 0.250\n# P (Class = Crew | Y = 1) = 0.298\n\n# P (Class = 1st | Y = 0) = 0.0819\n# P (Class = 2nd | Y = 0) = 0.112\n# P (Class = 3rd | Y = 0) = 0.354\n# P (Class = Crew | Y = 0) = 0.452\n\n\ntab &lt;- table(df[,c(\"Survived\", \"Sex\")])\nprint(tab)\n\n        Sex\nSurvived Female Male\n     No     126 1364\n     Yes    344  367\n\nsex.pro &lt;- prop.table(tab, margin = 1)\nprint(sex.pro)\n\n        Sex\nSurvived     Female       Male\n     No  0.08456376 0.91543624\n     Yes 0.48382560 0.51617440\n\n# P (Class = Female | Y = 1) = 0.484\n# P (Class = Male | Y = 1) = 0.516\n\n# P (Class = Female | Y = 0) = 0.085\n# P (Class = Male | Y = 0) = 0.915\n\n\ntab &lt;- table(df[,c(\"Survived\", \"Age\")])\nprint(tab)\n\n        Age\nSurvived Adult Child\n     No   1438    52\n     Yes   654    57\n\nage.pro &lt;- prop.table(tab, margin = 1)\nprint(age.pro)\n\n        Age\nSurvived      Adult      Child\n     No  0.96510067 0.03489933\n     Yes 0.91983122 0.08016878\n\n# P (Class = Adult | Y = 1) = 0.920\n# P (Class = Child | Y = 1) = 0.080\n\n# P (Class = Adult | Y = 0) = 0.965\n# P (Class = Child | Y = 0) = 0.035\n\n\n# Predict survival for an adult female passenger in 2nd class cabin.\n# P(Y = 1) *.                   # Survival \n# P(Class = 2nd|Y = 1) *        # 2nd Class\n# P(Sex = Female|Y = 1) *.      # Female\n# P(Age = Adult|Y = 1)          # Adult\n# = 0.323035 * 0.16596343 * 0.48382560 * 0.91983122\n# = 0.02385937\n\n# P(Y = 0) *                    # Survival \n# P(Class = 2nd|Y = 0) *        # 2nd Class\n# P(Sex = Female|Y = 0) *       # Female\n# P(Age = Adult|Y = 0)          # Adult\n# = 0.676965 * 0.11208054 * 0.08456376 * 0.96510067\n# = 0.006192319\n\n# Thus, the ratio between probability of survived \n# and not survived is 0.02385937/0.006192319 (3.85)\n# which is nearly 4 times. \n\n# We predict that an adult female passenger in 2nd class\n# cabin will survive.\n\n\n# 4. Compare your prediction in (c) with naiveBayes() function\nlibrary(e1071)\nM1 &lt;- naiveBayes(Survived ~ Class + Sex + Age, df)\nnewdata &lt;- data.frame(Class = \"2nd\", Sex = \"Female\", Age = \"Adult\")\n\npred.label &lt;- predict(M1, newdata)\nprint(pred.label)\n\n[1] Yes\nLevels: No Yes\n\npred.prob &lt;- predict(M1, newdata, \"raw\")\nprint(pred.prob)\n\n            No       Yes\n[1,] 0.2060556 0.7939444\n\nprint(pred.prob[1,\"Yes\"] / pred.prob[1,\"No\"])\n\n     Yes \n3.853059 \n\n# Ratio is same as calculated."
  },
  {
    "objectID": "index.html#t9-logistic-regression",
    "href": "index.html#t9-logistic-regression",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T9 (Logistic Regression)",
    "text": "T9 (Logistic Regression)\n\n# M1 = glm(y ~ x1 + x2 + ..., data = ..., family = binomial)\n\n# predict(M1, newdata = ..., type = \"response\")\n#    Get probability\n# predict(M1, newdata = ..., type = \"link\")\n#    Get Log-odds (Rarely used)\n\n# Coefficient of numerical features:\n# If other regressors are fixed, when X increases by one\n# unit, the LOG-odds of Y increases/decreases by C units.\n# Thus, odds of Y will be e^C = ... TIMES larger/smaller.\n\n\nOffsite Questions (Logistic Regression, ROC, AUC)\nConsider data set Titanic.csv again which was mentioned in Tutorial 8.\n\nPerform logistic regression using all the feature variables to predict the survival status, called model M2.\n\ndf &lt;- read.csv(\"Titanic.csv\")\nstr(df)\n\n'data.frame':   2201 obs. of  4 variables:\n $ Class   : chr  \"3rd\" \"3rd\" \"3rd\" \"3rd\" ...\n $ Sex     : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ Age     : chr  \"Child\" \"Child\" \"Child\" \"Child\" ...\n $ Survived: chr  \"No\" \"No\" \"No\" \"No\" ...\n\ndf$survived_numeric &lt;- ifelse(df$Survived == \"No\", 0, 1)\n\nM2 &lt;- glm(survived_numeric ~ Class + Sex + Age, data = df, family = binomial)\nsummary(M2)\n\n\nCall:\nglm(formula = survived_numeric ~ Class + Sex + Age, family = binomial, \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.0438     0.1679  12.171  &lt; 2e-16 ***\nClass2nd     -1.0181     0.1960  -5.194 2.05e-07 ***\nClass3rd     -1.7778     0.1716 -10.362  &lt; 2e-16 ***\nClassCrew    -0.8577     0.1573  -5.451 5.00e-08 ***\nSexMale      -2.4201     0.1404 -17.236  &lt; 2e-16 ***\nAgeChild      1.0615     0.2440   4.350 1.36e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2769.5  on 2200  degrees of freedom\nResidual deviance: 2210.1  on 2195  degrees of freedom\nAIC: 2222.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nWrite down the fitted equation of model M2.\n\n# p_hat = predicted probability of survival\n# log(p_hat/(1-p_hat)) = 2.0438 - 1.0181 * I(Class=2nd) - 1.7778 * I(Class = 3rd) \n# - 0.8577 * I(Class = Crew) - 2.4201 * I(Sex = Male) + 1.0615 * I(Age = Child)\n\nInterpret the coefficient of the variable ‘Sex’ in M2.\n\n# Female is the reference, Male is indicated by indicator.\n# Meaning, given the same condition on class and age, when \n# comparing to a female, the LOG-odds of survival for a \n# male is less than that of a female by 2.4201.\n# Thus, odds of survival of a male passenger will be less\n# than that of a female by e^2.4201 = 11.25 TIMES\n\nInterpret the coefficient of the variable ‘Age’ in M2.\n\n# Adult is the reference, Child is indicated by indicator.\n# Meaning, given the same condition on class and sex, when # comparing to a adult, the LOG-odds of survival for a \n# child is higher than that of an adult by 1.0615.\n# Thus, odds of survival of a child passenger will be more\n# than that of an adult by e^1.0615 = 2.89 TIMES\n\nObtain and compare the ROC curves and AUC for the two classifiers: naive Bayes (from Tutorial 8) and logistic regression.\n\nlibrary('e1071')\nM1 &lt;- naiveBayes(survived_numeric ~ Class + Sex + Age, data = df)\nM2 &lt;- glm(survived_numeric ~ Class + Sex + Age, data = df, family = binomial)\n\npred.M2 &lt;- predict(M2, newdata = df, type = 'response')\npred_lr &lt;- prediction(pred.M2, df$survived_numeric)\nroc_lr &lt;- performance(pred_lr, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc_lr, col = \"red\")\n\npred.M1 &lt;- predict(M1, newdata = df, type = 'raw')\npred_nb &lt;- prediction(pred.M1[, 2], df$survived_numeric) # pred.M1[,2] is score \nroc_nb &lt;-  performance(pred_nb, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc_nb, col = \"blue\", add = TRUE)\n\nlegend(x = \"bottomright\", \n       c(\"logistic regression\", \"naiveBayes\"),\n       col = c(\"red\", \"blue\"), \n       lty = 1)\n\n\n\n\n\n\n\nauc_lr = performance(pred_lr, measure =\"auc\")@y.values[[1]]\nauc_lr # 0.7597259\n\n[1] 0.7597259\n\nauc_nb &lt;- performance(pred_nb, \"auc\")@y.values[[1]]\nauc_nb #0.7164944\n\n[1] 0.7164944\n\n# LR is slightly better than NB in \n# predicting survival status \n\n\n\n\nOnsite Questions (Logistic Regression, Threshold)\n(Data set Smarket.csv contains data on percentage returns for the S&P 500 stock index over 1250 days, from the beginning of 2001 until the end of 2005. For each date, the data contains the trading volume and the percentage returns for each of the five previous trading days, Lag1 through Lag5.\n\nFind the proportion of the days that the direction is up.\n\ndf &lt;- read.csv(\"Smarket.csv\")\nstr(df)\n\n'data.frame':   1250 obs. of  10 variables:\n $ X        : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Year     : int  2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ...\n $ Lag1     : num  0.381 0.959 1.032 -0.623 0.614 ...\n $ Lag2     : num  -0.192 0.381 0.959 1.032 -0.623 ...\n $ Lag3     : num  -2.624 -0.192 0.381 0.959 1.032 ...\n $ Lag4     : num  -1.055 -2.624 -0.192 0.381 0.959 ...\n $ Lag5     : num  5.01 -1.055 -2.624 -0.192 0.381 ...\n $ Volume   : num  1.19 1.3 1.41 1.28 1.21 ...\n $ Today    : num  0.959 1.032 -0.623 0.614 0.213 ...\n $ Direction: chr  \"Up\" \"Up\" \"Down\" \"Up\" ...\n\ndf$X &lt;- NULL\ntab &lt;- prop.table(table(df$Direction))\nprint(tab)\n\n\n  Down     Up \n0.4816 0.5184 \n\n# Proportion of days where direction is up is 0.5184\n\nFit a logistic regression model (called model M3) that helps to predict the direction of S&P 500 index, based on trading volume and the percentage returns for each of the five previous trading days, Lag1 through Lag5.\n\ndf$num_dir &lt;- ifelse(df$Direction == \"Up\", 1, 0)\nstr(df)\n\n'data.frame':   1250 obs. of  10 variables:\n $ Year     : int  2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ...\n $ Lag1     : num  0.381 0.959 1.032 -0.623 0.614 ...\n $ Lag2     : num  -0.192 0.381 0.959 1.032 -0.623 ...\n $ Lag3     : num  -2.624 -0.192 0.381 0.959 1.032 ...\n $ Lag4     : num  -1.055 -2.624 -0.192 0.381 0.959 ...\n $ Lag5     : num  5.01 -1.055 -2.624 -0.192 0.381 ...\n $ Volume   : num  1.19 1.3 1.41 1.28 1.21 ...\n $ Today    : num  0.959 1.032 -0.623 0.614 0.213 ...\n $ Direction: chr  \"Up\" \"Up\" \"Down\" \"Up\" ...\n $ num_dir  : num  1 1 0 1 1 1 0 1 1 1 ...\n\nM3 &lt;- glm(num_dir ~ Volume + Lag1 + Lag2 + Lag3 + Lag4 + Lag5,\n          data = df,\n          family = binomial)\nsummary(M3)\n\n\nCall:\nglm(formula = num_dir ~ Volume + Lag1 + Lag2 + Lag3 + Lag4 + \n    Lag5, family = binomial, data = df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nVolume       0.135441   0.158360   0.855    0.392\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nConsider δ = 0.5184 as the border line to predict the direction be up or down, calculate the accuracy of model M3 when predicting for the days in the data set given.\nHint: To recall on what the role of δ is, please revise Tutorial 5 question 2.\n\nsigma &lt;- 0.5184\npred.prob &lt;- predict(M3, df, type = \"response\")\nhead(pred.prob)\n\n        1         2         3         4         5         6 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 \n\npred.lab &lt;- ifelse(pred.prob &gt;= sigma, 1, 0)\nhead(pred.lab)\n\n1 2 3 4 5 6 \n0 0 0 0 0 0 \n\nacc &lt;- mean(pred.lab == df$num_dir)\nprint(acc)\n\n[1] 0.5352\n\n# acc got 0.5352\n\n(Extra) Write R code to plot the ROC curve for model M3. Derive and report AUC value of it.\n\nlibrary(ROCR)\npred &lt;- prediction(pred.prob, df$num_dir)\nroc &lt;- performance(pred, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc, col = \"blue\")\n\n\n\n\n\n\n\nauc &lt;- performance(pred , \"auc\")@y.values[[1]]\nprint(auc) # 0.5387341\n\n[1] 0.5387341"
  },
  {
    "objectID": "index.html#t10-k-means-clustering",
    "href": "index.html#t10-k-means-clustering",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T10 (K-Means Clustering)",
    "text": "T10 (K-Means Clustering)\nUse kmeans() function: kout &lt;- kmeans(data, centers = n)\nTo get final centroid positions: kout$centers\nTo get size of each cluster: kout$size\nTo get WSS for each cluster: kout$withinss\nTo get Total WSS: sum(kout$withinss) OR kout$tot.withinss\n\nOffsite Questions (K-Means Mathematics)\n\nSuppose we have data for five objects on two features:\n\n\n\n\nObject\nx1\nx2\n\n\n\n\nA\n1\n1\n\n\nB\n1.5\n2\n\n\nC\n3\n4\n\n\nD\n3.5\n5\n\n\nE\n4.5\n5\n\n\n\nWe set k=2 to cluster the five data points into two clusters, P and Q, and initialize the algorithm with the centroids \\((x_1, x_2, P) = (2, 2)\\) and \\((x_1, x_2, Q) = (4, 4)\\).\n(a) Fill up the following table to identify the objects in each cluster during the first iteration of the kkk-means algorithm:\n\n\n\nCluster\nObject(s)\n\n\n\n\nP\n\n\n\nQ\n\n\n\n\n\nx1 = c(1, 1.5, 3, 3.5, 4.5)\nx2 = c(1,2,4,5,5)\n\nplot(x1, x2, pch = 20, col = \"blue\")\n\ntext(1.1,1.1,\"A\")\ntext(1.6, 2.2, 'B')\ntext(3.1, 4.1, 'C')\ntext(3.63, 5, 'D')\ntext(4.35, 5, 'E')\n\n# Adding the starting centroids \npoints(2,2, pch = 2, col = 'red')\ntext(2.2, 2.1, 'C-P')\npoints(4,4, pch = 10, col = 'darkgreen')\ntext(4,3.8, 'C-Q')\n\n\n\n\n\n\n\n# 2.458333\n\n(b) Compute the new centroids for the two clusters based on cluster assignment in (a).\n\n# Adding the new centroids after the first iteration:\nplot(x1, x2, pch = 20, col = \"blue\")\n\npoints(1.25, 1.5, col = 'red', pch = 2)\ntext(1.35, 1.4, 'C-P-new')\npoints(11/3, 14/3, col = 'darkgreen', pch = 10)\ntext(11/3, 4.5, 'C-Q-new')\n\n\n\n\n\n\n\n\n(c) Based on the centroids computed in (b), identify the objects in each cluster during the second iteration of the k-means algorithm.\n\ndata = data.frame(x1, x2)\ndata\n\n   x1 x2\n1 1.0  1\n2 1.5  2\n3 3.0  4\n4 3.5  5\n5 4.5  5\n\nkout = kmeans(data, centers = 2)\n\n(d) Calculate the Within Sum of Squares (WSS) for the clustering assignment in (c).\n\nkout$withinss\n\n[1] 1.833333 0.625000\n\nkout$tot.withinss\n\n[1] 2.458333\n\n\n\n(K-Means) Consider data set hdb-2012-to-2014.csv which was extracted from the published data. The file has information on the HDB resale flats from Jan 2012 to Dec 2014.\n\n(a) Load data into R. Use k-means algorithm to pick an optimal value for k in terms of WSS, based on two variables, resale_price and floor_area_sqm.\n\ndf &lt;- read.csv(\"hdb-2012-to-2014.csv\")\n# str(df)\ndf$X &lt;- NULL\nnumeric_df &lt;- df[sapply(df, is.numeric)]\nnumeric_df &lt;- scale(numeric_df)\n# str(numeric_df)\n\nK &lt;- 20\nwss &lt;- numeric(K)\nfor (k in 1:K) {\n  kout &lt;- kmeans(numeric_df[,c(\"resale_price\", \"floor_area_sqm\")], centers = k)\n  wss[k] &lt;- kout$tot.withinss\n}\nprint(wss)\n\n [1] 12092.0000  5504.6301  2993.4997  2095.7739  1793.0680  1567.9575\n [7]  1186.4503  1063.3284   970.4396   795.9938   725.1273   698.4266\n[13]   656.0890   606.6262   571.2220   533.8408   507.0974   509.0501\n[19]   433.0342   419.6298\n\nplot(1:K, wss, type=\"b\")\n\n\n\n\n\n\n\n# k = 3 is a good choice, \n# because the reduction in WSS \n# from k = 3 to k = 4\n# is not significant\n\n(b) With the optimal k in part (a), plot the data points in the k clusters determined.\n\nkout &lt;- kmeans(numeric_df, centers = 3)\nplot(\n  df$floor_area_sqm,\n  df$resale_price,\n  col = kout$cluster\n)\n\n\n\n\n\n\n\n\n\nunscaled_kout &lt;- kmeans(df[,c(\"floor_area_sqm\", \"resale_price\")], centers = 3)\nplot(\n  df$floor_area_sqm,\n  df$resale_price,\n  col = unscaled_kout$cluster\n)\n\n\n\n\n\n\n\n\n\n\nOnsite Questions (K-Means Clustering)\nConsider the famous Iris Flower Data set which was first introduced in 1936 by the famous statistician Ronald Fisher. This data set consists of observations from flowers of Iris species. For each observation, four features were measured: the flower’s length and width of the sepals and petals (in cm).\nThe data set is given in a file named iris.csv.\n\nUse K-means clustering method to cluster all the flowers into k groups where k=1,2,3,…,10 where for each value of k the value of WSS - the within sum of squares is obtained.\n\ndf &lt;- read.csv(\"iris.csv\")\nstr(df)\n\n'data.frame':   150 obs. of  5 variables:\n $ sepal.length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ sepal.width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ petal.length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal.width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ class       : chr  \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n\ndf$class &lt;- NULL\ndf &lt;- scale(df)\n\nwss_list &lt;- numeric(10)\nset.seed(10)\nfor (k in 1:10) {\n  kout &lt;- kmeans(df, centers = k)\n  wss_list[k] &lt;- kout$tot.withinss\n  }\nprint(wss_list)\n\n [1] 596.00000 222.24046 140.02604 113.60040 105.71145  81.67272  88.44134\n [8]  66.71099  57.17694  47.35887\n\n\nWrite code to obtain the plot of WSS against k. Which value of k would you choose as the number of clusters for all the observations in the data set? Explain.\n\nplot(1:10, wss_list, \n     type = 'b',\n     col = \"black\",\n     xlab = \"Number of Clusters\", \n     ylab = \"Within Sum of Squares\")\n\n\n\n\n\n\n\n# k = 3 is a good choice, \n# because the reduction in WSS \n# from k = 3 to k = 4\n# is not significant\n\nWith the value of k chosen above, report the centroids of all the clusters and the number of the observations in each cluster.\n\nkout &lt;- kmeans(df, centers = 3)\nkout$centers\n\n  sepal.length sepal.width petal.length petal.width\n1  -1.01119138   0.8394944   -1.3005215  -1.2509379\n2   1.13217737   0.0962759    0.9929445   1.0137756\n3  -0.05005221  -0.8773526    0.3463713   0.2811215\n\nkout$size\n\n[1] 50 47 53"
  },
  {
    "objectID": "index.html#t11-association-rules",
    "href": "index.html#t11-association-rules",
    "title": "DSA1101 FINAL: Everything I Need",
    "section": "T11 (Association Rules)",
    "text": "T11 (Association Rules)\nMachine learning concept used in Market Based Analysis\n\nPatterns of co-occurrence\nLink between products purchased together\n\nRules do not extract an individual’s preference, rather find relationships between set of elements of every distinct transaction. Different from collaborative filtering.\nSupport: Measure of how frequent an itemset is in all the transactions.\n\nConfidence: Measure of likeliness of occurrence of buying a specific item given that the user is buying another item.\n\nLift: The rise in probability of having {Y} in the cart with the knowledge of {X} being present (compared to no knowledge on {X} being present)\n\nLift &lt; 1 : Having {X} on the cart does not increase the chances of occurrence of {Y} on the cart\nLift &gt; 1 :  Vouches for high association between {Y} and {X}. Higher value indicates greater chances of preference to buy {Y} if the customer has already bought {X}\nLeverage: Similar notion to Lift, except instead of using Ratios, it uses differences\n\nSimilar notion to Lift, except instead of using Ratios, it uses differences\nLeverage &gt; 0 : Strong relationship for {X} -&gt; {Y}\nLeverage = 0 : Statistically Independent ({X} does not affect {Y})\nLeverage &lt; 0 : Negative relationship between {X} and {Y}\n\nOffsite Questions (Association Rules Mathematics)\n\nA local retailer has a database that stores 10,000 transactions of last summer. After analyzing the data, a data science team has identified the following statistics:\n\n\n{battery} appears in 6000 transactions\n{sunscreen} appears in 5000 transactions\n{sandals} appears in 4000 transactions\n{bowls} appears in 2000 transactions\n{battery, sunscreen} appears in 1500 transactions\n{battery, sandals} appears in 1000 transactions\n{battery, bowls} appears in 250 transactions\n{battery, sunscreen, sandals} appears in 600 transactions\n\n\nWhat are the support values of the preceding itemsets?\n\nAssuming the minimum support is 0.05, which itemsets are considered frequent?\n\nWhat are the confidence values of {battery} → {sunscreen} and {battery, sunscreen} → {sandals}? Which of these two rules is more interesting, i.e., has higher values of confidence?\n\n\n\nSuppose for three products ( A, B ) and ( C ), support({A}) = 0.6, support({B}) = 0.6, confidence({B} → {A}) = 0.9 and confidence({C} → {A, B}) = 0.5. Compute the following quantities:\n\nLift({A} → {B})\nLeverage({A} → {B})\nConfidence({A} → {B})\nLift({A, B} → {C})\n\n\n\n\n\nQ&A\n\nCan you explain the order function?\nReturns indexes of sorted version of vector passed to it\n\nx &lt;- c(9, 18, 5, 15, 12)\norder(x)\n\n[1] 3 1 5 4 2\n\n\n\nx &lt;- c(9, 18, 5, 15, 12)\nsorted_indexes &lt;- order(x)\nx[sorted_indexes]\n\n[1]  5  9 12 15 18\n\n\n\nx &lt;- c(9, 18, 5, 15, 12)\nx[order(x)]\n\n[1]  5  9 12 15 18\n\n\n\ndf &lt;- data.frame(\n  x = c(9, 18, 5, 15, 12),\n  y = c(\"A\", \"B\", \"C\", \"D\", \"E\"))\n\ndf\n\n   x y\n1  9 A\n2 18 B\n3  5 C\n4 15 D\n5 12 E\n\nsorted_index &lt;- order(df$x)\ndf[sorted_index, ]\n\n   x y\n3  5 C\n1  9 A\n5 12 E\n4 15 D\n2 18 B\n\nsorted_index &lt;- order(df$x, decreasing = TRUE)\ndf[sorted_index, ]\n\n   x y\n2 18 B\n4 15 D\n5 12 E\n1  9 A\n3  5 C\n\n\nIs it possible to give a template code for every topic?\n\nTemplate code is provided in the Tutorial R Code files on Canvas\nCan also look through previous tutorial slides for sample code\nGo through the code, understand the logic and be prepared to modify it where necessary to suit the final’s questions’ needs\n\n\n# (3.e) Repeat question 3d for k = 3, 5, 10. Which k gives the best precision?\n\nk_values = c(3, 5, 10)\nfor (k_val in k_values) {\n  knn.pred &lt;- knn(train.X, test.X, train.Y, k = k_val) \n  conf.matrix &lt;- table(test.Y, knn.pred)\n  precision &lt;- conf.matrix[2,2]/sum(conf.matrix[,2])\n  print(paste(k_val, precision))\n}\n\n\n# (1.e.) Use 5-fold cross validation to get average accuracy (K = 1).\n\nn_folds &lt;- 5\nfolds &lt;- rep(1:n_folds, length.out = dim(data)[1])\nfolds &lt;- sample(folds)\naccuracies &lt;- numeric(n_folds)\n\nfor (curr_fold in 1:n_folds) {\n  test_index &lt;- which(folds == curr_fold)\n  train.X &lt;- standardised.X[-test_index,]\n  test.X &lt;- standardised.X[test_index,]\n  train.Y &lt;- data$Creditability[-test_index]\n  test.Y &lt;- data$Creditability[test_index]\n  pred &lt;- knn(train.X, test.X, train.Y, k = 1)\n  accuracies[curr_fold] &lt;- mean(test.Y == pred)\n}\n\nIf a NB question has numeric inputs in the dataset given, are we expected to change it to categorical by using as.factor? Even if qn doesn’t say anything? Or qn wud usually specify?\n\nNaive Bayes can actually handle numeric features, but it is not taught in this course\nAs such, any features for Naive Bayes should be categorical\nDo not change it to categorical yourself, question will usually specify\n\n\nCan you explain conditional entropy and its calculation?\n\nEntropy is a measure of the randomness\n\nImagine using this to measure entropy of flipping a coin\n\nFormula for conditional entropy"
  }
]